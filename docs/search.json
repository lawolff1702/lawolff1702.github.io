[
  {
    "objectID": "posts/Sparse Kernel Machines/index.html",
    "href": "posts/Sparse Kernel Machines/index.html",
    "title": "Sparse Kernel Machines",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom kernel import KernelLogisticRegression\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nHere is the link to my implementation of sparse ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍kernelized ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍logistic ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍regression (kernel.py).\n\nAbstract\n– insert here —\n\n\nModel"
  },
  {
    "objectID": "posts/Implementing Logistic Regression/index.html",
    "href": "posts/Implementing Logistic Regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nMy implementation of the logistic regression algorithm (logistic.py) can be found here.\n\nAbstract\nIn this blog post we will explore the implementation of logistic regression and focus on optimizing the binary cross-entropy loss through gradient descent with momentum. We will validate and explore the model we implemented through several experiments on synthetic datasets. We will observe both vanilla gradient descent and momentum-enhanced gradient descent and hwo the latter can improve the speed of our models. We will also examine the challenges of overfitting by working with high-dimensional data sets. Finally, we will apply our logistic regression model to predict FIFA World Cup match outcomes using real-world statistics and highlight the challenges and benefits therein. Through these experiments, we illustrate the advantages and trade-offs between convergence efficiency and model generalizability, and offer some thoughts on practical classification tasks and possible future work.\n\n\nImplementation\nThe logistic regression model I implemented minimizes the binary cross-entropy loss to classify data. During training, we use momentum to help the model move more quickly in the right direction by combining the current gradient with a fraction of the previous update as we try search the loss-space for a minimum. The momentum should help reduce oscillations and speed up convergence! Lets see how it works!\n\n\nExploration\nWe will start with vanilla gradient descent. We will be working with two-dimensional data (x1, x2) and setting our \\(\\beta\\) value to zero to nullify the momentum term.\nThe code below provides functions to generate and plot data for a classification problem that we can address using our model. To test our vanilla model, lets generate some linearly separable data!\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# torch.manual_seed(67)\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\ndef plot_lr_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"PRGn\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = classification_data(noise = 0.2, p_dims = 2)\nplot_lr_data(X, y, ax)\n\n\n\n\n\n\n\n\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec_vanilla = []\nmax_iter = 1000\n\nfor _ in range(max_iter): \n    loss = LR.loss(X, y)\n    loss_vec_vanilla.append(loss)\n\n    opt.step(X, y, alpha=0.45, beta=0)\n\nloss\n\ntensor(0.0140)\n\n\nThats great! Looks like we were able to separate the data by achieving a minimal loss! Lets take a look at the line that separates our points.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_lr_data(X, y, ax)\ndraw_line(LR.w, x_min = -0.5, x_max = 1.5, ax = ax, color = \"slategrey\")\n\n\n\n\n\n\n\n\nAs seen above, our line very handily separates the two classes of points. Lets take a look at how our loss evolved over time by plotting our loss at each step.\n\nplt.plot(loss_vec_vanilla, color = \"purple\", lw=2)\n# plt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"purple\")\nlabs = plt.gca().set(xlabel = \"Logistic Regression Iteration\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nIt is interesting to note that our Logistic Regression is not jumping around as we observed with the Perceptron algorithm in the previous blog post. On the contrary, using gradient descent on a convex loss function, we are slowly advancing towards the minimum of the function.\nNow lets look at how momentum can help us! We will use the same dataset, however, this time we will instantiate a model with a \\(\\beta=0.9\\).\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec_momentum = []\n\nfor _ in range(max_iter): \n    loss = LR.loss(X, y)\n    loss_vec_momentum.append(loss)\n\n    opt.step(X, y, alpha=0.45, beta=0.9)\n\nloss\n\ntensor(0.0020)\n\n\nLets see how these processes stack up against each other. For the graph we are\n\nnum_iter = 500\nplt.plot(loss_vec_vanilla[:num_iter], color = \"purple\", lw=2, label = \"vanilla\")\nplt.plot(loss_vec_momentum[:num_iter], color = \"blue\", lw=2, label = \"momentum\")\nlabs = plt.gca().set(xlabel = \"Logistic Regression Iteration\", ylabel = \"loss\", title = f\"Momentum vs. Vanilla Gradient Descent (first {num_iter} iterations)\")\nplt.legend()\n\n\n\n\n\n\n\n\nAs we can see, including a momentum factor helps the model narrow in on the minimum loss a lot faster! When we use vanilla, we are updating our weight vector only using teh current gradient. However, with momentum we are also considering the direction of the previous update, so we build up momentum when we are going the right direction which helps us find the minimum faster.\nNext, we are going to take a look at some issues pertaining to overfitting our model. We are going to generate data where the the number of features is higher than the number of points we have, p_dim &gt; n_points. To illustrate the issues of overfitting, we will generate separate testing and validation data sets, then compare their accuracy.\n\nX_train, y_train = classification_data(noise = 1.2, p_dims = 100, n_points = 50)\nX_test, y_test = classification_data(noise = 1.2, p_dims = 100, n_points = 50)\n\nHere is a small helper function to calculate our accuracy.\n\ndef acc(model, X, y):\n    y_hat = model.predict(X)\n    return (y_hat == y).float().mean().item()\n\nNow lets train a model that achieves 100% training accuracy!\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\ntrain_loss = []\ntest_loss = []\n\nfor _ in range(50):\n    train_loss.append(LR.loss(X_train, y_train))\n    test_loss.append(LR.loss(X_test, y_test))\n\n    opt.step(X_train, y_train, alpha=0.5, beta=0.9)\n    \ntrain_acc = acc(LR, X_train, y_train)\nprint(f\"Training Accuracy: {train_acc * 100:.2f}% Training Loss: {LR.loss(X_train, y_train):.4f}\")\n\nTraining Accuracy: 100.00% Training Loss: 0.0000\n\n\nVoila! This seems amazing! We have 100% accuracy and a very low loss! However, our testing data may not ignite the same happiness within us.\n\ntest_acc = acc(LR, X_test, y_test)\nprint(f\"Test Accuracy: {test_acc * 100:.2f}% Test Loss: {LR.loss(X_test, y_test):.4f}\")\n\nTest Accuracy: 88.00% Test Loss: 1.1519\n\n\nYouch. This is a lot lower accuracy that we would have liked. In addition, the testing loss is quite high! Lets have a look at how our model performs at each iteration.\n\nplt.plot(train_loss, color = \"green\", lw=2, label = \"Training\")\nplt.plot(test_loss, color = \"orange\", lw=2, label = \"Testing\")\nlabs = plt.gca().set(xlabel = \"Logistic Regression Iteration\", ylabel = \"loss\", title = f\"Training vs Testing Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nWe can see our training loss decrease and converge to zero, however, at a certain point our testing loss that was once decreasing too, begins to increase! The model continues to learn (and thus overfit) the training data, even as it starts to perform worse on generalizing to new data. When we have a higher number of features than we do samples, overfitting can become problematic as it did above.\n\n\nPredicting World Cup Match Winners\nTo illustrate our Logistic Regression model on some empirical data, we are going to use World Cup Match Data and try and predict match winners! The dataset we are using was compiled on Kaggle by Brenda Loznik in 2022 from publicly available FIFA World Cup data. Players were manually classified as either goalkeeper, defender, midfielder, or offensive player. For each team, only the top-performing players were selected. These plater statistics were used to create the power scores for each team. In some cases, data is missing — this indicates that a country did not have enough qualifying players to met the selection criteria. Additionally, the dataset assumes that each season runs from September to the following August.\n\nimport pandas as pd\ndf = pd.read_csv('international_matches.csv')\ndf.head()\n\n\n\n\n\n\n\n\ndate\nhome_team\naway_team\nhome_team_continent\naway_team_continent\nhome_team_fifa_rank\naway_team_fifa_rank\nhome_team_total_fifa_points\naway_team_total_fifa_points\nhome_team_score\n...\nshoot_out\nhome_team_result\nhome_team_goalkeeper_score\naway_team_goalkeeper_score\nhome_team_mean_defense_score\nhome_team_mean_offense_score\nhome_team_mean_midfield_score\naway_team_mean_defense_score\naway_team_mean_offense_score\naway_team_mean_midfield_score\n\n\n\n\n0\n1993-08-08\nBolivia\nUruguay\nSouth America\nSouth America\n59\n22\n0\n0\n3\n...\nNo\nWin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1993-08-08\nBrazil\nMexico\nSouth America\nNorth America\n8\n14\n0\n0\n1\n...\nNo\nDraw\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1993-08-08\nEcuador\nVenezuela\nSouth America\nSouth America\n35\n94\n0\n0\n5\n...\nNo\nWin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1993-08-08\nGuinea\nSierra Leone\nAfrica\nAfrica\n65\n86\n0\n0\n1\n...\nNo\nWin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n1993-08-08\nParaguay\nArgentina\nSouth America\nSouth America\n67\n5\n0\n0\n1\n...\nNo\nLose\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 25 columns\n\n\n\nBelow are the training features we will be using, I selected mainly score based features like rank, points, and positional scores because I thought they would work best.\n\nfeatures = [\n    'home_team_fifa_rank',\n    'away_team_fifa_rank',\n    'home_team_total_fifa_points',\n    'away_team_total_fifa_points',\n    'home_team_goalkeeper_score',\n    'home_team_mean_defense_score',\n    'home_team_mean_offense_score',\n    'home_team_mean_midfield_score',\n    'away_team_goalkeeper_score',\n    'away_team_mean_defense_score',\n    'away_team_mean_offense_score',\n    'away_team_mean_midfield_score'\n]\n\nHere is a first look at our raw data. Lets do some quick preprocessing to clean the dataset to make sure we have data in all fields, create a target vector, etc.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ndf_wc = df[df['tournament'].str.contains(\"World Cup\", case=False)]\ndf_wc_clean = df_wc.dropna()\ndf_wc_clean = df_wc_clean[df_wc_clean['home_team_result'] != 'Draw']\ndf_wc_clean = df_wc_clean[df_wc_clean['shoot_out'] == 'No']\nle.fit(df_wc_clean[\"home_team_result\"])\ndf_wc_clean[\"home_team_result\"] = le.transform(df_wc_clean[\"home_team_result\"])\n\nX_features = torch.tensor(df_wc_clean[features].values, dtype=torch.float32)\ny_tensor = torch.tensor(df_wc_clean[\"home_team_result\"].values, dtype=torch.float32)\n\n# Add bias term\nX_tensor = torch.cat((X_features, torch.ones((X_features.shape[0], 1))), 1)\n\nX_train_tensor, X_temp_tensor, y_train_tensor, y_temp_tensor = train_test_split(X_tensor, y_tensor, test_size=0.4, random_state=72, stratify=y_tensor)\nX_val_tensor, X_test_tensor, y_val_tensor, y_test_tensor = train_test_split(X_temp_tensor, y_temp_tensor, test_size=0.5, random_state=72, stratify=y_temp_tensor)\n\nI selected only World Cup Matches, dropped the matches with missing data, dropped all the matches that ended in a draw or went to penalties, and made a binary encoding for who won the match. Finally I created training, validation, and testing split. Our target value is home_team_result.\nLets go ahead and train our model! Lets first do without momentum then with momentum!\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nmax_iter = 30000\na = 0.00001\n\ntrain_loss = []\nval_loss = []\n\nfor _ in range(max_iter):\n    train_loss.append(LR.loss(X_train_tensor, y_train_tensor))\n    val_loss.append(LR.loss(X_val_tensor, y_val_tensor))\n\n    opt.step(X_train_tensor, y_train_tensor, alpha=a, beta=0)\n    \ntrain_acc = acc(LR, X_train_tensor, y_train_tensor)\nval_acc = acc(LR, X_val_tensor, y_val_tensor)\nprint(f\"Training Accuracy: {train_acc * 100:.2f}% Training Loss: {LR.loss(X_train_tensor, y_train_tensor):.4f}\")\nprint(f\"Validation Accuracy: {val_acc * 100:.2f}% Validation Loss: {LR.loss(X_val_tensor, y_val_tensor):.4f}\")\n\nTraining Accuracy: 70.46% Training Loss: 0.7281\nValidation Accuracy: 80.22% Validation Loss: 0.6169\n\n\nThis is pretty good considering the nature of football! After some serious experimentation and terrible results of tuning the hyper parameters, I landed on quite a low \\(\\alpha\\) learning rate and turned the number of iterations up quite high to make sure our model had more time to train and get a lower loss level. At first I was testing with \\(\\alpha = 0.1\\) with 1000 iterations. Interestingly, my accuracy stayed about the same ~70%, however, the loss curve was quite jagged and oscillated in the region of 4.5-6 — a great indication that we needed to take smaller steps and let the model train for longer! After tuning, I was sufficiently happy with the loss being sub-one despite not being closer to zero. Lets see how that worked on our test data!\n\ntest_acc = acc(LR, X_test_tensor, y_test_tensor)\nprint(f\"Test Accuracy: {test_acc * 100:.2f}% Test Loss: {LR.loss(X_test_tensor, y_test_tensor):.4f}\")\n\nTest Accuracy: 74.18% Test Loss: 0.6130\n\n\nWe are still performing similarly on unseen data! That is a great sign that our model wasn’t overfitting. Let’s explore how using momentum will affect the model.\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\ntrain_loss_m = []\nval_loss_m = []\n\nfor _ in range(max_iter):\n    train_loss_m.append(LR.loss(X_train_tensor, y_train_tensor))\n    val_loss_m.append(LR.loss(X_val_tensor, y_val_tensor))\n\n    opt.step(X_train_tensor, y_train_tensor, alpha=a, beta=0.9)\n    \ntrain_acc = acc(LR, X_train_tensor, y_train_tensor)\nval_acc = acc(LR, X_val_tensor, y_val_tensor)\ntest_acc = acc(LR, X_test_tensor, y_test_tensor)\n\nprint(f\"Training Accuracy: {train_acc * 100:.2f}% Training Loss: {LR.loss(X_train_tensor, y_train_tensor):.4f}\")\nprint(f\"Validation Accuracy: {val_acc * 100:.2f}% Validation Loss: {LR.loss(X_val_tensor, y_val_tensor):.4f}\")\nprint(f\"Test Accuracy: {test_acc * 100:.2f}% Test Loss: {LR.loss(X_test_tensor, y_test_tensor):.4f}\")\n\nTraining Accuracy: 75.41% Training Loss: 0.5183\nValidation Accuracy: 79.12% Validation Loss: 0.4741\nTest Accuracy: 73.63% Test Loss: 0.4944\n\n\nUsing momentum seemed to help slightly on our training and validation, however, given the drop off in testing accuracy, this may be a result of overfitting over the high number of iterations. Lets actually take a look at the evolution of our loss to see how the training process evolved and how momentum may have made an impact.\n\nfirst = 500\nfig, axes = plt.subplots(1, 2, figsize=(14, 6)) \n\naxes[0].plot(train_loss, \"--\", color=\"brown\", lw=2, label=\"Training\", alpha=0.5)\naxes[0].plot(val_loss, \"--\", color=\"orange\", lw=2, label=\"Validation\", alpha=0.5)\naxes[0].plot(train_loss_m, color=\"blue\", lw=2, label=\"Momentum Training\", alpha=0.5)\naxes[0].plot(val_loss_m, color=\"purple\", lw=2, label=\"Momentum Validation\", alpha=0.5)\naxes[0].set_title(\"Training vs Validation Loss (All Iterations)\")\naxes[0].set_xlabel(\"Logistic Regression Iteration\")\naxes[0].set_ylabel(\"Loss\")\naxes[0].legend()\n\naxes[1].plot(train_loss[:first], \"--\", color=\"brown\", lw=2, label=\"Training\", alpha=0.5)\naxes[1].plot(val_loss[:first], \"--\", color=\"orange\", lw=2, label=\"Validation\", alpha=0.5)\naxes[1].plot(train_loss_m[:first], color=\"blue\", lw=2, label=\"Momentum Training\", alpha=0.5)\naxes[1].plot(val_loss_m[:first], color=\"purple\", lw=2, label=\"Momentum Validation\", alpha=0.5)\naxes[1].set_title(f\"Training vs Validation Loss (First {first} Iterations)\")\naxes[1].set_xlabel(\"Logistic Regression Iteration\")\naxes[1].set_ylabel(\"Loss\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nMomentum definitely had an effect on the convergence speed of the model! At around 1000 and definitely by 5000 iterations we see that the loss has essentially plateaued. On the other hand, our vanilla logistic regression that only really got there around 6000 and could keep training even after 30000 iterations. I found the first 500 iterations especially fascinating. We can see that our momentum may have sent us too far in the wrong direction at times, causing the loss to jump up before eventually steadying out and beginning to trend in the right direction. With respect to this issue, it would be interesting to implement variable learning rates throughout the process, possibly starting very small and as we begin to movement in the right direction we can increase the step size (similar to how momentum works) and do the inverse when we go in teh wrong direction. We could also think about variable momentum coefficients that work in a similar fashion. One interesting factor to observe is that our validation loss was consistently lower that our training loss which I honestly cannot explain for the moment apart from luck with the validation set. Nonetheless, we can observe that momentum was a significant optimizing factor in training our model.\n\n\nDiscussion\nIn this post, we implemented a logistic regression model and conducted a series of experiments to better understand its behavior. We started off using a simple two-dimensional dataset to compare vanilla gradient descent with momentum aided descent. We demonstrated that momentum can smooth the convergence process and accelerate progress in a clean, linearly separable context. We then tackled the issue of overfitting by experimenting with scenarios where the number of features exceeded the number of data points, highlighting some of the pitfalls of excessive model complexity. Finally, we applied the model to predict World Cup match winners using real match data. With very small learning rates and high iteration numbers we were able to achieve decent accuracy and loss. In the real-world scenario we still observed the benefits of introducing momentum as our model converged a lot faster using momentum. In conclusion, these experiments deepened my understanding of model implementation and optimization, the balance required to prevent the dangers of overfitting, and challenges applying logistic regression to real-world applications. We also outlined some further optimizations concerning variable parameters that we could implement in the future."
  },
  {
    "objectID": "posts/Deep Music Genre Classification/index.html",
    "href": "posts/Deep Music Genre Classification/index.html",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "Abstract\n– Enter Here –\n\n\nData\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchinfo import summary\n\nimport pandas as pd\nimport numpy as np\nimport time\n\n# for train-test split\nfrom sklearn.model_selection import train_test_split\n\n# for suppressing bugged warnings from torchinfo\nimport warnings\nwarnings.filterwarnings(\"ignore\", category = UserWarning)\n\n# tokenizers from HuggingFace\nfrom transformers import BertTokenizer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nc:\\Users\\lukka\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nWe are loading in a Kaggle dataset that contains information about music made between the years 1950 and 2019 collected through Spotify. The dataset contains lyrics, artist info, track names, etc. Importantly it also includes music metadata like sadness, danceability, loudness, acousticness, etc.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\nLets have a look at some of the raw data!\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n0\n0\nmukesh\nmohabbat bhi jhoothi\n1950\npop\nhold time feel break feel untrue convince spea...\n95\n0.000598\n0.063746\n0.000598\n...\n0.380299\n0.117175\n0.357739\n0.454119\n0.997992\n0.901822\n0.339448\n0.137110\nsadness\n1.0\n\n\n1\n4\nfrankie laine\ni believe\n1950\npop\nbelieve drop rain fall grow believe darkest ni...\n51\n0.035537\n0.096777\n0.443435\n...\n0.001284\n0.001284\n0.331745\n0.647540\n0.954819\n0.000002\n0.325021\n0.263240\nworld/life\n1.0\n\n\n2\n6\njohnnie ray\ncry\n1950\npop\nsweetheart send letter goodbye secret feel bet...\n24\n0.002770\n0.002770\n0.002770\n...\n0.002770\n0.225422\n0.456298\n0.585288\n0.840361\n0.000000\n0.351814\n0.139112\nmusic\n1.0\n\n\n3\n10\npérez prado\npatricia\n1950\npop\nkiss lips want stroll charm mambo chacha merin...\n54\n0.048249\n0.001548\n0.001548\n...\n0.225889\n0.001548\n0.686992\n0.744404\n0.083935\n0.199393\n0.775350\n0.743736\nromantic\n1.0\n\n\n4\n12\ngiorgos papadopoulos\napopse eida oneiro\n1950\npop\ntill darling till matter know till dream live ...\n48\n0.001350\n0.001350\n0.417772\n...\n0.068800\n0.001350\n0.291671\n0.646489\n0.975904\n0.000246\n0.597073\n0.394375\nromantic\n1.0\n\n\n\n\n5 rows × 31 columns\n\n\n\nHere is a brief look at how many songs we have in each represented genre.\n\ndf.groupby(\"genre\").size()\n\ngenre\nblues      4604\ncountry    5445\nhip hop     904\njazz       3845\npop        7042\nreggae     2498\nrock       4034\ndtype: int64\n\n\nThis is a pretty large number of songs to classify… and some genres I personally dont care for. So, to make the dataframe more manageable and applicable to me personally, we are going to narrow down to only observe reggae, hip hop, rock and jazz.\n\ngenres = {\n    \"hip hop\"   : 0,\n    \"jazz\" : 1,\n    \"reggae\" : 2,\n    \"rock\" : 3,\n}\n\ndf = df[df[\"genre\"].apply(lambda x: x in genres.keys())]\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n17091\n54304\ngene ammons\nit's the talk of the town\n1950\njazz\nlovers sweethearts hard understand know happen...\n61\n0.001096\n0.001096\n0.001096\n...\n0.319570\n0.001096\n0.352323\n0.620388\n0.868474\n0.235830\n0.430132\n0.282260\nsadness\n1.0\n\n\n17092\n54305\ngene ammons\nyou go to my head\n1950\njazz\nhead linger like haunt refrain spin round brai...\n48\n0.001754\n0.340964\n0.001754\n...\n0.001754\n0.001754\n0.379400\n0.638541\n0.907630\n0.900810\n0.221970\n0.184159\nviolence\n1.0\n\n\n17093\n54307\nbud powell\nyesterdays\n1950\njazz\nmusic speak start hear musicians like dizzy gi...\n107\n0.001144\n0.001144\n0.074762\n...\n0.001144\n0.097082\n0.489873\n0.467400\n0.992972\n0.927126\n0.334295\n0.228204\nmusic\n1.0\n\n\n17094\n54311\ntony bennett\nstranger in paradise\n1950\njazz\nhand stranger paradise lose wonderland strange...\n41\n0.002105\n0.180524\n0.002105\n...\n0.527429\n0.002105\n0.179032\n0.559470\n0.983936\n0.001781\n0.086974\n0.235211\nsadness\n1.0\n\n\n17095\n54313\ndean martin\nzing-a zing-a zing boom\n1950\njazz\nzinga zinga zinga zinga zinga zinga zinga zing...\n160\n0.001253\n0.001253\n0.001253\n...\n0.425721\n0.001253\n0.580851\n0.687409\n0.655622\n0.000000\n0.936109\n0.418400\nsadness\n1.0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\ndf[\"genre\"] = df[\"genre\"].apply(genres.get)\ndf\n\n\n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n17091\n54304\ngene ammons\nit's the talk of the town\n1950\n1\nlovers sweethearts hard understand know happen...\n61\n0.001096\n0.001096\n0.001096\n...\n0.319570\n0.001096\n0.352323\n0.620388\n0.868474\n0.235830\n0.430132\n0.282260\nsadness\n1.000000\n\n\n17092\n54305\ngene ammons\nyou go to my head\n1950\n1\nhead linger like haunt refrain spin round brai...\n48\n0.001754\n0.340964\n0.001754\n...\n0.001754\n0.001754\n0.379400\n0.638541\n0.907630\n0.900810\n0.221970\n0.184159\nviolence\n1.000000\n\n\n17093\n54307\nbud powell\nyesterdays\n1950\n1\nmusic speak start hear musicians like dizzy gi...\n107\n0.001144\n0.001144\n0.074762\n...\n0.001144\n0.097082\n0.489873\n0.467400\n0.992972\n0.927126\n0.334295\n0.228204\nmusic\n1.000000\n\n\n17094\n54311\ntony bennett\nstranger in paradise\n1950\n1\nhand stranger paradise lose wonderland strange...\n41\n0.002105\n0.180524\n0.002105\n...\n0.527429\n0.002105\n0.179032\n0.559470\n0.983936\n0.001781\n0.086974\n0.235211\nsadness\n1.000000\n\n\n17095\n54313\ndean martin\nzing-a zing-a zing boom\n1950\n1\nzinga zinga zinga zinga zinga zinga zinga zing...\n160\n0.001253\n0.001253\n0.001253\n...\n0.425721\n0.001253\n0.580851\n0.687409\n0.655622\n0.000000\n0.936109\n0.418400\nsadness\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n28367\n82447\nmack 10\n10 million ways\n2019\n0\ncause fuck leave scar tick tock clock come kno...\n78\n0.001350\n0.001350\n0.001350\n...\n0.065664\n0.001350\n0.889527\n0.759711\n0.062549\n0.000000\n0.751649\n0.695686\nobscene\n0.014286\n\n\n28368\n82448\nm.o.p.\nante up (robbin hoodz theory)\n2019\n0\nminks things chain ring braclets yap fame come...\n67\n0.001284\n0.001284\n0.035338\n...\n0.001284\n0.001284\n0.662082\n0.789580\n0.004607\n0.000002\n0.922712\n0.797791\nobscene\n0.014286\n\n\n28369\n82449\nnine\nwhutcha want?\n2019\n0\nget ban get ban stick crack relax plan attack ...\n77\n0.001504\n0.154302\n0.168988\n...\n0.001504\n0.001504\n0.663165\n0.726970\n0.104417\n0.000001\n0.838211\n0.767761\nobscene\n0.014286\n\n\n28370\n82450\nwill smith\nswitch\n2019\n0\ncheck check yeah yeah hear thing call switch g...\n67\n0.001196\n0.001196\n0.001196\n...\n0.001196\n0.001196\n0.883028\n0.786888\n0.007027\n0.000503\n0.508450\n0.885882\nobscene\n0.014286\n\n\n28371\n82451\njeezy\nr.i.p.\n2019\n0\nremix killer alive remix thriller trap bitch s...\n83\n0.001012\n0.075202\n0.001012\n...\n0.001012\n0.033995\n0.828875\n0.674794\n0.015862\n0.000000\n0.475474\n0.492477\nobscene\n0.014286\n\n\n\n\n11281 rows × 31 columns\n\n\n\nThe base rate on our classification is the proportion of the data set occupied by the largest label class:\n\ndf.groupby(\"genre\").size() / len(df)\n\ngenre\n0    0.080135\n1    0.340839\n2    0.221434\n3    0.357592\ndtype: float64\n\n\nIf we always guessed category 3, then we would expect an accuracy of roughly 36%. So, our task will be to see whether we can train a model to beat this.\nAs we try to predict the genre of the track, we will use lyrics alongside some other engineered features (metadata) that we define below.\n\nengineered_features = ['dating', 'violence', 'world/life', 'night/time','shake the audience','family/gospel', 'romantic', 'communication','obscene', 'music', 'movement/places', 'light/visual perceptions','family/spiritual', 'like/girls', 'sadness', 'feelings', 'danceability','loudness', 'acousticness', 'instrumentalness', 'valence', 'energy']      \n\nOur models will only need these engineered features, lyrics, and our target value which will be genre so we can throw them all into the same dataframe and use slicing to access different parts later.\n\ndf_clean= df[engineered_features + ['lyrics', 'genre']].copy()\ndf_clean.head()\n\n\n\n\n\n\n\n\ndating\nviolence\nworld/life\nnight/time\nshake the audience\nfamily/gospel\nromantic\ncommunication\nobscene\nmusic\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\nlyrics\ngenre\n\n\n\n\n17091\n0.001096\n0.001096\n0.001096\n0.001096\n0.036316\n0.001096\n0.001096\n0.460773\n0.086498\n0.001096\n...\n0.319570\n0.001096\n0.352323\n0.620388\n0.868474\n0.235830\n0.430132\n0.282260\nlovers sweethearts hard understand know happen...\n1\n\n\n17092\n0.001754\n0.340964\n0.001754\n0.001754\n0.001754\n0.001754\n0.131872\n0.001754\n0.001754\n0.001754\n...\n0.001754\n0.001754\n0.379400\n0.638541\n0.907630\n0.900810\n0.221970\n0.184159\nhead linger like haunt refrain spin round brai...\n1\n\n\n17093\n0.001144\n0.001144\n0.074762\n0.046173\n0.001144\n0.018789\n0.001144\n0.001655\n0.001144\n0.421734\n...\n0.001144\n0.097082\n0.489873\n0.467400\n0.992972\n0.927126\n0.334295\n0.228204\nmusic speak start hear musicians like dizzy gi...\n1\n\n\n17094\n0.002105\n0.180524\n0.002105\n0.002105\n0.002105\n0.002105\n0.002105\n0.201965\n0.002105\n0.002105\n...\n0.527429\n0.002105\n0.179032\n0.559470\n0.983936\n0.001781\n0.086974\n0.235211\nhand stranger paradise lose wonderland strange...\n1\n\n\n17095\n0.001253\n0.001253\n0.001253\n0.001253\n0.001253\n0.081126\n0.001253\n0.111951\n0.001253\n0.268737\n...\n0.425721\n0.001253\n0.580851\n0.687409\n0.655622\n0.000000\n0.936109\n0.418400\nzinga zinga zinga zinga zinga zinga zinga zing...\n1\n\n\n\n\n5 rows × 24 columns\n\n\n\nFinally, we will perform a train-validation split to later evaluate our data\n\ndf_train, df_val = train_test_split(df_clean,shuffle = True, test_size = 0.2)\n\n\n\nText Vectorization\nWe now need to vectorize the lyrics. We’re going to use tokenization to break up the lyrics into a sequence of tokens, and then vectorize that sequence.\nWe will be using a tokenizer imported from HuggingFace.\n\ntokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n\nFor our purposes it’s more convenient to assign an integer to each token, which we can do like this:\n\nencoded = tokenizer(\"I love reggae music!\")\nencoded\n\n{'input_ids': [101, 1045, 2293, 15662, 2189, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\nTo do the reverse, we can use the .decode method of the tokenizer:\n\ntokenizer.decode(encoded[\"input_ids\"])\n\n'[CLS] i love reggae music! [SEP]'\n\n\nHere is some code to help us prepare our dataset with encodings. A lot of our lyrics are different lengths so we will pad the shorter ones with 0s and truncate others that are especially long. We will make use of the torch Dataset class to help manage our data.\n\nmax_len = 512 # BERT capacity\n\ndef pad(l, max_len):\n    assert len(l) &lt;= max_len\n    to_add = max_len - len(l)\n    return l + [0]*to_add\n\ndef preprocess(df, tokenizer, max_len):\n    # X = tokenizer(list(df[\"lyrics\"]))[\"input_ids\"]\n    # X = [pad(t, max_len) for t in X]\n    X = tokenizer(list(df[\"lyrics\"]), padding=\"max_length\", truncation=True, max_length=max_len)[\"input_ids\"]\n    X = [x + e for x, e in zip(X, df[engineered_features].values.tolist())]\n    y = list(df[\"genre\"])\n    return X, y\n\nclass TextDataFromDF(Dataset):\n    def __init__(self, df):\n        self.X, self.y = preprocess(df, tokenizer, max_len)\n\n    def __getitem__(self, ix):\n        return self.X[ix], self.y[ix]\n\n    def __len__(self):\n        return len(self.y)\n\nLets make our encoded datasets!\n\ntrain_data = TextDataFromDF(df_train)\nval_data   = TextDataFromDF(df_val)\n\nHere is what a single songs information looks like now:\n\nX, y = train_data[1]\nprint(X)\nprint(y)\n\n[101, 25237, 25237, 2645, 4682, 2264, 25237, 25237, 2681, 2215, 4536, 8795, 3524, 3524, 3233, 2954, 2655, 2655, 4151, 2455, 2489, 2954, 2204, 2954, 2903, 2157, 25237, 2935, 8391, 2954, 2204, 2954, 25237, 2935, 8391, 2233, 2233, 2455, 2188, 2709, 4828, 10497, 5358, 8739, 7195, 14318, 2015, 2264, 8135, 2272, 10689, 2907, 2152, 15908, 3426, 7354, 27524, 9895, 2368, 2574, 5510, 3886, 4781, 4125, 2892, 2455, 2954, 2204, 2954, 2903, 2157, 25237, 2935, 8391, 2954, 2204, 2954, 25237, 2935, 8391, 2645, 2645, 7354, 27524, 21038, 2015, 3582, 6750, 15834, 4536, 15834, 2954, 3696, 2892, 16579, 2015, 2563, 5000, 8391, 14437, 2668, 5472, 25237, 25237, 5722, 4562, 2925, 6225, 15616, 2954, 2204, 2954, 2903, 2157, 25237, 2935, 8391, 2954, 2204, 2954, 25237, 2935, 8391, 2272, 25237, 2645, 22825, 2954, 2204, 2954, 2903, 2157, 25237, 2935, 8391, 25237, 2935, 8391, 2954, 2204, 2954, 2903, 2157, 25237, 2935, 8391, 25237, 2935, 8391, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0006662225501567, 0.3885539767840311, 0.1039642699113738, 0.1125086495754739, 0.0006662225253185, 0.1385745183743181, 0.0006662225963956, 0.000666222543954, 0.0006662225404265, 0.0006662225612787, 0.0496462185467528, 0.0006662225812354, 0.0831919481290733, 0.0006662225243746, 0.0006662225370302, 0.1018737873863844, 0.4021444817502437, 0.70771479116991, 0.0003744983679702, 0.0003613360323886, 0.3085325638911788, 0.6446335461127515]\n3\n\n\nWe are going to be feeding data in in batches, so we will need a dataloader which necessitates a collate function to ensure our we are imputing tensors of the right size.\n\ndef collate(data):\n    X = torch.tensor([d[0] for d in data])\n    y = torch.tensor([d[1] for d in data])\n    return X,y \n\ntrain_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn = collate)\nval_loader = DataLoader(val_data, batch_size=8, shuffle=True, collate_fn = collate)\n\nHere is what a batch of data looks like. The predictor data is now a tensor in which the entries give token indices, padded with 0s and ending with the values of our engineered features. For visualization purposes we’ll show only the first 3 rows:\n\nX, y = next(iter(train_loader))\nX[:3]\n\ntensor([[1.0100e+02, 2.1890e+03, 2.3730e+03,  ..., 1.3462e-04, 9.3405e-01,\n         5.1550e-01],\n        [1.0100e+02, 2.3265e+04, 6.5460e+03,  ..., 2.9757e-03, 8.7428e-01,\n         4.4743e-01],\n        [1.0100e+02, 3.0420e+03, 4.3760e+03,  ..., 6.2348e-01, 5.2082e-01,\n         5.6155e-01]])\n\n\n\ny[:3]\n\ntensor([2, 1, 1])\n\n\n\n\nModel Building\nWe are going to train three neural networks to classify our genres.\n\nUsing Lyrics to Classify\nUsing Engineered Features (Metadata) to Classify\nUsing Lyrics and Metadata to Classify\n\nLets build a model for classifying genres based on lyrics first.\n\nclass TextClassificationModel(nn.Module):\n\n    def __init__(self,vocab_size, embedding_dim, max_len, num_class):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.fc = nn.Linear(max_len*embedding_dim, num_class)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return(x)\n\n\nvocab_size = len(tokenizer.vocab)\nembedding_dim = 10\nnum_class = len(genres)\n\ntext_model = TextClassificationModel(vocab_size, embedding_dim, max_len, num_class).to(device)\n\n\nsummary(text_model, input_Size = (8, max_len))\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nTextClassificationModel                  --\n├─Embedding: 1-1                         305,230\n├─Linear: 1-2                            20,484\n=================================================================\nTotal params: 325,714\nTrainable params: 325,714\nNon-trainable params: 0\n=================================================================\n\n\n\ndef train(model, dataloader):\n    optimizer = torch.optim.Adam(model.parameters(), lr=.1)\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    epoch_start_time = time.time()\n    # keep track of some counts for measuring accuracy\n    total_acc, total_count = 0, 0\n    \n    for X, y in dataloader:\n        \n        # zero gradients\n        optimizer.zero_grad()\n        # form prediction on batch\n        predicted_label = model(X)\n        # evaluate loss on prediction\n        loss = loss_fn(predicted_label, y)\n        # compute gradient\n        loss.backward()\n        # take an optimization step\n        optimizer.step()\n                \n        # for printing accuracy\n        total_acc   += (predicted_label.argmax(1) == y).sum().item()\n        total_count += y.size(0)\n\n    print(f'| epoch {epoch:3d} | train accuracy {total_acc/total_count:8.3f} | time: {time.time() - epoch_start_time:5.2f}s')\n\ndef accuracy(model, dataloader):\n\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            predicted_label = model(X)\n            total_acc += (predicted_label.argmax(1) == y).sum().item()\n            total_count += y.size(0)\n    return total_acc/total_count"
  },
  {
    "objectID": "posts/Auditing Bias/index.html",
    "href": "posts/Auditing Bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Abstract\nThis project audits bias in automated decision-making systems by analyzing employment predictions from the 2018 American Community Survey (ACS) data for Georgia. A Random Forest Classifier was trained to predict employment status based on demographic features such as age, education, sex, disability, and nativity, while examining racial bias specifically between White and Black/African American individuals. The audit revealed approximately balanced accuracy, positive predictive values, and error rates across these racial groups, though slight discrepancies exist. Despite good numerical fairness, we must still consider ethical considerations regarding consent, data recency, and the ethical deployment of such models in different decision-making contexts.\n\n\nData and Feature Selection\nWe are using the folkables package to access data from the 2018 American Community Survey’s Public Use Microdata Sample (PUMS) for the state of Georgia.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"GA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000025\n5\n1\n3700\n3\n13\n1013097\n68\n51\n...\n124\n69\n65\n63\n117\n66\n14\n68\n114\n121\n\n\n1\nP\n2018GQ0000035\n5\n1\n1900\n3\n13\n1013097\n69\n56\n...\n69\n69\n7\n5\n119\n74\n78\n72\n127\n6\n\n\n2\nP\n2018GQ0000043\n5\n1\n4000\n3\n13\n1013097\n89\n23\n...\n166\n88\n13\n13\n15\n91\n163\n13\n89\n98\n\n\n3\nP\n2018GQ0000061\n5\n1\n500\n3\n13\n1013097\n10\n43\n...\n19\n20\n3\n9\n20\n3\n3\n10\n10\n10\n\n\n4\nP\n2018GQ0000076\n5\n1\n4300\n3\n13\n1013097\n11\n20\n...\n13\n2\n14\n2\n1\n2\n2\n13\n14\n12\n\n\n\n\n5 rows × 286 columns\n\n\n\nThis data set contains a large amount of features for each individual, so we are going to narrow it down to only those that we may use to train our model.\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n51\n13.0\n5\n16\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n2\n6.0\n\n\n1\n56\n16.0\n3\n16\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n1\n2.0\n2\n1\n6.0\n\n\n2\n23\n20.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n1.0\n2\n2\n1.0\n\n\n3\n43\n17.0\n1\n16\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n1\n2\n6.0\n\n\n4\n20\n19.0\n5\n16\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n\n\n\n\n\nA few key features to note are:\n\nESR is employment status (1 if employed, 0 if not)\nRAC1P is race (1 for White Alone, 2 for Black/African American Alone, 3 and above for other self-identified racial groups)\nSEX is binary sex (1 for male, 2 for female)\n\nNow we select for the features we want to use and we will be able to constuct a BasicProblem that expresses our desire to use these features to predict employment status ESR, using RAC1P as the group label.\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nWe now have a feature matrix features, a label vector label, and a group label vector group.\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(100855, 15)\n(100855,)\n(100855,)\n\n\nWe are now going to split our data into a training set and a testing set:\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nData Exploration\nBefore we dive straight into model training, lets take a deeper look at our data.\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\nHere is a quick look at our data frame containing our training data:\n\ndf.head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\ngroup\nlabel\n\n\n\n\n0\n48.0\n16.0\n1.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n2\nTrue\n\n\n1\n52.0\n24.0\n2.0\n0.0\n2.0\n0.0\n1.0\n3.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n1\nTrue\n\n\n2\n55.0\n18.0\n5.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n3.0\n1.0\n2.0\n2.0\n2.0\n1.0\n2\nTrue\n\n\n3\n15.0\n12.0\n5.0\n3.0\n2.0\n1.0\n4.0\n3.0\n0.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2.0\n6\nFalse\n\n\n4\n26.0\n22.0\n1.0\n0.0\n2.0\n0.0\n5.0\n1.0\n4.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2\nFalse\n\n\n\n\n\n\n\n\nlen(df)\n\n80684\n\n\nThis data set contains information from \\(80684\\) individuals in the state of Georgia.\n\ndf[\"label\"].value_counts()\n\nlabel\nFalse    44664\nTrue     36020\nName: count, dtype: int64\n\n\n\ndf['label'].mean()\n\nnp.float64(0.44643299786822666)\n\n\nOf these individuals, 44.64% or \\(36020\\) individuals are employed.\n\ndf['group'].value_counts()\n\ngroup\n1    53302\n2    20239\n6     3267\n9     1996\n8     1589\n3      159\n5       66\n7       66\nName: count, dtype: int64\n\n\n\ndf['group'].value_counts(normalize=True)\n\ngroup\n1    0.660627\n2    0.250843\n6    0.040491\n9    0.024738\n8    0.019694\n3    0.001971\n5    0.000818\n7    0.000818\nName: proportion, dtype: float64\n\n\nThe two largest racial groups are 1 White Alone with 53302 individuals making up 66% of the data, and 2 Black/African American Alone with 20239 individuals making up 25% of the data.\n\ndf.groupby('group')['label'].mean()\n\ngroup\n1    0.460771\n2    0.416127\n3    0.433962\n5    0.348485\n6    0.481175\n7    0.484848\n8    0.429830\n9    0.330160\nName: label, dtype: float64\n\n\n~46% of White Alone individuals are employed and ~42% of Black/African American Alone individuals are employed.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(style=\"whitegrid\")\n\n\ndef plot_intersection(df, col1='group', col2='sex'):\n    prop_df = df.groupby([col1, col2])['label'].mean().reset_index()\n    plt.figure(figsize=(8, 5))\n\n    ax = sns.barplot(x=col1, y='label', hue=col2, data=prop_df)\n    \n    plt.title(f'Employment Proportion by {col1} and {col2}')\n    plt.xlabel(col1)\n    plt.ylabel(f'Employment Proportion (%)')\n\n    for p in ax.patches:\n        ax.annotate(f'{p.get_height()*100:.2f}', \n                   (p.get_x() + p.get_width() / 2., p.get_height()),\n                   ha = 'center', va = 'bottom',\n                   xytext = (0, 5), textcoords = 'offset points')\n    \n    plt.tight_layout()\n    plt.show()\n\ngroup_sex = plot_intersection(df, 'group', 'SEX')\n    \n\n\n\n\n\n\n\n\nFor many groups, the percentage of men who are employed is higher than that of women. One notable group where this is not the case is 2 Black/African American where the percentage of employed women is ~44% against ~39% for men.\nNATIVITY indicates a persons place of birth. 1 being Native born and 2 being Foreign born.\n\ngroup_nativity = plot_intersection(df, 'group', 'NATIVITY')\n\n\n\n\n\n\n\n\nInterestingly, across the board we see that the percentage of foreign born individuals who are employed is much higher than the proportion of native born individuals. However, as seen in the plot below, this may be attributed to the fact that few foreign born individuals on the extremities of the age, reducing the influence of youth and seniority as factors in employment proportion.\n\nsns.displot(data=df, x=\"AGEP\", hue=\"NATIVITY\", kind=\"kde\", bw_adjust=0.5, fill=True, alpha=0.75)\n\n\n\n\n\n\n\n\nDIS represents an individuals disability status. 1 with disability, and 2 without a disability.\n\ngroup_dis = plot_intersection(df, 'group', 'DIS')\n\n\n\n\n\n\n\n\nAcross the board we see that people without disability are employed at a much higher proportion than people with disabilities.\n\n\nSupplementary plots that I thought were interesting.\n\ncit_sex = plot_intersection(df, 'CIT', 'SEX')\n\n\n\n\n\n\n\n\n\nmar_sex = plot_intersection(df, 'MAR', 'SEX')\n\n\n\n\n\n\n\n\n\nschl_sex = plot_intersection(df, 'SCHL', 'SEX')\n\n\n\n\n\n\n\n\n\n\n\nModel Training\nWe are now ready to create a model and train it on our training data. We will first scale our data, then we will employ a Random Forest Classifier. This approach uses an array of decision trees on various sub-samples of the data and aggregates their results. Learn more here.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\nacc = 0\nbest_depth = 0\nfor depth in range(5, 20):\n    model = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=depth))\n    model.fit(X_train, y_train)\n    cv_scores = cross_val_score(model, X_train, y_train, cv = 5)\n    if cv_scores.mean() &gt; acc:\n        best_depth = depth\n        acc = cv_scores.mean()\n\nprint(f\"Best maximum tree depth: {best_depth}, Accuracy: {acc*100:.2f}%\")\n\nBest maximum tree depth: 16, Accuracy: 83.39%\n\n\nAbove, we tuned our model complexity using the max_depth parameter of the RandomForestClassifier. This controls how deep each tree in our forest can get which impacts how general our model is with regards to things like overfitting. We examined values ranging from 5 to 20 for the max depth and found the highest cross validated accuracy when max_depth=16.\n\nRF = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=best_depth))\nRF.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('randomforestclassifier',\n                 RandomForestClassifier(max_depth=16))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('randomforestclassifier',\n                 RandomForestClassifier(max_depth=16))]) StandardScaler?Documentation for StandardScalerStandardScaler() RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(max_depth=16) \n\n\n\n\nModel Audit\n\nOverall Measures\n\ny_hat = RF.predict(X_test)\n(y_hat == y_test).mean()\n\nnp.float64(0.8281195776114223)\n\n\nOur model has an overall accuracy of 82.81% on the testing data suite. Not too shabby!\nWe will now address the positive predictive value (PPV) of our model (we wont evaluate true sufficiency right now as we are skipping NPV). Given that the prediction is positive (y_hat = 1), how likely is it that the prediction is accurate (y_test = 1)? In other words, if we predict someone to be employed, how likely is it that they are actually employed?\nWe can approximate this value with the following code:\n\ntp = (y_hat == 1) & (y_test == 1)\nfp = (y_hat == 1) & (y_test == 0)\nppv = tp.sum() / (tp.sum() + fp.sum())\nprint(f\"Positive Predictive Value: {ppv*100:.2f}%\")\n\nPositive Predictive Value: 77.46%\n\n\nSo, when our model predicts someone is employed, they are actually employed 77.46% of the time.\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\ncm = confusion_matrix(y_test, y_hat)\nConfusionMatrixDisplay(cm, display_labels=['Not Employed', 'Employed']).plot(cmap='Blues')\nplt.title('Confusion Matrix')\n\nText(0.5, 1.0, 'Confusion Matrix')\n\n\n\n\n\n\n\n\n\nAbove is our confusion matrix, lets use this information to find the false negative and false positive rates for our model.\n\nfn = (y_hat == 0) & (y_test == 1)\nfnr = fn.sum() / (tp.sum() + fn.sum())\nprint(f\"False Negative Rate: {fnr*100:.2f}%\")\n\nFalse Negative Rate: 12.74%\n\n\nOur model incorrectly classified people as unemployed when they were in fact employed 12.74% of the time.\n\nfp = (y_hat == 1) & (y_test == 0)\ntn = (y_hat == 0) & (y_test == 0)\nfpr = fp.sum() / (fp.sum() + tn.sum())\nprint(f\"False Positive Rate: {fpr*100:.2f}%\")\n\nFalse Positive Rate: 20.84%\n\n\nOur model incorrectly classified people as employed when they were in fact unemployed 20.84% of the time.\n\n\nBy-Group Measures\nNow, lets explore how our model treats people in their respective groups. We are going to focus primarily on the possible discrepancies between individuals in 1 White Alone and 2 Black/African American Alone groups.\n\nwa = (y_hat == y_test)[group_test == 1].mean()\naa = (y_hat == y_test)[group_test == 2].mean()\nprint(f\"White Alone Accuracy: {wa*100:.2f}%\\nBlack/African American Alone Accuracy: {aa*100:.2f}%\")\n\nWhite Alone Accuracy: 82.40%\nBlack/African American Alone Accuracy: 83.17%\n\n\nOur model has pretty comparable accuracy scores for both groups, only slightly lower for White Alone individuals.\n\ntp_wa = ((y_hat == 1) & (y_test == 1) & (group_test == 1))\nfp_wa = ((y_hat == 1) & (y_test == 0) & (group_test == 1))\nppv_wa = tp_wa.sum() / (tp_wa.sum() + fp_wa.sum())\n\ntp_aa = ((y_hat == 1) & (y_test == 1) & (group_test == 2))\nfp_aa = ((y_hat == 1) & (y_test == 0) & (group_test == 2))\nppv_aa = tp_aa.sum() / (tp_aa.sum() + fp_aa.sum())\n\nprint(f\"PPV for White Alone: {ppv_wa*100:.2f}%\")\nprint(f\"PPV for Black/African American Alone: {ppv_aa*100:.2f}%\")\n\nPPV for White Alone: 77.96%\nPPV for Black/African American Alone: 75.84%\n\n\nOnce again our PPV rates are quite comparable, although the White Alone group does have a slightly higher PPV.\n\nfn_wa = (y_hat == 0) & (y_test == 1) & (group_test == 1)\nfnr_wa = fn_wa.sum() / (tp_wa.sum() + fn_wa.sum())\n\nfn_aa = (y_hat == 0) & (y_test == 1) & (group_test == 2)\nfnr_aa = fn_aa.sum() / (tp_aa.sum() + fn_aa.sum())\n\nprint(f\"FNR for White Alone: {fnr*100:.2f}%\")\nprint(f\"FNR for Black/African American Alone: {fnr_aa*100:.2f}%\")\n\nFNR for White Alone: 12.74%\nFNR for Black/African American Alone: 12.86%\n\n\nThe false negative rates are once again very similar across groups.\n\nfp_wa = ((y_hat == 1) & (y_test == 0) & (group_test == 1))\ntn_wa = ((y_hat == 0) & (y_test == 0) & (group_test == 1))\nfpr_wa = fp_wa.sum() / (fp_wa.sum() + tn_wa.sum())\n\nfp_aa = ((y_hat == 1) & (y_test == 0) & (group_test == 2))\ntn_aa = ((y_hat == 0) & (y_test == 0) & (group_test == 2))\nfpr_aa = fp_aa.sum() / (fp_aa.sum() + tn_aa.sum())\n\nprint(f\"FPR for White Alone: {fpr_wa*100:.2f}%\")\nprint(f\"FPR for Black/African American Alone: {fpr_aa*100:.2f}%\")\n\nFPR for White Alone: 21.33%\nFPR for Black/African American Alone: 19.64%\n\n\nThere is some slight discrepancy here as persons in the White Alone group is more often mistaken for having a job than persons in the Black/African American Alone group.\n\n\nBias Measures\nIn terms of accuracy, our model seems to be performing well. Let’s take a deeper look at how our model might be biased or unfair by examining calibration, error rate balance, and statistical parity.\nOur model can be considered well-calibrated or sufficient if it reflects equal likelihood of employment irrespective of the individuals’ group membership. That is, free from predictive bias, this our PPV for both groups should be the same. Looking back to our calculation of these scores, we saw that they were about equal. PPV for White Alone: 77.96% and PPV for Black/African American Alone: 75.84%. Thus, we can say our model is well-calibrated.\n\nOur model can only satisfy approximate error rate balance given that the true positive rate (TPR) and false positive rates (FPR) be equal on the two groups.\n\ntpr_wa = tp_wa.sum() / (tp_wa.sum() + fn_wa.sum())\ntpr_aa = tp_aa.sum() / (tp_aa.sum() + fn_aa.sum())\nprint(f\"TPR --&gt; WA: {tpr_wa*100:.2f}% ~~ AA: {tpr_aa*100:.2f}%\")\nprint(f\"FPR --&gt; WA: {fpr_wa*100:.2f}% ~~ AA: {fpr_aa*100:.2f}%\")\n\nTPR --&gt; WA: 86.69% ~~ AA: 87.14%\nFPR --&gt; WA: 21.33% ~~ AA: 19.64%\n\n\nWe can see that both groups have an approximately equal TPR and FPRs. Thus our model satisfies approximate error rate balance.\n\nOur model satisfies statistical parity if the proportion of individuals classified as employed is the same for each group.\n\nprop_wa = (y_hat == 1)[group_test == 1].mean()\nprop_aa = (y_hat == 1)[group_test == 2].mean()\nprint(f\"Proportion of White Alone Predicted to be Employed: {prop_wa*100:.2f}%\")\nprint(f\"Proportion of Black/African American Alone Predicted to be Employed: {prop_aa*100:.2f}%\")\n\nProportion of White Alone Predicted to be Employed: 51.74%\nProportion of Black/African American Alone Predicted to be Employed: 47.61%\n\n\nWe can observe some differences in these two scores, a higher proportion of White Alone persons are predicted to be employed than Black/African American Alone persons. The difference remains small as the rated are within 5% of one another… as we don’t have a set threshold we cant know if the difference is significant, ergo we cant say that we do or do not satisfy statistical parity.\n\n\n\nFeasible FNR and FPRs\n\np_wa = ((y_test == 1) & (y_hat == 1))[group_test == 1].mean()\np_aa = ((y_test == 1) & (y_hat == 1))[group_test == 2].mean()\nprint(f\"Prevalence of White Alone: {p_wa*100:.2f}%\")\nprint(f\"Prevalence of Black/African American Alone: {p_aa*100:.2f}%\")\n\nPrevalence of White Alone: 40.34%\nPrevalence of Black/African American Alone: 36.11%\n\n\nThe proportion of true positive values is higher with in the White Alone group.\n\nimport numpy as np\n\nplt.figure(figsize=(8, 5))\n\n\nfnr_range = np.linspace(0, 1, 100)\nfpr_aa_alt = (p_aa / (1 - p_aa)) * ((1 - ppv_aa) / ppv_aa) * (1 - fnr_aa)\nfpr_wa_alt = (p_wa / (1 - p_wa)) * ((1 - ppv_wa) / ppv_wa) * (1 - fnr_wa)\n\n# Calibrate on low PPV --&gt; PPv from Black/African American Alone\nfpr_aa_feasible = (p_aa / (1 - p_aa)) * ((1 - ppv_aa) / ppv_aa) * (1 - fnr_range) # PPVb is set equal to the observed value of PPVw\nfpr_wa_feasible = (p_wa / (1 - p_wa)) * ((1 - ppv_aa) / ppv_aa) * (1 - fnr_range) # pw and PPVw are both held fixed\n\nplt.plot(fnr_wa, fpr_wa_alt, 'o', color='orange', label='White Alone')\nplt.plot(fnr_aa, fpr_aa_alt, 'o', color='black', label='Black/African American Alone')\n\nplt.plot(fnr_range, fpr_aa_feasible, '-', color='black')\nplt.plot(fnr_range, fpr_wa_feasible, '-', color='orange')\n\n\n# Shading Attempts\n# delta = np.abs(ppv_wa - ppv_aa)\n# low = (p_wa / (1 - p_wa)) * ((1 - ppv_wa - (delta &lt; 0.05)) / ppv_wa - (delta &lt; 0.05)) * (1 - fnr_range)\n# high = (p_wa / (1 - p_wa)) * ((1 - ppv_wa + (delta &lt; 0.05)) / ppv_wa + (delta &lt; 0.05)) * (1 - fnr_range)\n# plt.fill_between(fnr_range, low, high, color='blue', alpha=0.3)\n\nplt.xlabel('False Negative Rate')\nplt.ylabel('False Positive Rate')\nplt.title('Feasible (FNR, FPR) combinations')\nplt.legend()\n\n\n\n\n\n\n\n\n\nOur current model appears to be working quite well for both groups as we can see their (FNR, FPR) points are not far separated. However, if we did want to equalize our false positive rates (classify someone as employed when they are not), this would necessitate an increase in the false negative rates for White Alone to around 0.22 from its current around 0.12. This would inevitably lead to a drop in accuracy. Contextually, this would mean classifying more White Alone persons who are actually employed as unemployed to match FPR.\nNonetheless, our model performs close to even for both groups.\n\n\n\nConclusion\nThere are many applications for a model of this sort. Making predictions on who is or isn’t employed would benefit many companies that operate on credit or lending. Knowing who is employed would help determine whether it would be wise to approve a higher credit limit, approve a mortgage application, or let someone lease a car. This is the case because employment can be widely used aa feature or indicator of someones ability to pay back their debts and the interest included therein. If you determine that better, your company can be more profitable. This could also be useful for marketing and advertising companies who can use these predictions to improve the targeting of their ads.\nDeploying this specific Random Forest Based model in a large scale setting could make the small discrepancies in error much larger. For instance, lets assume this model is being used by the US government to determine who needs unemployment aid. The small differences in FPRs (WA: ~21% & AA: ~19) could mean tens of millions of people could be classified as employed and not receive the help they need. This would also be slightly disproportionate as millions more White Alone people would not get the aid they need when compared to the ratio of Black/African American individuals. Conversely, this FPR could be to their advantage in a commercial setting that would give them lower interest rates on loans, and possibly better odds at landing a job.\nBased on my Bias Audit, most of my standards of evaluation were quite close. None of them differed enough for me to view the model as problematic with regards to White Alone or Black/African American. Of course, the model also used data from other groups, thus necessitating further exploration into those groups to determine if there are problematic levels of bias.\nBias aside, there could be other potential issues in deploying this algorithm. First and foremost, I believe that, with the exception of advertisement targeting, employment status should be requested and declared with the consent of the individual. There should be some government regulations in what industries are allowed to use such algorithms and how. In the situations that it is used, I worry about the transparency of the model processes. Transparency on what data the data is being used for as it is being collected, and where it came from when it is being used. Moreover, there should be transparency and easily interpretable in use cases like credit approvals, etc. Finally, using data from a Georgia may not be generalizable across the country or even in current day Georgia as many things have changed since 2018 and after COVID. There should be an effort made to keep the training data reflective of current day trends. Thus, even though the model appears acceptable with regards to numerical fairness metrics, there are broader ethical and practical concerns that we should address in order to deploy a responsible and effective model."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, my name is Lukka Wolff. Welcome to my Blog!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lukka Wolff’s Machine Learning Blog",
    "section": "",
    "text": "Deep Music Genre Classification\n\n\n\n\n\n\nNatural Language Processing\n\n\nNeural Networks\n\n\nIn Progress\n\n\n\nDeep Music Genre Classification in Python\n\n\n\n\n\nMay 12, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Kernel Machines\n\n\n\n\n\n\nSparse Kernel Machines\n\n\nLogistic Regression\n\n\nImplementation\n\n\nIn Progress\n\n\n\nImplementing Sparse Kernelized Logistic Regression in Python\n\n\n\n\n\nApr 28, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\n\nLogistic Regression\n\n\nImplementation\n\n\n\nImplementing Logistic Regression in Python\n\n\n\n\n\nApr 9, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\n\nPerceptron\n\n\nImplementation\n\n\n\nImplementing the Perceptron Algorithm in Python\n\n\n\n\n\nApr 2, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\n\nClassification\n\n\nEthics\n\n\n\nAuditing Bias in Automated Decision-Making Systems\n\n\n\n\n\nMar 12, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\n\nClassification\n\n\nEthics\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nMar 5, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\n\nData Science\n\n\nClassification\n\n\n\nClassifying Palmer Penguins using Machine Learning\n\n\n\n\n\nFeb 26, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Classifying Palmer Penguins/index.html",
    "href": "posts/Classifying Palmer Penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nThis blog post will explore the use of machine learning to classify Palmer Penguins based on their physical measurements and location. Beginning with data preprocessing and exploration cleaned up and contextualized our analysis for the subsequent model training. We employed logistic regression and selected features in a repeatable and cross-validated manor achieving an 100% testing accuracy when classifying the penguins. This result shows the effectiveness of using culmen length, culmen depth, and island location as predictive features. The blog post also lays out provides a tradition workflow to use machine learning on ecological datasets.\n\n\nfrom IPython.display import Image, display\ndisplay(Image(filename='palmer_station.png'))\n\n\n\n\n\n\n\n\nImage Source\n\n\nData\nThis code block handles our Training Data Acquisition from the Palmer Penguins data set. This collection holds data for three types of Penguins living across three islands. There is a mix of quantitative measurements and qualitative observations.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nHere is a first look at our raw data:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nWe first extract the species name for data presentation purposed, drop unnecessary columns, remove missing values, and filter out any invalid data. We return the processed feature set (X_train) and target species labels (y_train).\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Mapping full species names exclude scientific names\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nThis is what our formatted feature set looks like:\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nData Exploration\nThe pair plot below gives us a introduces us to our data with a wide array of visualizations. These pairwise relationships are a great starting point to begin understanding the nature of the data.\n\nimport seaborn as sns\nsns.set_theme()\nsns.pairplot(data=train, hue=\"Species\")\n\n\n\n\n\n\n\n\nI scanned the above plot for data pairs that I thought showed promise in indicating species differentiation.\nHere is the relationship between Culmen Length (mm) and Culmen Depth (mm):\n\nsns.jointplot(data = train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue = \"Species\")\n\n\n\n\n\n\n\n\nWe can see that the three species of Penguins fall into three more or less distinct regions of the scatter plot with minimal overlap. This suggests that Culmen Length and Culmen Depth could be good points of reference when training a classification model.\nBelow is a plot that demonstrates the relationship between Gender, Flipper Length, and Body Mass:\n\nsns.catplot(data = X_train, x = \"Sex_FEMALE\", y = \"Flipper Length (mm)\", hue = \"Body Mass (g)\")\n\n\n\n\n\n\n\n\nThe above plot shows us that flipper length and body mass are positively correlated. In addition, we see that Males penguins have a higher growth ceiling in terms of body mass and flipper length.\nThe figure below illustrated some interesting data pertaining to species distribution over the islands. In addition, it gives us insight on the flipper length by species.\n\nimport numpy as np\ntrain.groupby([\"Island\", \"Species\"])[\"Flipper Length (mm)\"].agg(mean_flipper_length_mm=\"mean\", std_flipper_length_mm=\"std\")\n\n\n\n\n\n\n\n\n\nmean_flipper_length_mm\nstd_flipper_length_mm\n\n\nIsland\nSpecies\n\n\n\n\n\n\nBiscoe\nAdelie\n188.636364\n6.570855\n\n\nGentoo\n216.752577\n5.933715\n\n\nDream\nAdelie\n190.133333\n6.780989\n\n\nChinstrap\n196.000000\n7.423419\n\n\nTorgersen\nAdelie\n191.195122\n6.626536\n\n\n\n\n\n\n\nWe can observe that not every species is found on every island. The Adelie Penguin the only penguin found on all three surveyed islands. The Gentoo penguin is found exclusively on Biscoe Island and the Chinstrap penguin is found exclusively on Dream Island. In addition, the Gentoo penguins have the largest and least variable mean flipper length. They are followed by the Chinstrap penguins in size, then the Adelie with the smallest mean flipper lengths. Adelie and Chinstrap flipper lengths may overlap quite a bit, potentially making flipper length an unreliable feature for classifying the two species.\n\n\nModel Training\nIn the following we will employ a Logistic Regression model to classify our penguins. An integral step is figuring out which features we want to use to train our model. My methodology for this is a the brute force approach of testing out all the possible combinations using the handy combinations tool. The combinations tool will be used in conjunction with the cross_val_score tool from sklearn that will cross validate to avoid overfitting to the data. This step will help us perform better on data that the model has never seen before.\nAfter encountering maximum iteration issues with the logistic regression model, I scaled the data with the StandardScaler to be more manageable for the model\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\nLR = LogisticRegression()\nscaler = StandardScaler()\n\nX_train[all_quant_cols] = scaler.fit_transform(X_train[all_quant_cols])\nscore = 0\nfinal_cols = []\n\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    cross_val_scores = cross_val_score(LR, X_train[cols], y_train, cv = 10)\n    if cross_val_scores.mean() &gt; score:\n      score = cross_val_scores.mean()\n      final_cols = cols\n\nprint(f\"The best model scored {score*100}% accuracy when testing on training data using: \\n{final_cols}\")    \n\nThe best model scored 99.21538461538461% accuracy when testing on training data using: \n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nNow we have a repeatable, quantitative approach to justify training our model on the following features: Island, Culmen Length (mm), Culmen Depth (mm)\nI rearrange final_cols below to lead with quantitative features to conform to the graphing parameters that I will present later\n\nfinal_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nLR = LogisticRegression()\nLR.fit(X_train[final_cols], y_train)\nLR.score(X_train[final_cols], y_train)\n\n0.99609375\n\n\nOur model performed with ~99% accuracy when using our selected three features and testing on our training data. This validates some of our visual predictions we identified in our exploration section. This is a great accuracy to have… however we are still testing our model on the data it was trained with. Next we will test it against unseen data!\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nX_test, y_test = prepare_data(test)\nX_test[all_quant_cols] = scaler.fit_transform(X_test[all_quant_cols])\n\nLR.score(X_test[final_cols], y_test)\n\n1.0\n\n\nWow! We achieved an 100% testing accuracy on our test data! In context, we were able to correctly classify what type of penguin an individual was based on what island they were on and their culmen length and depth.\n\n\nEvaluation\nThe following block sets up a plot panel of decision regions that represent our classifier.\n\nfrom matplotlib import pyplot as plt\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (9, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[final_cols], y_train)\n\n\n\n\n\n\n\n\nAs we noted in our exploration, not all penguins are found on all islands. This mean that our model essentially only had to account for a maximum of two species of penguins on any given island. we can see that the Culmen Length and Depth were also clustered nicely to have clear linear segmentation for our decision regions.\nLets take a look at our confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[final_cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nAs we had a 100% testing accuracy, this is exactly the sort of confusion matrix we would expect. The diagonal representing our correct classifications. Above and below the diagonal are empty because we did not misclassify any penguins.\nHere is another way to digest the confusion matrix:\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguins who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguins who were classified as Adelie.\nThere were 0 Adelie penguins who were classified as Chinstrap.\nThere were 0 Adelie penguins who were classified as Gentoo.\nThere were 0 Chinstrap penguins who were classified as Adelie.\nThere were 11 Chinstrap penguins who were classified as Chinstrap.\nThere were 0 Chinstrap penguins who were classified as Gentoo.\nThere were 0 Gentoo penguins who were classified as Adelie.\nThere were 0 Gentoo penguins who were classified as Chinstrap.\nThere were 26 Gentoo penguins who were classified as Gentoo.\n\n\n\n\nDiscussion\nExploring the Palmer Penguins data is a great introduction to data analysis and machine learning. Our results highlighted the effectiveness of using Culmen Depth, Culmen Length and Island Location as predictive features to train a Logistic regression model. We began by exploring the data set. We set up a series of plots and tables that helped us contextualize the data and make predictions about which features of the data may be helpful. While this was useful for understanding the data we were working with, we needed a repeatable method for choosing our eventual three features. We then turned to training and testing logistic regression models on different feature combinations. We scored each combination with how it performed against the training data and was cross validated against smaller subsets of the data to avoid overfitting. Finally we used the cross validated features with the largest accuracy and tested them against a separate test data set. Here we achieved the desired 100% testing accuracy. Finally, we took a moment to evaluate these results by examining the decision regions, and looking at the confusion matrix. Visualizing decision regions highlighted how well logistic regression can separate species based on our selected features. The confusion matrix confirmed the reliability of our model as there were no misclassifications. This blog gave me good insight on data analysis and practical machine learning workflows. Several important takeaways were the importance of separating our training and testing data to ensure our model works on unseen data. The importance of cross validation is also key to not overfitting our data."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Abstract\nIn this study, we set out to design a data-driven system for automating credit decisions at a hypothetical bank. Our objective was to create a score function that captures the balance between potential revenue and risk, using predictive models trained on historical data to estimate the likelihood of loan repayment. Our primary drive as the hypothetical bank was to maximize profit, however, we observe that this choice has potentially negative impacts on our diverse borrowers.\n\n\nData\nWe are diving into a Credit Risk Dataset that simulates credit bureau data.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\nHere is a first look at the raw data:\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nI want to highlight several important features of this dataset.\n\nloan_percent_income is ratio of the loan amount to the individual’s income\nloan_int_rate is the annual interest rate on the loan.\nloan_status tells us whether or not the individual defaulted on their loan. This a a binary feature where 1 indicates the individual defaulted, and 0 indicates the loan was repaid in full. This is our Target Variable.\n\nLets have a look at how common defaulting is in our training data:\n\ndf_train[\"loan_status\"].value_counts(normalize=True)\n\nloan_status\n0    0.78242\n1    0.21758\nName: proportion, dtype: float64\n\n\nIn the dataset, around 21% of borrowers default on their loan. This is going to be the our base rate for prediction.\n\ndf_train[\"person_age\"].describe()\n\ncount    26064.000000\nmean        27.734385\nstd          6.362612\nmin         20.000000\n25%         23.000000\n50%         26.000000\n75%         30.000000\nmax        144.000000\nName: person_age, dtype: float64\n\n\nThere seems to be some slight errors in our data with the age of certain individuals. 144, although impressive, is highly unlikely. Thus, without context for why the data has such an outlier, I am going to filter our data to exclude persons over 100 years old. In addition, I will do some other basic data cleaning like removing NaN entries, encoding qualitative features, etc.\n\ndf_train = df_train[df_train[\"person_age\"] &lt; 100]\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(df_train[\"cb_person_default_on_file\"])\n\ndf_train[\"cb_person_default_on_file\"] = le.transform(df_train[\"cb_person_default_on_file\"])\n\ndf_train = df_train.dropna()\n\nNow our data set looks like this!\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\n1\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\n0\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\n0\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\n0\n10\n\n\n6\n21\n21700\nRENT\n2.0\nHOMEIMPROVEMENT\nD\n5500\n14.91\n1\n0.25\n0\n2\n\n\n\n\n\n\n\n\n\nExploration\nLet’s dive a bit into our data to see if we can pick up on any interesting trends.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(style=\"whitegrid\")\n\n\nsns.displot(data=df_train, x=\"loan_percent_income\", hue=\"loan_status\", kind=\"kde\", bw_adjust=0.9, fill=True, alpha=0.25)\n\n\n\n\n\n\n\n\nThe above figure uses Kernel Density Estimation (KDE), a technique that helps visualize the distribution of data points. In this case we are examining how loan_percent_income is distributed in our data set and its possible relation to defaulting on your loan. We can see here that there is a high density of people who repaid their loans where the loan amount factored between 10-20% of their annual income. This may point to the fact that 10-20% of ones income is a manageable amount, and where we see a spike in 1 around 35%+ may be an rate that is hard to pay off and thus lead to defaults. It is also notable that not many people are requesting loans that are 30%+ of their annual income.\n\nsns.kdeplot(data=df_train, x=\"cb_person_cred_hist_length\", y=\"loan_int_rate\", cmap=\"rocket\", fill=True)\n\n\n\n\n\n\n\n\nThis is another KDE plot that shows the relation in loan interest rates given by the bank and an individuals credit history length. We can observe that there is a high density of people who have a credit history length between 0-5 years. Within this rage there are two hot spots for the interest rates on their loans, around 7.5% and around 11% interest. So although a large amount of people have relatively short credit histories, the bank are differentiating their offers based on other factors.\n\nsns.catplot(data=df_train, kind=\"bar\", x=\"person_home_ownership\", y=\"loan_percent_income\", hue=\"loan_status\")\n\n\n\n\n\n\n\n\nThis figure really highlights the importance of loan_percent _income as a possible meaningful predictor of who will default. We see that across the board, regardless of home ownership type, those who repaid their loans requested loans that were around 15% of their annual income. However, those who defaulted on their loans were consistently requesting loans that were a much higher rate (ranging from ~17%-35%).\nLet’s examine how the connection between age, credit history length, and employment length.\n\nimport numpy as np\n\nbins = [0, 5, 10, 15, 20, np.inf]\nlabels = ['0-5', '5-10', '10-15', '15-20', '20+']\n\ndf_train.groupby(\n    pd.cut(df_train['cb_person_cred_hist_length'], bins=bins, labels=labels), observed=True\n)[['person_age', 'person_emp_length']].agg(['mean'])\n\n\n\n\n\n\n\n\nperson_age\nperson_emp_length\n\n\n\nmean\nmean\n\n\ncb_person_cred_hist_length\n\n\n\n\n\n\n0-5\n24.253351\n4.277778\n\n\n5-10\n29.975727\n5.417458\n\n\n10-15\n40.011823\n5.982576\n\n\n15-20\n41.297771\n6.046178\n\n\n20+\n57.318471\n5.923567\n\n\n\n\n\n\n\nAs expected credit history length increases with age. However it is interesting to note that the employment length (time spent at an individuals last job) remain roughly the same, between 4-6 years on average.\n\n\nModel Building\nNow we will dive into creating a model that will hopefully do a good job at predicting who we should give loans to. As a bank what we care about is profit! However, to train a model that will get us good profit, I will start by finding features to train our model on that perform with high accuracy.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\n\nall_qual_cols = [\"person_home_ownership\", \"loan_intent\"]\nall_quant_cols = [\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\", \"cb_person_default_on_file\"]\n\ndf_train = pd.get_dummies(df_train)\n\nLR = LogisticRegression(max_iter = 1000)\nscaler = StandardScaler()\n\n# df_train[all_quant_cols] = scaler.fit_transform(df_train[all_quant_cols])\nscore = 0\nfinal_cols = []\n\nX_train = df_train.drop(columns = \"loan_status\")\ny_train = df_train[\"loan_status\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    cross_val_scores = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    if cross_val_scores.mean() &gt; score:\n      score = cross_val_scores.mean()\n      final_cols = cols\n\n\nprint(f\"The best model scored {score*100:.3f}% accuracy when testing on training data using: \\n{final_cols}\")    \n\nThe best model scored 84.875% accuracy when testing on training data using: \n['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_percent_income']\n\n\nThis accuracy, although not stellar, should be high enough to hopefully be a good predictor to maximize our profit margins.\nBut how are we going to decide who to give loans to? Great question! We are going to give each loan applicant a score s which predicts their likelihood to default on a loan. Higher scores indicate greater reliability.\n\ndef linear_score(X, w):\n    return X@w\n\nThe above function calculates our linear score as a function of the features X and a weight vector w. We can get this vector of weight by fitting a Linear Regression model as shown below.\n\nLR.fit(X_train[final_cols], y_train)\nLR.coef_\n\narray([[-7.59666254e-01, -9.36504060e-02, -1.80228891e+00,\n         2.75434204e-01, -3.61277124e-03,  8.27700853e+00]])\n\n\n\nw = LR.coef_[0]\ns = linear_score(df_train[final_cols], w)\n\nWe now have s which is a vector of scores for each applicant. We can see the distribution of these scores below.\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nhist = ax.hist(s, bins = 50, color = \"purple\", alpha = 0.6, linewidth = 0.75, edgecolor = \"black\")\nlabs = ax.set(xlabel = r\"Score $s$\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\nAs a reminder, as the bank, we want to use these scores to determine who we should give loans to in order to turn the maximum profit. Thus we need to determine a threshold at which we make the most money and use it as a cutoff.\nBelow you can find some simplified functions for how we determine our profit margins. - The calculate_profit formula is used when a loan is fully repaid and assumes that the profit earned each year by the bank on a 10-year loan is equal to 25% of the interest rate each year, with the other 75% of the interest going into various costs to manage the bank. This corresponds to the True Negative category. - The calculate_loss formula is employed for defaults, it uses the same mechanism as above but assumes that the borrower defaults three years into the loan and that the bank loses 70% of the principal. This corresponds to the False Negative category.\n\ndef calculate_profit(df_train):\n    return (df_train[\"loan_amnt\"]*(1 + 0.25*(df_train[\"loan_int_rate\"]/ 100))**10) - df_train[\"loan_amnt\"]\n\ndef calculate_loss(df_train):\n    return (df_train[\"loan_amnt\"]*(1 + 0.25*(df_train[\"loan_int_rate\"]/ 100))**3) - (1.7*df_train[\"loan_amnt\"])\n\nBelow, we are testing threshold values between -4 and 4 and using our formulas to calculate our net gain at each interval. Our best threshold is the one that corresponds to the highest net gain.\n\nbest_profit = 0\nbest_threshold = 0\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nfor t in np.linspace(-4, 4, 101): \n    y_pred = s &gt;= t\n    tn = ((y_pred == 0) & (y_train == 0))\n    fn = ((y_pred == 0) & (y_train == 1))\n\n    gain = calculate_profit(df_train[tn]).sum() + calculate_loss(df_train[fn]).sum()\n    gain /= len(df_train)\n    ax.scatter(t, gain, color = \"steelblue\", s = 10)\n    if gain &gt; best_profit: \n        best_profit = gain\n        best_threshold = t\n\n\nax.axvline(best_threshold, linestyle = \"-.\", color = \"grey\", zorder = -10)\nlabs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Net benefit\", title = f\"Best benefit ${best_profit:.2f} at best threshold t = {best_threshold:.3f}\")\n\n\n\n\n\n\n\n\nOur net gain per person when we only give loans to people with a risk score of 2.64 or above is $1443.81 per person! Lets see if this threshold works on data the model has never seen before…\n\n\nBank Evaluation\nHere we are importing the test data set and applying the same preprocessing steps that we used on the training data.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\ndf_test[\"cb_person_default_on_file\"] = le.transform(df_test[\"cb_person_default_on_file\"])\ndf_test = df_test.dropna()\ndf_test = pd.get_dummies(df_test)\n# df_test[all_quant_cols] = scaler.fit_transform(df_test[all_quant_cols])\n\nX_test = df_test.drop(columns = \"loan_status\")\ny_test = df_test[\"loan_status\"]\n\nWe once again calculate a net gain per person score…\n\ntest_scores = linear_score(df_test[final_cols], w)\ny_pred = test_scores &gt; best_threshold\n\ntest_tn = ((y_pred == 0) & (y_test == 0))\ntest_fn = ((y_pred == 0) & (y_test == 1))\n\ntest_gain = calculate_profit(df_test.loc[test_tn]).sum() + calculate_loss(df_test.loc[test_fn]).sum()\ntest_gain /= len(df_test)\n\n\nprint(f\"The net benefit on the test set is ${test_gain:.2f}\")\n\nThe net benefit on the test set is $1384.50\n\n\nOur model did not perform as well on the test data… however, we are still making quite a bit of profit per person from the bank!\n\n\nBorrower’s Evaluation\nThis is all fine and dandy for the bank… make money = bank happy… but what about the borrowers?\nWe are adding columns to our data frame that show each borrowers score and our models prediction.\n\ndf_test[\"person_risk\"] = test_scores\ndf_test[\"loan_status_pred\"] = y_pred\n\n\n(df_test[\"loan_status_pred\"] == df_test[\"loan_status\"]).mean()\n\nnp.float64(0.8474960739835979)\n\n\nOur model accurately predicted who would default on their loan 84.75% of the time. So the majority of people who would have defaulted on their loans were accurately denied… but who exactly are the people?\n\nAge Group Disparity\nLets break down how the model treats people of a certain age groups.\n\nsns.kdeplot(data = df_test, x = \"person_age\", y = \"person_risk\", hue = \"loan_status_pred\", alpha = 0.75, fill = True, bw_adjust=0.7)\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data = df_test, x = \"person_age\", y = \"person_risk\", hue = \"loan_status_pred\")\n\n\n\n\n\n\n\n\nWe can see that most of the people who are receiving loans are under the age age of 40, with a majority falling between 20-30 years old. This makes sense as many of our applicants fall into this age range. This may be a little unfair for persons aged over 40…\n\n\nMedical Loans\nHow about people who are applying for medical loans. I highly important demographic of borrowers whose health and livelihoods may be contingent on access to a loan. What fraction of the test dataset consists of medical loans that were approved?\n\napp_med_loans = (df_test[\"loan_intent_MEDICAL\"] == True) & (df_test[\"loan_status_pred\"] == False)\nprint(f\"Only {app_med_loans.mean()*100:.2f}% of people applying for medical loans were approved\")\n\nOnly 16.66% of people applying for medical loans were approved\n\n\n\ndef_rate_med_loans = (df_test[\"loan_intent_MEDICAL\"] == True) & (df_test[\"loan_status\"] == 1)\nprint(f\"While only {def_rate_med_loans.mean()*100:.2f}% actually defaulted on their medical loans\")\n\nWhile only 5.32% actually defaulted on their medical loans\n\n\nThis is quite harrowing… operating on a purely profit based system is taking loans away from people who may really need this money and would not have defaulted! We will dive into the fairness of this decision later…\n\n\nEducational Loans\nLets see our model treats people applying for educational loans.\n\napp_edu_loans = (df_test[\"loan_intent_EDUCATION\"] == True) & (df_test[\"loan_status_pred\"] == False)\nprint(f\"Only {app_edu_loans.mean()*100:.2f}% of people applying for education loans were approved\")\n\nOnly 18.71% of people applying for education loans were approved\n\n\n\ndef_rate_edu_loans = (df_test[\"loan_intent_EDUCATION\"] == True) & (df_test[\"loan_status\"] == 1)\nprint(f\"While only {def_rate_edu_loans.mean()*100:.2f}% actually defaulted on their loans\")\n\nWhile only 3.44% actually defaulted on their loans\n\n\nOnce again we are denying loans to a huge amount of people who in reality paid off their loans.\n\n\nBusiness Venture Loans\n\napp_bus_loans = (df_test[\"loan_intent_VENTURE\"] == True) & (df_test[\"loan_status_pred\"] == False)\nprint(f\"Only {app_bus_loans.mean()*100:.2f}% of people applying for business venture loans were approved\")\n\nOnly 15.42% of people applying for business venture loans were approved\n\n\n\ndef_rate_bus_loans = (df_test[\"loan_intent_VENTURE\"] == True) & (df_test[\"loan_status\"] == 1)\nprint(f\"While only {def_rate_bus_loans.mean()*100:.2f}% actually defaulted on their loans\")\n\nWhile only 2.46% actually defaulted on their loans\n\n\nOnce again we denied people who would have paid back their loans!\n\n\n\nFairness Discussion\nWe observed some trends in our model that seemed pretty unfair, people who would have paid off their loans were denied… but what exactly does it mean to be fair? Fairness should be making a model that makes decisions without bias towards certain groups, especially dealing with sensitive features like race, gender, or socioeconomic status. Fairness also constitutes taking into account features that are relevant to the situation. At face value our model is treating people pretty harshly across the board with regards to the intent of their loan. We do however consider people age in the model, and we do seem to have a slight bias towards younger people… But in the medical loan situation, doesn’t it seem unfair that people are being denied. doesn’t the severity / intent behind the loan seem like an important and relevant factor? In a sense we are being biased towards people who need the money… people who are ill should be treated with extra care and that should be a relevant factor in where we draw our threshold and how we train our model.\n\n\nConclusion\nOur analysis revealed that automated decision system can be beneficial to a bank’s overall profitability by approving loans only when the expected risk-adjusted returns are maximized. However, the findings also reveal the importance of continuously monitoring the system’s performance to ensure that it does not inadvertently disadvantage certain segments of the population. We observed that borrowers applying for loans of high importance, such as medical loans were drastically undeserved. In addition, older generation wer not given access to credit all in the name of profit. Ultimately, this exploration highlights the need for a balanced approach that integrates profit maximization with fairness and inclusivity taking into account other factors that are relevant to the lives of borrowers."
  },
  {
    "objectID": "posts/Implementing the Perceptron Algorithm/index.html",
    "href": "posts/Implementing the Perceptron Algorithm/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nAbstract\nThis blog post explores the implementation of the perceptron algorithm in Python exploring both the standard and mini-batch versions of perceptron. We demonstrate how the algorithm iteratively adjusts the weight vector to reduce misclassifications by updating based on individual data points or averaged contributions from mini-batches. We discuss how and why our implementation works, demonstrate its capacities, and discuss parameter tuning and time complexity. Notably, we show the performance of the algorithm on linearly separable and non-linearly separable data. Through this post, I expanded my understanding of how basic learning algorithms work behind the scenes for classification purposes.\n\n\nImplementation\nIn perceptron.py, we implement the perceptron algorithm. The .score(), .loss() and .step() methods are pretty intuitive. The .grad() (Gradient Descent) I will address in further depth. The goal of perceptron is to adjust our weights w so that we push our line in the right direction to minimize our loss function. In other words, push our line in a direction that reduces the number of points we misclassify at each update. We do so with respect to a single point. We randomly select a point (or a set of points in mini-batch then average them) then see if it is misclassified, we push our weights w closer to classifying that point correctly. This push can be scaled the the learning rate, which is 1 by default, smaller values will make smaller jumps.\nSome important highlights in the code are:\n\nself.score(X) * y_ computes a score for a given point that is positive when we accurately classify a point\n(( ... &lt; 0 ) * y_) is an indicator random variable that will only add to our weights when we find a misclassified point, otherwise our binary encoding will nullify the rest of the term.\n.unsqueeze(1) * X Allows for us to use mini-batches and not encounter size broadcasting errors with matrices and vectors in PyTorch.\n\nOur actual update happens in the .step() method completing the implementation.\nFor the first part of the analysis we will be using a default batch size of k = 1 and learning rate of 1 which is the default perceptron algorithm as opposed to mini-batch perceptron that we will explore later.\nBelow we are using Professor Chodrow’s functions to generate and visualize linearly separable data to test our implementation.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nAs we can observe we have generated a set of linearly separable data that our perceptron should be able to separate with 100% accuracy (equivalent to 0 loss)\n\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y)\n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nloss\n\ntensor(0.)\n\n\nGreat! It worked, we have a perceptron that minimizes our loss to zero! We can see below the step by step process as the perceptron algorithm iteratively finds the line that separates the squares and the circles.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nHere is the line we found:\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n\n\n\n\n\n\n\n\n\n\nExperiments\nLet us delve into how we arrived to this line iteratively on two dimensional data. First, we are going to have a look at a situation where we know our data is linearly separable. We are going to explore this with a slightly modified function from the lecture notes. This skeleton implements the iterative aspect of perceptron. We loop our code until we have reached a loss score of 0 (separated the two clusters). Within each loop, we choose a point at random and find the loss score of that point given our current line. Then depending on whether or not we correctly classified it using our current line, we shift the line in the direction that would lead to correctly classifying that point. We do this, again, until our overall loss is zero.\n\ntorch.manual_seed(3141)\nX, y = perceptron_data(n_points=50, noise=0.3)\nn = X.shape[0]\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nplot_data = []\nloss_vec = []\n\nloss = 1\nwhile loss &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n        plot_data.append((old_w, torch.clone(p.w), i, loss))\n\n\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex=True, sharey=True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1: 0, 1: 1}\n\nplot_data = plot_data[-6:]\n\nfor current_ax, (old_w, new_w, i, loss) in enumerate(plot_data):\n    ax = axarr.ravel()[current_ax]\n    plot_perceptron_data(X, y, ax)\n    draw_line(old_w, x_min=-1, x_max=2, ax=ax, color=\"black\", linestyle=\"dashed\")\n    draw_line(new_w, x_min=-1, x_max=2, ax=ax, color=\"black\")\n    ax.scatter(X[i, 0], X[i, 1], color=\"black\", facecolors=\"none\", edgecolors=\"black\", \n               marker=markers[marker_map[2 * (y[i].item()) - 1]])\n    ax.set_title(f\"loss = {loss:.3f}\")\n    ax.set(xlim=(-1, 2), ylim=(-1, 2))\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe above figure shows the last six iterations of the perceptron as it is adjusts the weights based on local loss on data that we know is linearly separable. What is interesting to not here is that the line is not narrowly converging towards a loss of zero but rather, adjusts for each point without respect to the whole. The result of this method is that our loss seemingly jumps around in directions that may seem counter intuitive until we reach our loss of zero and terminate.\nThe graph below illustrates this quite well:\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThis is all nice and dandy when we have access to linearly separable data, but what about when our data is not linearly separable. In such a case, it is impossible to have a loss of zero. So we will implement a maximum number of iterations so that our code doesn’t run forever to no avail.\n\ntorch.manual_seed(124816)\nX, y = perceptron_data(n_points=100, noise=0.8)\nn = X.shape[0]\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nmax_iter = 1000 # Maximum number of iterations\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min=-2, x_max=3, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-2, 3), ylim=(-2, 3))\nplt.show()\n\n\n\n\n\n\n\n\nIn the figure above we can observe first that our data is not linearly separable, and thus even after 1000 iterations we achieved a loss score that is not terrible, but not zero.\nBelow we can see the evolution of the loss over these iterations, and note that there is not point with loss zero.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nplt.ylim(0, 0.55)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nWith non linearly separable data we can also see that in some cases our loss can be above 50% misclassified for certain lines. In addition, our final iteration was not a representation of the best we could get, rather where we were after 1000 iterations. We can see that in earlier updates we achieved closer to 15% misclassified.\nThus far we have only been working with two dimensional data, however the perceptron algorithm works with higher dimensional data! Below we are going to run the algorithm on a data set with seven dimensions and observe the evolution of the loss as we iterate.\n\ntorch.manual_seed(2003)\nX, y = perceptron_data(n_points=100, noise=1.72, p_dims=7)\nn = X.shape[0]\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nmax_iter = 1000 # Maximum number of iterations\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nplt.ylim(0, max(loss_vec) + 0.05)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nWe can see that over 1000 iterations our loss never reached zero. We can also note somewhat of a plateau of points around 0.25 that we would lead me to believe that the data is not linearly separable. We can have linearly separable data in seven dimensions as illustrated below. I chose seven because I like the number 7…\n\ntorch.manual_seed(2003)\nX, y = perceptron_data(n_points=100, noise=0.5, p_dims=7)\nn = X.shape[0]\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nmax_iter = 1000\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nplt.ylim(0, max(loss_vec) + 0.05)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAs we can see, in the end we do achieve a loss score of zero indicating we can accurately separate and classify the different sorts of points with a hyperplane of sorts in \\(\\mathbb{R}^7\\). This data was achieved by simply turning down the noise parameter when we generate points.\n\n\nMinibatch Perceptron\nHere we implement the mini-batch perceptron algorithm that computes each update using \\(k\\) random points at once.\nWe begin with a k = 1 that performs similar to regular perceptron.\n\ntorch.manual_seed(6791)\nX, y = perceptron_data(n_points=100, noise=0.25)\nn = X.shape[0]\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nk=1\n\nmax_iter = 1000\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i, k)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min=-2, x_max=3, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-2, 3), ylim=(-2, 3))\nplt.show()\n\n\n\n\n\n\n\n\nWe separate the two clusters just as we would in regular perceptron, and achieve a zero loss score.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nNext we with use k = 10 that can still find a separating line in two dimensions on the same data.\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nk=10\n\nmax_iter = 1000\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i, k)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min=-2, x_max=3, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-2, 3), ylim=(-2, 3))\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nGiven the same data set having a batch size of k = 10 increased the number of iterations we needed to find the separating line.\nWhat will our results be when the batch size is our entire dataset? That is k = n where n = 100. Lets first try it on the same dataset.\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nk=n\n\nmax_iter = 1000\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i, k)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min=-2, x_max=3, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-2, 3), ylim=(-2, 3))\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nWe can see that in this case we are able to find the separating line much faster!\nNow lets see what we can learn from using mini-batch perceptron on data that is not linearly separable. For this we will also tune our learning rate to be 0.01 so we take smaller steps at each iteration.\n\ntorch.manual_seed(1017)\nX, y = perceptron_data(n_points=200, noise=0.7)\nn = X.shape[0]\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nk=n\nlearning_rate = 0.01\n\nmax_iter = 1000\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i, k, learning_rate)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min=-2, x_max=3, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-2, 3), ylim=(-2, 3))\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nWe observe that even with non linearly separable data that the algorithm is still able to converge as we take smaller steps towards a line that fits pretty well!\n\n\nDiscussion\nThe runtime complexity of an iteration of the perceptron algorithm is O(p). This comes from doing the dot product in our .score() function on a matrix that is size \\(n \\times p\\), where \\(n\\) is the number of points and \\(p\\) is the number of features. In addition, updating our weights w is the same complexity. This is independent of the number of points \\(n\\).\nBased on this, when we do minibatch perceptron, we are doing k dot products which would give us O(kp). In some cases this might depend on \\(n\\) as it did in our final example where we set k = n. So while mini-batch helps us with non linearly separable data, it comes at the expense of increased time complexity.\n\n\nConclusion\nIn this post, we implemented the perceptron algorithm and discussed it’s inner workings. We also explored the benefits of mini-batch processing that although it’s per-iteration complexity is O(kp), can improve convergence characteristics for non linearly separable data. Our experiments on both linearly separable and non-linearly separable datasets highlighted the importance of tuning parameter like batch size and learning rate. Additionally, we showed how perceptron can accurately classify high dimensional data through our experimentation with testing on seven dimensional data. Overall, these findings gave us the conceptual foundations for linear classification and provided a practical roadmap for applying and optimizing perceptron-based models."
  }
]
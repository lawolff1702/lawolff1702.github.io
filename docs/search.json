[
  {
    "objectID": "posts/Sparse Kernel Machines/index.html",
    "href": "posts/Sparse Kernel Machines/index.html",
    "title": "Sparse Kernel Machines",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom kernel import KernelLogisticRegression\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nHere is the link to my implementation of sparse ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍kernelized ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍logistic ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍regression (kernel.py).\n\nAbstract\n– insert here —\n\n\nModel"
  },
  {
    "objectID": "posts/Implementing Logistic Regression/index.html",
    "href": "posts/Implementing Logistic Regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nMy implementation of the logistic regression algorithm (logistic.py) can be found here.\n\nAbstract\nIn this blog post we will explore the implementation of logistic regression and focus on optimizing the binary cross-entropy loss through gradient descent with momentum. We will validate and explore the model we implemented through several experiments on synthetic datasets. We will observe both vanilla gradient descent and momentum-enhanced gradient descent and hwo the latter can improve the speed of our models. We will also examine the challenges of overfitting by working with high-dimensional data sets. Finally, we will apply our logistic regression model to predict FIFA World Cup match outcomes using real-world statistics and highlight the challenges and benefits therein. Through these experiments, we illustrate the advantages and trade-offs between convergence efficiency and model generalizability, and offer some thoughts on practical classification tasks and possible future work.\n\n\nImplementation\nThe logistic regression model I implemented minimizes the binary cross-entropy loss to classify data. During training, we use momentum to help the model move more quickly in the right direction by combining the current gradient with a fraction of the previous update as we try search the loss-space for a minimum. The momentum should help reduce oscillations and speed up convergence! Lets see how it works!\n\n\nExploration\nWe will start with vanilla gradient descent. We will be working with two-dimensional data (x1, x2) and setting our \\(\\beta\\) value to zero to nullify the momentum term.\nThe code below provides functions to generate and plot data for a classification problem that we can address using our model. To test our vanilla model, lets generate some linearly separable data!\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# torch.manual_seed(67)\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\ndef plot_lr_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"PRGn\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = classification_data(noise = 0.2, p_dims = 2)\nplot_lr_data(X, y, ax)\n\n\n\n\n\n\n\n\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec_vanilla = []\nmax_iter = 1000\n\nfor _ in range(max_iter): \n    loss = LR.loss(X, y)\n    loss_vec_vanilla.append(loss)\n\n    opt.step(X, y, alpha=0.45, beta=0)\n\nloss\n\ntensor(0.0140)\n\n\nThats great! Looks like we were able to separate the data by achieving a minimal loss! Lets take a look at the line that separates our points.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_lr_data(X, y, ax)\ndraw_line(LR.w, x_min = -0.5, x_max = 1.5, ax = ax, color = \"slategrey\")\n\n\n\n\n\n\n\n\nAs seen above, our line very handily separates the two classes of points. Lets take a look at how our loss evolved over time by plotting our loss at each step.\n\nplt.plot(loss_vec_vanilla, color = \"purple\", lw=2)\n# plt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"purple\")\nlabs = plt.gca().set(xlabel = \"Logistic Regression Iteration\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nIt is interesting to note that our Logistic Regression is not jumping around as we observed with the Perceptron algorithm in the previous blog post. On the contrary, using gradient descent on a convex loss function, we are slowly advancing towards the minimum of the function.\nNow lets look at how momentum can help us! We will use the same dataset, however, this time we will instantiate a model with a \\(\\beta=0.9\\).\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec_momentum = []\n\nfor _ in range(max_iter): \n    loss = LR.loss(X, y)\n    loss_vec_momentum.append(loss)\n\n    opt.step(X, y, alpha=0.45, beta=0.9)\n\nloss\n\ntensor(0.0020)\n\n\nLets see how these processes stack up against each other. For the graph we are\n\nnum_iter = 500\nplt.plot(loss_vec_vanilla[:num_iter], color = \"purple\", lw=2, label = \"vanilla\")\nplt.plot(loss_vec_momentum[:num_iter], color = \"blue\", lw=2, label = \"momentum\")\nlabs = plt.gca().set(xlabel = \"Logistic Regression Iteration\", ylabel = \"loss\", title = f\"Momentum vs. Vanilla Gradient Descent (first {num_iter} iterations)\")\nplt.legend()\n\n\n\n\n\n\n\n\nAs we can see, including a momentum factor helps the model narrow in on the minimum loss a lot faster! When we use vanilla, we are updating our weight vector only using teh current gradient. However, with momentum we are also considering the direction of the previous update, so we build up momentum when we are going the right direction which helps us find the minimum faster.\nNext, we are going to take a look at some issues pertaining to overfitting our model. We are going to generate data where the the number of features is higher than the number of points we have, p_dim &gt; n_points. To illustrate the issues of overfitting, we will generate separate testing and validation data sets, then compare their accuracy.\n\nX_train, y_train = classification_data(noise = 1.2, p_dims = 100, n_points = 50)\nX_test, y_test = classification_data(noise = 1.2, p_dims = 100, n_points = 50)\n\nHere is a small helper function to calculate our accuracy.\n\ndef acc(model, X, y):\n    y_hat = model.predict(X)\n    return (y_hat == y).float().mean().item()\n\nNow lets train a model that achieves 100% training accuracy!\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\ntrain_loss = []\ntest_loss = []\n\nfor _ in range(50):\n    train_loss.append(LR.loss(X_train, y_train))\n    test_loss.append(LR.loss(X_test, y_test))\n\n    opt.step(X_train, y_train, alpha=0.5, beta=0.9)\n    \ntrain_acc = acc(LR, X_train, y_train)\nprint(f\"Training Accuracy: {train_acc * 100:.2f}% Training Loss: {LR.loss(X_train, y_train):.4f}\")\n\nTraining Accuracy: 100.00% Training Loss: 0.0000\n\n\nVoila! This seems amazing! We have 100% accuracy and a very low loss! However, our testing data may not ignite the same happiness within us.\n\ntest_acc = acc(LR, X_test, y_test)\nprint(f\"Test Accuracy: {test_acc * 100:.2f}% Test Loss: {LR.loss(X_test, y_test):.4f}\")\n\nTest Accuracy: 88.00% Test Loss: 1.1519\n\n\nYouch. This is a lot lower accuracy that we would have liked. In addition, the testing loss is quite high! Lets have a look at how our model performs at each iteration.\n\nplt.plot(train_loss, color = \"green\", lw=2, label = \"Training\")\nplt.plot(test_loss, color = \"orange\", lw=2, label = \"Testing\")\nlabs = plt.gca().set(xlabel = \"Logistic Regression Iteration\", ylabel = \"loss\", title = f\"Training vs Testing Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nWe can see our training loss decrease and converge to zero, however, at a certain point our testing loss that was once decreasing too, begins to increase! The model continues to learn (and thus overfit) the training data, even as it starts to perform worse on generalizing to new data. When we have a higher number of features than we do samples, overfitting can become problematic as it did above.\n\n\nPredicting World Cup Match Winners\nTo illustrate our Logistic Regression model on some empirical data, we are going to use World Cup Match Data and try and predict match winners! The dataset we are using was compiled on Kaggle by Brenda Loznik in 2022 from publicly available FIFA World Cup data. Players were manually classified as either goalkeeper, defender, midfielder, or offensive player. For each team, only the top-performing players were selected. These plater statistics were used to create the power scores for each team. In some cases, data is missing — this indicates that a country did not have enough qualifying players to met the selection criteria. Additionally, the dataset assumes that each season runs from September to the following August.\n\nimport pandas as pd\ndf = pd.read_csv('international_matches.csv')\ndf.head()\n\n\n\n\n\n\n\n\ndate\nhome_team\naway_team\nhome_team_continent\naway_team_continent\nhome_team_fifa_rank\naway_team_fifa_rank\nhome_team_total_fifa_points\naway_team_total_fifa_points\nhome_team_score\n...\nshoot_out\nhome_team_result\nhome_team_goalkeeper_score\naway_team_goalkeeper_score\nhome_team_mean_defense_score\nhome_team_mean_offense_score\nhome_team_mean_midfield_score\naway_team_mean_defense_score\naway_team_mean_offense_score\naway_team_mean_midfield_score\n\n\n\n\n0\n1993-08-08\nBolivia\nUruguay\nSouth America\nSouth America\n59\n22\n0\n0\n3\n...\nNo\nWin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1993-08-08\nBrazil\nMexico\nSouth America\nNorth America\n8\n14\n0\n0\n1\n...\nNo\nDraw\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1993-08-08\nEcuador\nVenezuela\nSouth America\nSouth America\n35\n94\n0\n0\n5\n...\nNo\nWin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1993-08-08\nGuinea\nSierra Leone\nAfrica\nAfrica\n65\n86\n0\n0\n1\n...\nNo\nWin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n1993-08-08\nParaguay\nArgentina\nSouth America\nSouth America\n67\n5\n0\n0\n1\n...\nNo\nLose\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 25 columns\n\n\n\nBelow are the training features we will be using, I selected mainly score based features like rank, points, and positional scores because I thought they would work best.\n\nfeatures = [\n    'home_team_fifa_rank',\n    'away_team_fifa_rank',\n    'home_team_total_fifa_points',\n    'away_team_total_fifa_points',\n    'home_team_goalkeeper_score',\n    'home_team_mean_defense_score',\n    'home_team_mean_offense_score',\n    'home_team_mean_midfield_score',\n    'away_team_goalkeeper_score',\n    'away_team_mean_defense_score',\n    'away_team_mean_offense_score',\n    'away_team_mean_midfield_score'\n]\n\nHere is a first look at our raw data. Lets do some quick preprocessing to clean the dataset to make sure we have data in all fields, create a target vector, etc.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ndf_wc = df[df['tournament'].str.contains(\"World Cup\", case=False)]\ndf_wc_clean = df_wc.dropna()\ndf_wc_clean = df_wc_clean[df_wc_clean['home_team_result'] != 'Draw']\ndf_wc_clean = df_wc_clean[df_wc_clean['shoot_out'] == 'No']\nle.fit(df_wc_clean[\"home_team_result\"])\ndf_wc_clean[\"home_team_result\"] = le.transform(df_wc_clean[\"home_team_result\"])\n\nX_features = torch.tensor(df_wc_clean[features].values, dtype=torch.float32)\ny_tensor = torch.tensor(df_wc_clean[\"home_team_result\"].values, dtype=torch.float32)\n\n# Add bias term\nX_tensor = torch.cat((X_features, torch.ones((X_features.shape[0], 1))), 1)\n\nX_train_tensor, X_temp_tensor, y_train_tensor, y_temp_tensor = train_test_split(X_tensor, y_tensor, test_size=0.4, random_state=72, stratify=y_tensor)\nX_val_tensor, X_test_tensor, y_val_tensor, y_test_tensor = train_test_split(X_temp_tensor, y_temp_tensor, test_size=0.5, random_state=72, stratify=y_temp_tensor)\n\nI selected only World Cup Matches, dropped the matches with missing data, dropped all the matches that ended in a draw or went to penalties, and made a binary encoding for who won the match. Finally I created training, validation, and testing split. Our target value is home_team_result.\nLets go ahead and train our model! Lets first do without momentum then with momentum!\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nmax_iter = 30000\na = 0.00001\n\ntrain_loss = []\nval_loss = []\n\nfor _ in range(max_iter):\n    train_loss.append(LR.loss(X_train_tensor, y_train_tensor))\n    val_loss.append(LR.loss(X_val_tensor, y_val_tensor))\n\n    opt.step(X_train_tensor, y_train_tensor, alpha=a, beta=0)\n    \ntrain_acc = acc(LR, X_train_tensor, y_train_tensor)\nval_acc = acc(LR, X_val_tensor, y_val_tensor)\nprint(f\"Training Accuracy: {train_acc * 100:.2f}% Training Loss: {LR.loss(X_train_tensor, y_train_tensor):.4f}\")\nprint(f\"Validation Accuracy: {val_acc * 100:.2f}% Validation Loss: {LR.loss(X_val_tensor, y_val_tensor):.4f}\")\n\nTraining Accuracy: 70.46% Training Loss: 0.7281\nValidation Accuracy: 80.22% Validation Loss: 0.6169\n\n\nThis is pretty good considering the nature of football! After some serious experimentation and terrible results of tuning the hyper parameters, I landed on quite a low \\(\\alpha\\) learning rate and turned the number of iterations up quite high to make sure our model had more time to train and get a lower loss level. At first I was testing with \\(\\alpha = 0.1\\) with 1000 iterations. Interestingly, my accuracy stayed about the same ~70%, however, the loss curve was quite jagged and oscillated in the region of 4.5-6 — a great indication that we needed to take smaller steps and let the model train for longer! After tuning, I was sufficiently happy with the loss being sub-one despite not being closer to zero. Lets see how that worked on our test data!\n\ntest_acc = acc(LR, X_test_tensor, y_test_tensor)\nprint(f\"Test Accuracy: {test_acc * 100:.2f}% Test Loss: {LR.loss(X_test_tensor, y_test_tensor):.4f}\")\n\nTest Accuracy: 74.18% Test Loss: 0.6130\n\n\nWe are still performing similarly on unseen data! That is a great sign that our model wasn’t overfitting. Let’s explore how using momentum will affect the model.\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\ntrain_loss_m = []\nval_loss_m = []\n\nfor _ in range(max_iter):\n    train_loss_m.append(LR.loss(X_train_tensor, y_train_tensor))\n    val_loss_m.append(LR.loss(X_val_tensor, y_val_tensor))\n\n    opt.step(X_train_tensor, y_train_tensor, alpha=a, beta=0.9)\n    \ntrain_acc = acc(LR, X_train_tensor, y_train_tensor)\nval_acc = acc(LR, X_val_tensor, y_val_tensor)\ntest_acc = acc(LR, X_test_tensor, y_test_tensor)\n\nprint(f\"Training Accuracy: {train_acc * 100:.2f}% Training Loss: {LR.loss(X_train_tensor, y_train_tensor):.4f}\")\nprint(f\"Validation Accuracy: {val_acc * 100:.2f}% Validation Loss: {LR.loss(X_val_tensor, y_val_tensor):.4f}\")\nprint(f\"Test Accuracy: {test_acc * 100:.2f}% Test Loss: {LR.loss(X_test_tensor, y_test_tensor):.4f}\")\n\nTraining Accuracy: 75.41% Training Loss: 0.5183\nValidation Accuracy: 79.12% Validation Loss: 0.4741\nTest Accuracy: 73.63% Test Loss: 0.4944\n\n\nUsing momentum seemed to help slightly on our training and validation, however, given the drop off in testing accuracy, this may be a result of overfitting over the high number of iterations. Lets actually take a look at the evolution of our loss to see how the training process evolved and how momentum may have made an impact.\n\nfirst = 500\nfig, axes = plt.subplots(1, 2, figsize=(14, 6)) \n\naxes[0].plot(train_loss, \"--\", color=\"brown\", lw=2, label=\"Training\", alpha=0.5)\naxes[0].plot(val_loss, \"--\", color=\"orange\", lw=2, label=\"Validation\", alpha=0.5)\naxes[0].plot(train_loss_m, color=\"blue\", lw=2, label=\"Momentum Training\", alpha=0.5)\naxes[0].plot(val_loss_m, color=\"purple\", lw=2, label=\"Momentum Validation\", alpha=0.5)\naxes[0].set_title(\"Training vs Validation Loss (All Iterations)\")\naxes[0].set_xlabel(\"Logistic Regression Iteration\")\naxes[0].set_ylabel(\"Loss\")\naxes[0].legend()\n\naxes[1].plot(train_loss[:first], \"--\", color=\"brown\", lw=2, label=\"Training\", alpha=0.5)\naxes[1].plot(val_loss[:first], \"--\", color=\"orange\", lw=2, label=\"Validation\", alpha=0.5)\naxes[1].plot(train_loss_m[:first], color=\"blue\", lw=2, label=\"Momentum Training\", alpha=0.5)\naxes[1].plot(val_loss_m[:first], color=\"purple\", lw=2, label=\"Momentum Validation\", alpha=0.5)\naxes[1].set_title(f\"Training vs Validation Loss (First {first} Iterations)\")\naxes[1].set_xlabel(\"Logistic Regression Iteration\")\naxes[1].set_ylabel(\"Loss\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nMomentum definitely had an effect on the convergence speed of the model! At around 1000 and definitely by 5000 iterations we see that the loss has essentially plateaued. On the other hand, our vanilla logistic regression that only really got there around 6000 and could keep training even after 30000 iterations. I found the first 500 iterations especially fascinating. We can see that our momentum may have sent us too far in the wrong direction at times, causing the loss to jump up before eventually steadying out and beginning to trend in the right direction. With respect to this issue, it would be interesting to implement variable learning rates throughout the process, possibly starting very small and as we begin to movement in the right direction we can increase the step size (similar to how momentum works) and do the inverse when we go in teh wrong direction. We could also think about variable momentum coefficients that work in a similar fashion. One interesting factor to observe is that our validation loss was consistently lower that our training loss which I honestly cannot explain for the moment apart from luck with the validation set. Nonetheless, we can observe that momentum was a significant optimizing factor in training our model.\n\n\nDiscussion\nIn this post, we implemented a logistic regression model and conducted a series of experiments to better understand its behavior. We started off using a simple two-dimensional dataset to compare vanilla gradient descent with momentum aided descent. We demonstrated that momentum can smooth the convergence process and accelerate progress in a clean, linearly separable context. We then tackled the issue of overfitting by experimenting with scenarios where the number of features exceeded the number of data points, highlighting some of the pitfalls of excessive model complexity. Finally, we applied the model to predict World Cup match winners using real match data. With very small learning rates and high iteration numbers we were able to achieve decent accuracy and loss. In the real-world scenario we still observed the benefits of introducing momentum as our model converged a lot faster using momentum. In conclusion, these experiments deepened my understanding of model implementation and optimization, the balance required to prevent the dangers of overfitting, and challenges applying logistic regression to real-world applications. We also outlined some further optimizations concerning variable parameters that we could implement in the future."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Abstract\nIn this study, we set out to design a data-driven system for automating credit decisions at a hypothetical bank. Our objective was to create a score function that captures the balance between potential revenue and risk, using predictive models trained on historical data to estimate the likelihood of loan repayment. Our primary drive as the hypothetical bank was to maximize profit, however, we observe that this choice has potentially negative impacts on our diverse borrowers.\n\n\nData\nWe are diving into a Credit Risk Dataset that simulates credit bureau data.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\nHere is a first look at the raw data:\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nI want to highlight several important features of this dataset.\n\nloan_percent_income is ratio of the loan amount to the individual’s income\nloan_int_rate is the annual interest rate on the loan.\nloan_status tells us whether or not the individual defaulted on their loan. This a a binary feature where 1 indicates the individual defaulted, and 0 indicates the loan was repaid in full. This is our Target Variable.\n\nLets have a look at how common defaulting is in our training data:\n\ndf_train[\"loan_status\"].value_counts(normalize=True)\n\nloan_status\n0    0.78242\n1    0.21758\nName: proportion, dtype: float64\n\n\nIn the dataset, around 21% of borrowers default on their loan. This is going to be the our base rate for prediction.\n\ndf_train[\"person_age\"].describe()\n\ncount    26064.000000\nmean        27.734385\nstd          6.362612\nmin         20.000000\n25%         23.000000\n50%         26.000000\n75%         30.000000\nmax        144.000000\nName: person_age, dtype: float64\n\n\nThere seems to be some slight errors in our data with the age of certain individuals. 144, although impressive, is highly unlikely. Thus, without context for why the data has such an outlier, I am going to filter our data to exclude persons over 100 years old. In addition, I will do some other basic data cleaning like removing NaN entries, encoding qualitative features, etc.\n\ndf_train = df_train[df_train[\"person_age\"] &lt; 100]\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(df_train[\"cb_person_default_on_file\"])\n\ndf_train[\"cb_person_default_on_file\"] = le.transform(df_train[\"cb_person_default_on_file\"])\n\ndf_train = df_train.dropna()\n\nNow our data set looks like this!\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\n1\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\n0\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\n0\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\n0\n10\n\n\n6\n21\n21700\nRENT\n2.0\nHOMEIMPROVEMENT\nD\n5500\n14.91\n1\n0.25\n0\n2\n\n\n\n\n\n\n\n\n\nExploration\nLet’s dive a bit into our data to see if we can pick up on any interesting trends.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(style=\"whitegrid\")\n\n\nsns.displot(data=df_train, x=\"loan_percent_income\", hue=\"loan_status\", kind=\"kde\", bw_adjust=0.9, fill=True, alpha=0.25)\n\n\n\n\n\n\n\n\nThe above figure uses Kernel Density Estimation (KDE), a technique that helps visualize the distribution of data points. In this case we are examining how loan_percent_income is distributed in our data set and its possible relation to defaulting on your loan. We can see here that there is a high density of people who repaid their loans where the loan amount factored between 10-20% of their annual income. This may point to the fact that 10-20% of ones income is a manageable amount, and where we see a spike in 1 around 35%+ may be an rate that is hard to pay off and thus lead to defaults. It is also notable that not many people are requesting loans that are 30%+ of their annual income.\n\nsns.kdeplot(data=df_train, x=\"cb_person_cred_hist_length\", y=\"loan_int_rate\", cmap=\"rocket\", fill=True)\n\n\n\n\n\n\n\n\nThis is another KDE plot that shows the relation in loan interest rates given by the bank and an individuals credit history length. We can observe that there is a high density of people who have a credit history length between 0-5 years. Within this rage there are two hot spots for the interest rates on their loans, around 7.5% and around 11% interest. So although a large amount of people have relatively short credit histories, the bank are differentiating their offers based on other factors.\n\nsns.catplot(data=df_train, kind=\"bar\", x=\"person_home_ownership\", y=\"loan_percent_income\", hue=\"loan_status\")\n\n\n\n\n\n\n\n\nThis figure really highlights the importance of loan_percent _income as a possible meaningful predictor of who will default. We see that across the board, regardless of home ownership type, those who repaid their loans requested loans that were around 15% of their annual income. However, those who defaulted on their loans were consistently requesting loans that were a much higher rate (ranging from ~17%-35%).\nLet’s examine how the connection between age, credit history length, and employment length.\n\nimport numpy as np\n\nbins = [0, 5, 10, 15, 20, np.inf]\nlabels = ['0-5', '5-10', '10-15', '15-20', '20+']\n\ndf_train.groupby(\n    pd.cut(df_train['cb_person_cred_hist_length'], bins=bins, labels=labels), observed=True\n)[['person_age', 'person_emp_length']].agg(['mean'])\n\n\n\n\n\n\n\n\nperson_age\nperson_emp_length\n\n\n\nmean\nmean\n\n\ncb_person_cred_hist_length\n\n\n\n\n\n\n0-5\n24.253351\n4.277778\n\n\n5-10\n29.975727\n5.417458\n\n\n10-15\n40.011823\n5.982576\n\n\n15-20\n41.297771\n6.046178\n\n\n20+\n57.318471\n5.923567\n\n\n\n\n\n\n\nAs expected credit history length increases with age. However it is interesting to note that the employment length (time spent at an individuals last job) remain roughly the same, between 4-6 years on average.\n\n\nModel Building\nNow we will dive into creating a model that will hopefully do a good job at predicting who we should give loans to. As a bank what we care about is profit! However, to train a model that will get us good profit, I will start by finding features to train our model on that perform with high accuracy.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\n\nall_qual_cols = [\"person_home_ownership\", \"loan_intent\"]\nall_quant_cols = [\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\", \"cb_person_default_on_file\"]\n\ndf_train = pd.get_dummies(df_train)\n\nLR = LogisticRegression(max_iter = 1000)\nscaler = StandardScaler()\n\n# df_train[all_quant_cols] = scaler.fit_transform(df_train[all_quant_cols])\nscore = 0\nfinal_cols = []\n\nX_train = df_train.drop(columns = \"loan_status\")\ny_train = df_train[\"loan_status\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    cross_val_scores = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    if cross_val_scores.mean() &gt; score:\n      score = cross_val_scores.mean()\n      final_cols = cols\n\n\nprint(f\"The best model scored {score*100:.3f}% accuracy when testing on training data using: \\n{final_cols}\")    \n\nThe best model scored 84.875% accuracy when testing on training data using: \n['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_percent_income']\n\n\nThis accuracy, although not stellar, should be high enough to hopefully be a good predictor to maximize our profit margins.\nBut how are we going to decide who to give loans to? Great question! We are going to give each loan applicant a score s which predicts their likelihood to default on a loan. Higher scores indicate greater reliability.\n\ndef linear_score(X, w):\n    return X@w\n\nThe above function calculates our linear score as a function of the features X and a weight vector w. We can get this vector of weight by fitting a Linear Regression model as shown below.\n\nLR.fit(X_train[final_cols], y_train)\nLR.coef_\n\narray([[-7.59666254e-01, -9.36504060e-02, -1.80228891e+00,\n         2.75434204e-01, -3.61277124e-03,  8.27700853e+00]])\n\n\n\nw = LR.coef_[0]\ns = linear_score(df_train[final_cols], w)\n\nWe now have s which is a vector of scores for each applicant. We can see the distribution of these scores below.\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nhist = ax.hist(s, bins = 50, color = \"purple\", alpha = 0.6, linewidth = 0.75, edgecolor = \"black\")\nlabs = ax.set(xlabel = r\"Score $s$\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\nAs a reminder, as the bank, we want to use these scores to determine who we should give loans to in order to turn the maximum profit. Thus we need to determine a threshold at which we make the most money and use it as a cutoff.\nBelow you can find some simplified functions for how we determine our profit margins. - The calculate_profit formula is used when a loan is fully repaid and assumes that the profit earned each year by the bank on a 10-year loan is equal to 25% of the interest rate each year, with the other 75% of the interest going into various costs to manage the bank. This corresponds to the True Negative category. - The calculate_loss formula is employed for defaults, it uses the same mechanism as above but assumes that the borrower defaults three years into the loan and that the bank loses 70% of the principal. This corresponds to the False Negative category.\n\ndef calculate_profit(df_train):\n    return (df_train[\"loan_amnt\"]*(1 + 0.25*(df_train[\"loan_int_rate\"]/ 100))**10) - df_train[\"loan_amnt\"]\n\ndef calculate_loss(df_train):\n    return (df_train[\"loan_amnt\"]*(1 + 0.25*(df_train[\"loan_int_rate\"]/ 100))**3) - (1.7*df_train[\"loan_amnt\"])\n\nBelow, we are testing threshold values between -4 and 4 and using our formulas to calculate our net gain at each interval. Our best threshold is the one that corresponds to the highest net gain.\n\nbest_profit = 0\nbest_threshold = 0\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nfor t in np.linspace(-4, 4, 101): \n    y_pred = s &gt;= t\n    tn = ((y_pred == 0) & (y_train == 0))\n    fn = ((y_pred == 0) & (y_train == 1))\n\n    gain = calculate_profit(df_train[tn]).sum() + calculate_loss(df_train[fn]).sum()\n    gain /= len(df_train)\n    ax.scatter(t, gain, color = \"steelblue\", s = 10)\n    if gain &gt; best_profit: \n        best_profit = gain\n        best_threshold = t\n\n\nax.axvline(best_threshold, linestyle = \"-.\", color = \"grey\", zorder = -10)\nlabs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Net benefit\", title = f\"Best benefit ${best_profit:.2f} at best threshold t = {best_threshold:.3f}\")\n\n\n\n\n\n\n\n\nOur net gain per person when we only give loans to people with a risk score of 2.64 or above is $1443.81 per person! Lets see if this threshold works on data the model has never seen before…\n\n\nBank Evaluation\nHere we are importing the test data set and applying the same preprocessing steps that we used on the training data.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\ndf_test[\"cb_person_default_on_file\"] = le.transform(df_test[\"cb_person_default_on_file\"])\ndf_test = df_test.dropna()\ndf_test = pd.get_dummies(df_test)\n# df_test[all_quant_cols] = scaler.fit_transform(df_test[all_quant_cols])\n\nX_test = df_test.drop(columns = \"loan_status\")\ny_test = df_test[\"loan_status\"]\n\nWe once again calculate a net gain per person score…\n\ntest_scores = linear_score(df_test[final_cols], w)\ny_pred = test_scores &gt; best_threshold\n\ntest_tn = ((y_pred == 0) & (y_test == 0))\ntest_fn = ((y_pred == 0) & (y_test == 1))\n\ntest_gain = calculate_profit(df_test.loc[test_tn]).sum() + calculate_loss(df_test.loc[test_fn]).sum()\ntest_gain /= len(df_test)\n\n\nprint(f\"The net benefit on the test set is ${test_gain:.2f}\")\n\nThe net benefit on the test set is $1384.50\n\n\nOur model did not perform as well on the test data… however, we are still making quite a bit of profit per person from the bank!\n\n\nBorrower’s Evaluation\nThis is all fine and dandy for the bank… make money = bank happy… but what about the borrowers?\nWe are adding columns to our data frame that show each borrowers score and our models prediction.\n\ndf_test[\"person_risk\"] = test_scores\ndf_test[\"loan_status_pred\"] = y_pred\n\n\n(df_test[\"loan_status_pred\"] == df_test[\"loan_status\"]).mean()\n\nnp.float64(0.8474960739835979)\n\n\nOur model accurately predicted who would default on their loan 84.75% of the time. So the majority of people who would have defaulted on their loans were accurately denied… but who exactly are the people?\n\nAge Group Disparity\nLets break down how the model treats people of a certain age groups.\n\nsns.kdeplot(data = df_test, x = \"person_age\", y = \"person_risk\", hue = \"loan_status_pred\", alpha = 0.75, fill = True, bw_adjust=0.7)\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data = df_test, x = \"person_age\", y = \"person_risk\", hue = \"loan_status_pred\")\n\n\n\n\n\n\n\n\nWe can see that most of the people who are receiving loans are under the age age of 40, with a majority falling between 20-30 years old. This makes sense as many of our applicants fall into this age range. This may be a little unfair for persons aged over 40…\n\n\nMedical Loans\nHow about people who are applying for medical loans. I highly important demographic of borrowers whose health and livelihoods may be contingent on access to a loan. What fraction of the test dataset consists of medical loans that were approved?\n\napp_med_loans = (df_test[\"loan_intent_MEDICAL\"] == True) & (df_test[\"loan_status_pred\"] == False)\nprint(f\"Only {app_med_loans.mean()*100:.2f}% of people applying for medical loans were approved\")\n\nOnly 16.66% of people applying for medical loans were approved\n\n\n\ndef_rate_med_loans = (df_test[\"loan_intent_MEDICAL\"] == True) & (df_test[\"loan_status\"] == 1)\nprint(f\"While only {def_rate_med_loans.mean()*100:.2f}% actually defaulted on their medical loans\")\n\nWhile only 5.32% actually defaulted on their medical loans\n\n\nThis is quite harrowing… operating on a purely profit based system is taking loans away from people who may really need this money and would not have defaulted! We will dive into the fairness of this decision later…\n\n\nEducational Loans\nLets see our model treats people applying for educational loans.\n\napp_edu_loans = (df_test[\"loan_intent_EDUCATION\"] == True) & (df_test[\"loan_status_pred\"] == False)\nprint(f\"Only {app_edu_loans.mean()*100:.2f}% of people applying for education loans were approved\")\n\nOnly 18.71% of people applying for education loans were approved\n\n\n\ndef_rate_edu_loans = (df_test[\"loan_intent_EDUCATION\"] == True) & (df_test[\"loan_status\"] == 1)\nprint(f\"While only {def_rate_edu_loans.mean()*100:.2f}% actually defaulted on their loans\")\n\nWhile only 3.44% actually defaulted on their loans\n\n\nOnce again we are denying loans to a huge amount of people who in reality paid off their loans.\n\n\nBusiness Venture Loans\n\napp_bus_loans = (df_test[\"loan_intent_VENTURE\"] == True) & (df_test[\"loan_status_pred\"] == False)\nprint(f\"Only {app_bus_loans.mean()*100:.2f}% of people applying for business venture loans were approved\")\n\nOnly 15.42% of people applying for business venture loans were approved\n\n\n\ndef_rate_bus_loans = (df_test[\"loan_intent_VENTURE\"] == True) & (df_test[\"loan_status\"] == 1)\nprint(f\"While only {def_rate_bus_loans.mean()*100:.2f}% actually defaulted on their loans\")\n\nWhile only 2.46% actually defaulted on their loans\n\n\nOnce again we denied people who would have paid back their loans!\n\n\n\nFairness Discussion\nWe observed some trends in our model that seemed pretty unfair, people who would have paid off their loans were denied… but what exactly does it mean to be fair? Fairness should be making a model that makes decisions without bias towards certain groups, especially dealing with sensitive features like race, gender, or socioeconomic status. Fairness also constitutes taking into account features that are relevant to the situation. At face value our model is treating people pretty harshly across the board with regards to the intent of their loan. We do however consider people age in the model, and we do seem to have a slight bias towards younger people… But in the medical loan situation, doesn’t it seem unfair that people are being denied. doesn’t the severity / intent behind the loan seem like an important and relevant factor? In a sense we are being biased towards people who need the money… people who are ill should be treated with extra care and that should be a relevant factor in where we draw our threshold and how we train our model.\n\n\nConclusion\nOur analysis revealed that automated decision system can be beneficial to a bank’s overall profitability by approving loans only when the expected risk-adjusted returns are maximized. However, the findings also reveal the importance of continuously monitoring the system’s performance to ensure that it does not inadvertently disadvantage certain segments of the population. We observed that borrowers applying for loans of high importance, such as medical loans were drastically undeserved. In addition, older generation wer not given access to credit all in the name of profit. Ultimately, this exploration highlights the need for a balanced approach that integrates profit maximization with fairness and inclusivity taking into account other factors that are relevant to the lives of borrowers."
  },
  {
    "objectID": "posts/Classifying Palmer Penguins/index.html",
    "href": "posts/Classifying Palmer Penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nThis blog post will explore the use of machine learning to classify Palmer Penguins based on their physical measurements and location. Beginning with data preprocessing and exploration cleaned up and contextualized our analysis for the subsequent model training. We employed logistic regression and selected features in a repeatable and cross-validated manor achieving an 100% testing accuracy when classifying the penguins. This result shows the effectiveness of using culmen length, culmen depth, and island location as predictive features. The blog post also lays out provides a tradition workflow to use machine learning on ecological datasets.\n\n\nfrom IPython.display import Image, display\ndisplay(Image(filename='palmer_station.png'))\n\n\n\n\n\n\n\n\nImage Source\n\n\nData\nThis code block handles our Training Data Acquisition from the Palmer Penguins data set. This collection holds data for three types of Penguins living across three islands. There is a mix of quantitative measurements and qualitative observations.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nHere is a first look at our raw data:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nWe first extract the species name for data presentation purposed, drop unnecessary columns, remove missing values, and filter out any invalid data. We return the processed feature set (X_train) and target species labels (y_train).\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Mapping full species names exclude scientific names\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nThis is what our formatted feature set looks like:\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nData Exploration\nThe pair plot below gives us a introduces us to our data with a wide array of visualizations. These pairwise relationships are a great starting point to begin understanding the nature of the data.\n\nimport seaborn as sns\nsns.set_theme()\nsns.pairplot(data=train, hue=\"Species\")\n\n\n\n\n\n\n\n\nI scanned the above plot for data pairs that I thought showed promise in indicating species differentiation.\nHere is the relationship between Culmen Length (mm) and Culmen Depth (mm):\n\nsns.jointplot(data = train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue = \"Species\")\n\n\n\n\n\n\n\n\nWe can see that the three species of Penguins fall into three more or less distinct regions of the scatter plot with minimal overlap. This suggests that Culmen Length and Culmen Depth could be good points of reference when training a classification model.\nBelow is a plot that demonstrates the relationship between Gender, Flipper Length, and Body Mass:\n\nsns.catplot(data = X_train, x = \"Sex_FEMALE\", y = \"Flipper Length (mm)\", hue = \"Body Mass (g)\")\n\n\n\n\n\n\n\n\nThe above plot shows us that flipper length and body mass are positively correlated. In addition, we see that Males penguins have a higher growth ceiling in terms of body mass and flipper length.\nThe figure below illustrated some interesting data pertaining to species distribution over the islands. In addition, it gives us insight on the flipper length by species.\n\nimport numpy as np\ntrain.groupby([\"Island\", \"Species\"])[\"Flipper Length (mm)\"].agg(mean_flipper_length_mm=\"mean\", std_flipper_length_mm=\"std\")\n\n\n\n\n\n\n\n\n\nmean_flipper_length_mm\nstd_flipper_length_mm\n\n\nIsland\nSpecies\n\n\n\n\n\n\nBiscoe\nAdelie\n188.636364\n6.570855\n\n\nGentoo\n216.752577\n5.933715\n\n\nDream\nAdelie\n190.133333\n6.780989\n\n\nChinstrap\n196.000000\n7.423419\n\n\nTorgersen\nAdelie\n191.195122\n6.626536\n\n\n\n\n\n\n\nWe can observe that not every species is found on every island. The Adelie Penguin the only penguin found on all three surveyed islands. The Gentoo penguin is found exclusively on Biscoe Island and the Chinstrap penguin is found exclusively on Dream Island. In addition, the Gentoo penguins have the largest and least variable mean flipper length. They are followed by the Chinstrap penguins in size, then the Adelie with the smallest mean flipper lengths. Adelie and Chinstrap flipper lengths may overlap quite a bit, potentially making flipper length an unreliable feature for classifying the two species.\n\n\nModel Training\nIn the following we will employ a Logistic Regression model to classify our penguins. An integral step is figuring out which features we want to use to train our model. My methodology for this is a the brute force approach of testing out all the possible combinations using the handy combinations tool. The combinations tool will be used in conjunction with the cross_val_score tool from sklearn that will cross validate to avoid overfitting to the data. This step will help us perform better on data that the model has never seen before.\nAfter encountering maximum iteration issues with the logistic regression model, I scaled the data with the StandardScaler to be more manageable for the model\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\nLR = LogisticRegression()\nscaler = StandardScaler()\n\nX_train[all_quant_cols] = scaler.fit_transform(X_train[all_quant_cols])\nscore = 0\nfinal_cols = []\n\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    cross_val_scores = cross_val_score(LR, X_train[cols], y_train, cv = 10)\n    if cross_val_scores.mean() &gt; score:\n      score = cross_val_scores.mean()\n      final_cols = cols\n\nprint(f\"The best model scored {score*100}% accuracy when testing on training data using: \\n{final_cols}\")    \n\nThe best model scored 99.21538461538461% accuracy when testing on training data using: \n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nNow we have a repeatable, quantitative approach to justify training our model on the following features: Island, Culmen Length (mm), Culmen Depth (mm)\nI rearrange final_cols below to lead with quantitative features to conform to the graphing parameters that I will present later\n\nfinal_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nLR = LogisticRegression()\nLR.fit(X_train[final_cols], y_train)\nLR.score(X_train[final_cols], y_train)\n\n0.99609375\n\n\nOur model performed with ~99% accuracy when using our selected three features and testing on our training data. This validates some of our visual predictions we identified in our exploration section. This is a great accuracy to have… however we are still testing our model on the data it was trained with. Next we will test it against unseen data!\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nX_test, y_test = prepare_data(test)\nX_test[all_quant_cols] = scaler.fit_transform(X_test[all_quant_cols])\n\nLR.score(X_test[final_cols], y_test)\n\n1.0\n\n\nWow! We achieved an 100% testing accuracy on our test data! In context, we were able to correctly classify what type of penguin an individual was based on what island they were on and their culmen length and depth.\n\n\nEvaluation\nThe following block sets up a plot panel of decision regions that represent our classifier.\n\nfrom matplotlib import pyplot as plt\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (9, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[final_cols], y_train)\n\n\n\n\n\n\n\n\nAs we noted in our exploration, not all penguins are found on all islands. This mean that our model essentially only had to account for a maximum of two species of penguins on any given island. we can see that the Culmen Length and Depth were also clustered nicely to have clear linear segmentation for our decision regions.\nLets take a look at our confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[final_cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nAs we had a 100% testing accuracy, this is exactly the sort of confusion matrix we would expect. The diagonal representing our correct classifications. Above and below the diagonal are empty because we did not misclassify any penguins.\nHere is another way to digest the confusion matrix:\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguins who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguins who were classified as Adelie.\nThere were 0 Adelie penguins who were classified as Chinstrap.\nThere were 0 Adelie penguins who were classified as Gentoo.\nThere were 0 Chinstrap penguins who were classified as Adelie.\nThere were 11 Chinstrap penguins who were classified as Chinstrap.\nThere were 0 Chinstrap penguins who were classified as Gentoo.\nThere were 0 Gentoo penguins who were classified as Adelie.\nThere were 0 Gentoo penguins who were classified as Chinstrap.\nThere were 26 Gentoo penguins who were classified as Gentoo.\n\n\n\n\nDiscussion\nExploring the Palmer Penguins data is a great introduction to data analysis and machine learning. Our results highlighted the effectiveness of using Culmen Depth, Culmen Length and Island Location as predictive features to train a Logistic regression model. We began by exploring the data set. We set up a series of plots and tables that helped us contextualize the data and make predictions about which features of the data may be helpful. While this was useful for understanding the data we were working with, we needed a repeatable method for choosing our eventual three features. We then turned to training and testing logistic regression models on different feature combinations. We scored each combination with how it performed against the training data and was cross validated against smaller subsets of the data to avoid overfitting. Finally we used the cross validated features with the largest accuracy and tested them against a separate test data set. Here we achieved the desired 100% testing accuracy. Finally, we took a moment to evaluate these results by examining the decision regions, and looking at the confusion matrix. Visualizing decision regions highlighted how well logistic regression can separate species based on our selected features. The confusion matrix confirmed the reliability of our model as there were no misclassifications. This blog gave me good insight on data analysis and practical machine learning workflows. Several important takeaways were the importance of separating our training and testing data to ensure our model works on unseen data. The importance of cross validation is also key to not overfitting our data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lukka Wolff’s Machine Learning Blog",
    "section": "",
    "text": "Evolution Based Weight Vector Optimization\n\n\n\n\n\n\nEvolutionary Algorithms\n\n\nGenetic Programming\n\n\nNeural Networks\n\n\nClassification\n\n\nMNIST\n\n\n\nImplementation of Evolution Based Weight Vector Optimization\n\n\n\n\n\nJul 5, 2026\n\n\nLukka Wolff, James Cummings, Jiffy Lesica, Yahya Rahhawi\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Music Genre Classification\n\n\n\n\n\n\nNatural Language Processing\n\n\nNeural Networks\n\n\nDeep Learning\n\n\n\nDeep Music Genre Classification in Python\n\n\n\n\n\nMay 12, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Kernel Machines\n\n\n\n\n\n\nSparse Kernel Machines\n\n\nLogistic Regression\n\n\nImplementation\n\n\nIn Progress\n\n\n\nImplementing Sparse Kernelized Logistic Regression in Python\n\n\n\n\n\nApr 28, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\n\nLogistic Regression\n\n\nImplementation\n\n\n\nImplementing Logistic Regression in Python\n\n\n\n\n\nApr 9, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\n\nPerceptron\n\n\nImplementation\n\n\n\nImplementing the Perceptron Algorithm in Python\n\n\n\n\n\nApr 2, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\n\nClassification\n\n\nEthics\n\n\n\nAuditing Bias in Automated Decision-Making Systems\n\n\n\n\n\nMar 12, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\n\nClassification\n\n\nEthics\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nMar 5, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\n\nData Science\n\n\nClassification\n\n\n\nClassifying Palmer Penguins using Machine Learning\n\n\n\n\n\nFeb 26, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, my name is Lukka Wolff. Welcome to my Blog!"
  },
  {
    "objectID": "posts/Auditing Bias/index.html",
    "href": "posts/Auditing Bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Abstract\nThis project audits bias in automated decision-making systems by analyzing employment predictions from the 2018 American Community Survey (ACS) data for Georgia. A Random Forest Classifier was trained to predict employment status based on demographic features such as age, education, sex, disability, and nativity, while examining racial bias specifically between White and Black/African American individuals. The audit revealed approximately balanced accuracy, positive predictive values, and error rates across these racial groups, though slight discrepancies exist. Despite good numerical fairness, we must still consider ethical considerations regarding consent, data recency, and the ethical deployment of such models in different decision-making contexts.\n\n\nData and Feature Selection\nWe are using the folkables package to access data from the 2018 American Community Survey’s Public Use Microdata Sample (PUMS) for the state of Georgia.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"GA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000025\n5\n1\n3700\n3\n13\n1013097\n68\n51\n...\n124\n69\n65\n63\n117\n66\n14\n68\n114\n121\n\n\n1\nP\n2018GQ0000035\n5\n1\n1900\n3\n13\n1013097\n69\n56\n...\n69\n69\n7\n5\n119\n74\n78\n72\n127\n6\n\n\n2\nP\n2018GQ0000043\n5\n1\n4000\n3\n13\n1013097\n89\n23\n...\n166\n88\n13\n13\n15\n91\n163\n13\n89\n98\n\n\n3\nP\n2018GQ0000061\n5\n1\n500\n3\n13\n1013097\n10\n43\n...\n19\n20\n3\n9\n20\n3\n3\n10\n10\n10\n\n\n4\nP\n2018GQ0000076\n5\n1\n4300\n3\n13\n1013097\n11\n20\n...\n13\n2\n14\n2\n1\n2\n2\n13\n14\n12\n\n\n\n\n5 rows × 286 columns\n\n\n\nThis data set contains a large amount of features for each individual, so we are going to narrow it down to only those that we may use to train our model.\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n51\n13.0\n5\n16\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n2\n6.0\n\n\n1\n56\n16.0\n3\n16\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n1\n2.0\n2\n1\n6.0\n\n\n2\n23\n20.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n1.0\n2\n2\n1.0\n\n\n3\n43\n17.0\n1\n16\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n1\n2\n6.0\n\n\n4\n20\n19.0\n5\n16\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n\n\n\n\n\nA few key features to note are:\n\nESR is employment status (1 if employed, 0 if not)\nRAC1P is race (1 for White Alone, 2 for Black/African American Alone, 3 and above for other self-identified racial groups)\nSEX is binary sex (1 for male, 2 for female)\n\nNow we select for the features we want to use and we will be able to constuct a BasicProblem that expresses our desire to use these features to predict employment status ESR, using RAC1P as the group label.\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nWe now have a feature matrix features, a label vector label, and a group label vector group.\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(100855, 15)\n(100855,)\n(100855,)\n\n\nWe are now going to split our data into a training set and a testing set:\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nData Exploration\nBefore we dive straight into model training, lets take a deeper look at our data.\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\nHere is a quick look at our data frame containing our training data:\n\ndf.head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\ngroup\nlabel\n\n\n\n\n0\n48.0\n16.0\n1.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n2\nTrue\n\n\n1\n52.0\n24.0\n2.0\n0.0\n2.0\n0.0\n1.0\n3.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n1\nTrue\n\n\n2\n55.0\n18.0\n5.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n3.0\n1.0\n2.0\n2.0\n2.0\n1.0\n2\nTrue\n\n\n3\n15.0\n12.0\n5.0\n3.0\n2.0\n1.0\n4.0\n3.0\n0.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2.0\n6\nFalse\n\n\n4\n26.0\n22.0\n1.0\n0.0\n2.0\n0.0\n5.0\n1.0\n4.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2\nFalse\n\n\n\n\n\n\n\n\nlen(df)\n\n80684\n\n\nThis data set contains information from \\(80684\\) individuals in the state of Georgia.\n\ndf[\"label\"].value_counts()\n\nlabel\nFalse    44664\nTrue     36020\nName: count, dtype: int64\n\n\n\ndf['label'].mean()\n\nnp.float64(0.44643299786822666)\n\n\nOf these individuals, 44.64% or \\(36020\\) individuals are employed.\n\ndf['group'].value_counts()\n\ngroup\n1    53302\n2    20239\n6     3267\n9     1996\n8     1589\n3      159\n5       66\n7       66\nName: count, dtype: int64\n\n\n\ndf['group'].value_counts(normalize=True)\n\ngroup\n1    0.660627\n2    0.250843\n6    0.040491\n9    0.024738\n8    0.019694\n3    0.001971\n5    0.000818\n7    0.000818\nName: proportion, dtype: float64\n\n\nThe two largest racial groups are 1 White Alone with 53302 individuals making up 66% of the data, and 2 Black/African American Alone with 20239 individuals making up 25% of the data.\n\ndf.groupby('group')['label'].mean()\n\ngroup\n1    0.460771\n2    0.416127\n3    0.433962\n5    0.348485\n6    0.481175\n7    0.484848\n8    0.429830\n9    0.330160\nName: label, dtype: float64\n\n\n~46% of White Alone individuals are employed and ~42% of Black/African American Alone individuals are employed.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(style=\"whitegrid\")\n\n\ndef plot_intersection(df, col1='group', col2='sex'):\n    prop_df = df.groupby([col1, col2])['label'].mean().reset_index()\n    plt.figure(figsize=(8, 5))\n\n    ax = sns.barplot(x=col1, y='label', hue=col2, data=prop_df)\n    \n    plt.title(f'Employment Proportion by {col1} and {col2}')\n    plt.xlabel(col1)\n    plt.ylabel(f'Employment Proportion (%)')\n\n    for p in ax.patches:\n        ax.annotate(f'{p.get_height()*100:.2f}', \n                   (p.get_x() + p.get_width() / 2., p.get_height()),\n                   ha = 'center', va = 'bottom',\n                   xytext = (0, 5), textcoords = 'offset points')\n    \n    plt.tight_layout()\n    plt.show()\n\ngroup_sex = plot_intersection(df, 'group', 'SEX')\n    \n\n\n\n\n\n\n\n\nFor many groups, the percentage of men who are employed is higher than that of women. One notable group where this is not the case is 2 Black/African American where the percentage of employed women is ~44% against ~39% for men.\nNATIVITY indicates a persons place of birth. 1 being Native born and 2 being Foreign born.\n\ngroup_nativity = plot_intersection(df, 'group', 'NATIVITY')\n\n\n\n\n\n\n\n\nInterestingly, across the board we see that the percentage of foreign born individuals who are employed is much higher than the proportion of native born individuals. However, as seen in the plot below, this may be attributed to the fact that few foreign born individuals on the extremities of the age, reducing the influence of youth and seniority as factors in employment proportion.\n\nsns.displot(data=df, x=\"AGEP\", hue=\"NATIVITY\", kind=\"kde\", bw_adjust=0.5, fill=True, alpha=0.75)\n\n\n\n\n\n\n\n\nDIS represents an individuals disability status. 1 with disability, and 2 without a disability.\n\ngroup_dis = plot_intersection(df, 'group', 'DIS')\n\n\n\n\n\n\n\n\nAcross the board we see that people without disability are employed at a much higher proportion than people with disabilities.\n\n\nSupplementary plots that I thought were interesting.\n\ncit_sex = plot_intersection(df, 'CIT', 'SEX')\n\n\n\n\n\n\n\n\n\nmar_sex = plot_intersection(df, 'MAR', 'SEX')\n\n\n\n\n\n\n\n\n\nschl_sex = plot_intersection(df, 'SCHL', 'SEX')\n\n\n\n\n\n\n\n\n\n\n\nModel Training\nWe are now ready to create a model and train it on our training data. We will first scale our data, then we will employ a Random Forest Classifier. This approach uses an array of decision trees on various sub-samples of the data and aggregates their results. Learn more here.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\nacc = 0\nbest_depth = 0\nfor depth in range(5, 20):\n    model = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=depth))\n    model.fit(X_train, y_train)\n    cv_scores = cross_val_score(model, X_train, y_train, cv = 5)\n    if cv_scores.mean() &gt; acc:\n        best_depth = depth\n        acc = cv_scores.mean()\n\nprint(f\"Best maximum tree depth: {best_depth}, Accuracy: {acc*100:.2f}%\")\n\nBest maximum tree depth: 16, Accuracy: 83.39%\n\n\nAbove, we tuned our model complexity using the max_depth parameter of the RandomForestClassifier. This controls how deep each tree in our forest can get which impacts how general our model is with regards to things like overfitting. We examined values ranging from 5 to 20 for the max depth and found the highest cross validated accuracy when max_depth=16.\n\nRF = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=best_depth))\nRF.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('randomforestclassifier',\n                 RandomForestClassifier(max_depth=16))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('randomforestclassifier',\n                 RandomForestClassifier(max_depth=16))]) StandardScaler?Documentation for StandardScalerStandardScaler() RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(max_depth=16) \n\n\n\n\nModel Audit\n\nOverall Measures\n\ny_hat = RF.predict(X_test)\n(y_hat == y_test).mean()\n\nnp.float64(0.8281195776114223)\n\n\nOur model has an overall accuracy of 82.81% on the testing data suite. Not too shabby!\nWe will now address the positive predictive value (PPV) of our model (we wont evaluate true sufficiency right now as we are skipping NPV). Given that the prediction is positive (y_hat = 1), how likely is it that the prediction is accurate (y_test = 1)? In other words, if we predict someone to be employed, how likely is it that they are actually employed?\nWe can approximate this value with the following code:\n\ntp = (y_hat == 1) & (y_test == 1)\nfp = (y_hat == 1) & (y_test == 0)\nppv = tp.sum() / (tp.sum() + fp.sum())\nprint(f\"Positive Predictive Value: {ppv*100:.2f}%\")\n\nPositive Predictive Value: 77.46%\n\n\nSo, when our model predicts someone is employed, they are actually employed 77.46% of the time.\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\ncm = confusion_matrix(y_test, y_hat)\nConfusionMatrixDisplay(cm, display_labels=['Not Employed', 'Employed']).plot(cmap='Blues')\nplt.title('Confusion Matrix')\n\nText(0.5, 1.0, 'Confusion Matrix')\n\n\n\n\n\n\n\n\n\nAbove is our confusion matrix, lets use this information to find the false negative and false positive rates for our model.\n\nfn = (y_hat == 0) & (y_test == 1)\nfnr = fn.sum() / (tp.sum() + fn.sum())\nprint(f\"False Negative Rate: {fnr*100:.2f}%\")\n\nFalse Negative Rate: 12.74%\n\n\nOur model incorrectly classified people as unemployed when they were in fact employed 12.74% of the time.\n\nfp = (y_hat == 1) & (y_test == 0)\ntn = (y_hat == 0) & (y_test == 0)\nfpr = fp.sum() / (fp.sum() + tn.sum())\nprint(f\"False Positive Rate: {fpr*100:.2f}%\")\n\nFalse Positive Rate: 20.84%\n\n\nOur model incorrectly classified people as employed when they were in fact unemployed 20.84% of the time.\n\n\nBy-Group Measures\nNow, lets explore how our model treats people in their respective groups. We are going to focus primarily on the possible discrepancies between individuals in 1 White Alone and 2 Black/African American Alone groups.\n\nwa = (y_hat == y_test)[group_test == 1].mean()\naa = (y_hat == y_test)[group_test == 2].mean()\nprint(f\"White Alone Accuracy: {wa*100:.2f}%\\nBlack/African American Alone Accuracy: {aa*100:.2f}%\")\n\nWhite Alone Accuracy: 82.40%\nBlack/African American Alone Accuracy: 83.17%\n\n\nOur model has pretty comparable accuracy scores for both groups, only slightly lower for White Alone individuals.\n\ntp_wa = ((y_hat == 1) & (y_test == 1) & (group_test == 1))\nfp_wa = ((y_hat == 1) & (y_test == 0) & (group_test == 1))\nppv_wa = tp_wa.sum() / (tp_wa.sum() + fp_wa.sum())\n\ntp_aa = ((y_hat == 1) & (y_test == 1) & (group_test == 2))\nfp_aa = ((y_hat == 1) & (y_test == 0) & (group_test == 2))\nppv_aa = tp_aa.sum() / (tp_aa.sum() + fp_aa.sum())\n\nprint(f\"PPV for White Alone: {ppv_wa*100:.2f}%\")\nprint(f\"PPV for Black/African American Alone: {ppv_aa*100:.2f}%\")\n\nPPV for White Alone: 77.96%\nPPV for Black/African American Alone: 75.84%\n\n\nOnce again our PPV rates are quite comparable, although the White Alone group does have a slightly higher PPV.\n\nfn_wa = (y_hat == 0) & (y_test == 1) & (group_test == 1)\nfnr_wa = fn_wa.sum() / (tp_wa.sum() + fn_wa.sum())\n\nfn_aa = (y_hat == 0) & (y_test == 1) & (group_test == 2)\nfnr_aa = fn_aa.sum() / (tp_aa.sum() + fn_aa.sum())\n\nprint(f\"FNR for White Alone: {fnr*100:.2f}%\")\nprint(f\"FNR for Black/African American Alone: {fnr_aa*100:.2f}%\")\n\nFNR for White Alone: 12.74%\nFNR for Black/African American Alone: 12.86%\n\n\nThe false negative rates are once again very similar across groups.\n\nfp_wa = ((y_hat == 1) & (y_test == 0) & (group_test == 1))\ntn_wa = ((y_hat == 0) & (y_test == 0) & (group_test == 1))\nfpr_wa = fp_wa.sum() / (fp_wa.sum() + tn_wa.sum())\n\nfp_aa = ((y_hat == 1) & (y_test == 0) & (group_test == 2))\ntn_aa = ((y_hat == 0) & (y_test == 0) & (group_test == 2))\nfpr_aa = fp_aa.sum() / (fp_aa.sum() + tn_aa.sum())\n\nprint(f\"FPR for White Alone: {fpr_wa*100:.2f}%\")\nprint(f\"FPR for Black/African American Alone: {fpr_aa*100:.2f}%\")\n\nFPR for White Alone: 21.33%\nFPR for Black/African American Alone: 19.64%\n\n\nThere is some slight discrepancy here as persons in the White Alone group is more often mistaken for having a job than persons in the Black/African American Alone group.\n\n\nBias Measures\nIn terms of accuracy, our model seems to be performing well. Let’s take a deeper look at how our model might be biased or unfair by examining calibration, error rate balance, and statistical parity.\nOur model can be considered well-calibrated or sufficient if it reflects equal likelihood of employment irrespective of the individuals’ group membership. That is, free from predictive bias, this our PPV for both groups should be the same. Looking back to our calculation of these scores, we saw that they were about equal. PPV for White Alone: 77.96% and PPV for Black/African American Alone: 75.84%. Thus, we can say our model is well-calibrated.\n\nOur model can only satisfy approximate error rate balance given that the true positive rate (TPR) and false positive rates (FPR) be equal on the two groups.\n\ntpr_wa = tp_wa.sum() / (tp_wa.sum() + fn_wa.sum())\ntpr_aa = tp_aa.sum() / (tp_aa.sum() + fn_aa.sum())\nprint(f\"TPR --&gt; WA: {tpr_wa*100:.2f}% ~~ AA: {tpr_aa*100:.2f}%\")\nprint(f\"FPR --&gt; WA: {fpr_wa*100:.2f}% ~~ AA: {fpr_aa*100:.2f}%\")\n\nTPR --&gt; WA: 86.69% ~~ AA: 87.14%\nFPR --&gt; WA: 21.33% ~~ AA: 19.64%\n\n\nWe can see that both groups have an approximately equal TPR and FPRs. Thus our model satisfies approximate error rate balance.\n\nOur model satisfies statistical parity if the proportion of individuals classified as employed is the same for each group.\n\nprop_wa = (y_hat == 1)[group_test == 1].mean()\nprop_aa = (y_hat == 1)[group_test == 2].mean()\nprint(f\"Proportion of White Alone Predicted to be Employed: {prop_wa*100:.2f}%\")\nprint(f\"Proportion of Black/African American Alone Predicted to be Employed: {prop_aa*100:.2f}%\")\n\nProportion of White Alone Predicted to be Employed: 51.74%\nProportion of Black/African American Alone Predicted to be Employed: 47.61%\n\n\nWe can observe some differences in these two scores, a higher proportion of White Alone persons are predicted to be employed than Black/African American Alone persons. The difference remains small as the rated are within 5% of one another… as we don’t have a set threshold we cant know if the difference is significant, ergo we cant say that we do or do not satisfy statistical parity.\n\n\n\nFeasible FNR and FPRs\n\np_wa = ((y_test == 1) & (y_hat == 1))[group_test == 1].mean()\np_aa = ((y_test == 1) & (y_hat == 1))[group_test == 2].mean()\nprint(f\"Prevalence of White Alone: {p_wa*100:.2f}%\")\nprint(f\"Prevalence of Black/African American Alone: {p_aa*100:.2f}%\")\n\nPrevalence of White Alone: 40.34%\nPrevalence of Black/African American Alone: 36.11%\n\n\nThe proportion of true positive values is higher with in the White Alone group.\n\nimport numpy as np\n\nplt.figure(figsize=(8, 5))\n\n\nfnr_range = np.linspace(0, 1, 100)\nfpr_aa_alt = (p_aa / (1 - p_aa)) * ((1 - ppv_aa) / ppv_aa) * (1 - fnr_aa)\nfpr_wa_alt = (p_wa / (1 - p_wa)) * ((1 - ppv_wa) / ppv_wa) * (1 - fnr_wa)\n\n# Calibrate on low PPV --&gt; PPv from Black/African American Alone\nfpr_aa_feasible = (p_aa / (1 - p_aa)) * ((1 - ppv_aa) / ppv_aa) * (1 - fnr_range) # PPVb is set equal to the observed value of PPVw\nfpr_wa_feasible = (p_wa / (1 - p_wa)) * ((1 - ppv_aa) / ppv_aa) * (1 - fnr_range) # pw and PPVw are both held fixed\n\nplt.plot(fnr_wa, fpr_wa_alt, 'o', color='orange', label='White Alone')\nplt.plot(fnr_aa, fpr_aa_alt, 'o', color='black', label='Black/African American Alone')\n\nplt.plot(fnr_range, fpr_aa_feasible, '-', color='black')\nplt.plot(fnr_range, fpr_wa_feasible, '-', color='orange')\n\n\n# Shading Attempts\n# delta = np.abs(ppv_wa - ppv_aa)\n# low = (p_wa / (1 - p_wa)) * ((1 - ppv_wa - (delta &lt; 0.05)) / ppv_wa - (delta &lt; 0.05)) * (1 - fnr_range)\n# high = (p_wa / (1 - p_wa)) * ((1 - ppv_wa + (delta &lt; 0.05)) / ppv_wa + (delta &lt; 0.05)) * (1 - fnr_range)\n# plt.fill_between(fnr_range, low, high, color='blue', alpha=0.3)\n\nplt.xlabel('False Negative Rate')\nplt.ylabel('False Positive Rate')\nplt.title('Feasible (FNR, FPR) combinations')\nplt.legend()\n\n\n\n\n\n\n\n\n\nOur current model appears to be working quite well for both groups as we can see their (FNR, FPR) points are not far separated. However, if we did want to equalize our false positive rates (classify someone as employed when they are not), this would necessitate an increase in the false negative rates for White Alone to around 0.22 from its current around 0.12. This would inevitably lead to a drop in accuracy. Contextually, this would mean classifying more White Alone persons who are actually employed as unemployed to match FPR.\nNonetheless, our model performs close to even for both groups.\n\n\n\nConclusion\nThere are many applications for a model of this sort. Making predictions on who is or isn’t employed would benefit many companies that operate on credit or lending. Knowing who is employed would help determine whether it would be wise to approve a higher credit limit, approve a mortgage application, or let someone lease a car. This is the case because employment can be widely used aa feature or indicator of someones ability to pay back their debts and the interest included therein. If you determine that better, your company can be more profitable. This could also be useful for marketing and advertising companies who can use these predictions to improve the targeting of their ads.\nDeploying this specific Random Forest Based model in a large scale setting could make the small discrepancies in error much larger. For instance, lets assume this model is being used by the US government to determine who needs unemployment aid. The small differences in FPRs (WA: ~21% & AA: ~19) could mean tens of millions of people could be classified as employed and not receive the help they need. This would also be slightly disproportionate as millions more White Alone people would not get the aid they need when compared to the ratio of Black/African American individuals. Conversely, this FPR could be to their advantage in a commercial setting that would give them lower interest rates on loans, and possibly better odds at landing a job.\nBased on my Bias Audit, most of my standards of evaluation were quite close. None of them differed enough for me to view the model as problematic with regards to White Alone or Black/African American. Of course, the model also used data from other groups, thus necessitating further exploration into those groups to determine if there are problematic levels of bias.\nBias aside, there could be other potential issues in deploying this algorithm. First and foremost, I believe that, with the exception of advertisement targeting, employment status should be requested and declared with the consent of the individual. There should be some government regulations in what industries are allowed to use such algorithms and how. In the situations that it is used, I worry about the transparency of the model processes. Transparency on what data the data is being used for as it is being collected, and where it came from when it is being used. Moreover, there should be transparency and easily interpretable in use cases like credit approvals, etc. Finally, using data from a Georgia may not be generalizable across the country or even in current day Georgia as many things have changed since 2018 and after COVID. There should be an effort made to keep the training data reflective of current day trends. Thus, even though the model appears acceptable with regards to numerical fairness metrics, there are broader ethical and practical concerns that we should address in order to deploy a responsible and effective model."
  },
  {
    "objectID": "posts/Deep Music Genre Classification/index.html",
    "href": "posts/Deep Music Genre Classification/index.html",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "In this blog post, we explore a deep learning approach to predicting the genres of music tracks. We leverage both song lyrics and engineered metadata features. We tokenize the lyrics with a BERT tokenizer and make use of Spotify’s engineered audio–semantic features (e.g., acousticness, danceability, thematic tags). We implement three neural networks: a lyric-based model, a metadata-only network, and a combined network that uses both lyric embeddings and engineered features. We compare how our different models stack up against one another and our base rate to assess the success of our different approaches to genre prediction."
  },
  {
    "objectID": "posts/Deep Music Genre Classification/index.html#lyrical-classification",
    "href": "posts/Deep Music Genre Classification/index.html#lyrical-classification",
    "title": "Deep Music Genre Classification",
    "section": "Lyrical Classification",
    "text": "Lyrical Classification\n\nclass TextClassificationModel(nn.Module):\n\n    def __init__(self,vocab_size, embedding_dim, max_len, num_class):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.dropout = nn.Dropout(0.2)\n        self.fc_flat = nn.Linear(embedding_dim, embedding_dim)\n        self.fc = nn.Linear(embedding_dim, num_class) # max_len*embedding_dim\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.fc_flat(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = x.mean(axis = 1)\n        # x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return(x)\n\nOur model begins with the embedding layer where each word is looked up in an embedding table and turned into a learned vector of size embedding_dim. Immediately after embedding, we pass each token’s embedding through a small fully-connected layer then a ReLU activation, the fully connected layer lets the model learn a richer representation before pooling. We then pass the embedding into a dropout layer where 20% of the embedding vectors are randomly zeroed. This is a form of regularization step meant to help us not be over-reliant on certain tokens. Our mean-pool layer reduces our dimension by averaging all token embeddings so each song is now a fixed-size vector. Finally, our linear layer gives us our probabilities for each genre.\nLet’s have a look at it!\n\nvocab_size = len(tokenizer.vocab)\nembedding_dim = 32\nnum_class = len(genres)\n\ntext_model = TextClassificationModel(vocab_size, embedding_dim, max_len, num_class).to(device)\n\n\nsummary(text_model, input_Size = (8, max_len))\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nTextClassificationModel                  --\n├─Embedding: 1-1                         976,736\n├─Dropout: 1-2                           --\n├─Linear: 1-3                            1,056\n├─Linear: 1-4                            132\n├─ReLU: 1-5                              --\n=================================================================\nTotal params: 977,924\nTrainable params: 977,924\nNon-trainable params: 0\n=================================================================\n\n\nWe have a huge amount of trainable parameters! We could make this architecture more lightweight by changing the size of our embedding dimension.\nBelow, we define our training loop which can be used for all of our three models that we will define shortly. We define an accuracy function that we will use to evaluate the accuracy of our model and another to evaluate the per class accuracy.\n\ndef train(model, dataloader, mode=\"lyrics\", vocab_freq=False):\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    epoch_start_time = time.time()\n    # keep track of some counts for measuring accuracy\n    total_acc, total_count = 0, 0\n    \n    for X, y in dataloader:\n        # unpack and move to device\n        tokens, engineered = X\n        y = y.to(device)\n\n        if mode == \"lyrics\":\n            \"\"\"\n            if vocab_freq:\n                vocab = build_vocab_from_iterator(tokens, specials=[\"&lt;unk&gt;\"], min_freq = 50)\n                tokens = torch.tensor(vocab)\n            \"\"\"\n            data = tokens.to(device)\n        elif mode == \"engineered\":\n            data = engineered.to(device)\n        else:\n            data = X\n\n        # zero gradients\n        optimizer.zero_grad()\n        # form prediction on batch\n        predicted_label = model(data)\n        # evaluate loss on prediction\n        loss = loss_fn(predicted_label, y)\n        # compute gradient\n        loss.backward()\n        # take an optimization step\n        optimizer.step()\n                \n        # for printing accuracy\n        total_acc += (predicted_label.argmax(1) == y).sum().item()\n        total_count += y.size(0)\n\n    print(f'| epoch {epoch:3d} | train accuracy {total_acc/total_count:8.3f} | time: {time.time() - epoch_start_time:5.2f}s')\n\ndef accuracy(model, dataloader, mode=\"lyrics\"):\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            # unpack and move to device\n            tokens, engineered = X\n            y = y.to(device)\n\n            if mode == \"lyrics\":\n                data = tokens.to(device)\n            elif mode == \"engineered\":\n                data = engineered.to(device)\n            elif mode == \"both\":\n                data = X\n\n            predicted_label = model(data)\n            total_acc += (predicted_label.argmax(1) == y).sum().item()\n            total_count += y.size(0)\n    return total_acc/total_count\n\ndef per_class_accuracy(model, dataloader, mode=\"lyrics\", num_classes=4):\n    model.eval()\n    correct = [0] * num_classes\n    total   = [0] * num_classes\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            tokens, engineered = X\n            y = y.to(device)\n\n            if mode == \"lyrics\":\n                data = tokens.to(device)\n            elif mode == \"engineered\":\n                data = engineered.to(device)\n            else:\n                data = X\n\n            outputs = model(data)\n            preds = outputs.argmax(dim=1)\n\n            for cls in range(len(correct)):\n                mask = (y == cls)\n                total[cls] += mask.sum().item()\n                correct[cls] += ((preds == cls) & mask).sum().item()\n\n    return {\n        cls: (correct[cls] / total[cls] if total[cls] &gt; 0 else 0.0)\n        for cls in range(len(correct))\n    }\n\nNow that we have those functions, lets jump right in and see how our model does when training on lyrics!\n\nEPOCHS = 25\nfor epoch in range(1, EPOCHS + 1):\n    train(text_model, train_loader, \"lyrics\")\n    print(\"     test accuracy  \", accuracy(text_model, val_loader))\n\n| epoch   1 | train accuracy    0.379 | time:  4.47s\n     test accuracy   0.3540097474523704\n| epoch   2 | train accuracy    0.398 | time:  3.97s\n     test accuracy   0.39787328311918474\n| epoch   3 | train accuracy    0.425 | time:  3.94s\n     test accuracy   0.4262295081967213\n| epoch   4 | train accuracy    0.457 | time:  4.03s\n     test accuracy   0.42977403633141337\n| epoch   5 | train accuracy    0.498 | time:  4.06s\n     test accuracy   0.46256092157731504\n| epoch   6 | train accuracy    0.556 | time:  3.77s\n     test accuracy   0.4980062029242357\n| epoch   7 | train accuracy    0.600 | time:  3.75s\n     test accuracy   0.538325210456358\n| epoch   8 | train accuracy    0.642 | time:  3.64s\n     test accuracy   0.5578201151971643\n| epoch   9 | train accuracy    0.674 | time:  3.64s\n     test accuracy   0.5604785112981834\n| epoch  10 | train accuracy    0.695 | time:  3.67s\n     test accuracy   0.5746566238369517\n| epoch  11 | train accuracy    0.712 | time:  3.68s\n     test accuracy   0.5813026140894993\n| epoch  12 | train accuracy    0.729 | time:  3.74s\n     test accuracy   0.5795303500221533\n| epoch  13 | train accuracy    0.745 | time:  4.02s\n     test accuracy   0.5724412937527692\n| epoch  14 | train accuracy    0.757 | time:  3.94s\n     test accuracy   0.5777580859548073\n| epoch  15 | train accuracy    0.767 | time:  4.04s\n     test accuracy   0.5764288879042977\n| epoch  16 | train accuracy    0.782 | time:  3.85s\n     test accuracy   0.5746566238369517\n| epoch  17 | train accuracy    0.795 | time:  3.86s\n     test accuracy   0.5755427558706248\n| epoch  18 | train accuracy    0.799 | time:  3.91s\n     test accuracy   0.5684536996012406\n| epoch  19 | train accuracy    0.813 | time:  4.09s\n     test accuracy   0.5755427558706248\n| epoch  20 | train accuracy    0.821 | time:  3.99s\n     test accuracy   0.5737704918032787\n| epoch  21 | train accuracy    0.831 | time:  4.46s\n     test accuracy   0.5693398316349136\n| epoch  22 | train accuracy    0.840 | time:  4.20s\n     test accuracy   0.5742135578201152\n| epoch  23 | train accuracy    0.849 | time:  4.23s\n     test accuracy   0.5622507753655295\n| epoch  24 | train accuracy    0.857 | time:  4.39s\n     test accuracy   0.561807709348693\n| epoch  25 | train accuracy    0.859 | time:  4.26s\n     test accuracy   0.562693841382366\n\n\n\naccuracy(text_model, val_loader)\n\n0.5666814355338945\n\n\nAn accuracy around 56% may not seem all that great at first glance… however, lets remember our base rate was 36%, so despite the fact that we don’t have a particularly high accuracy we can still say that this model is successful!\nLet’s look at our accuracy on each of our genres. A quick reminder that our genre keys are: - hip hop: 0 - jazz: 1 - reggae: 2 - rock: 3\n\nper_class_accuracy(text_model, val_loader, mode=\"lyrics\")\n\n{0: 0.47701149425287354,\n 1: 0.5816326530612245,\n 2: 0.5031055900621118,\n 3: 0.6090686274509803}\n\n\nEven our weakest genre (hip hop at around 48%) comfortably exceeds the base rate! Our model is indeed learning useful signals from the lyrics. Our best performances were on jazz and rock that may suggest that those lyrics have more distinct stylistic patterns. Hip hop and reggae, on the other hand, may have suffered because of slang or patois lyrics or possibly thematic overlap."
  },
  {
    "objectID": "posts/Deep Music Genre Classification/index.html#engineered-features-classification",
    "href": "posts/Deep Music Genre Classification/index.html#engineered-features-classification",
    "title": "Deep Music Genre Classification",
    "section": "Engineered Features Classification",
    "text": "Engineered Features Classification\nLet’s tackle using our engineered features to try and determine song genres!\n\nclass MetadataClassificationModel(nn.Module):\n\n    def __init__(self, num_features, num_class):\n        super().__init__()\n    \n        self.pipeline = nn.Sequential(\n            nn.Linear(num_features, 18), \n            nn.ReLU(),\n            nn.Linear(18, 12), \n            nn.ReLU(),\n            nn.Linear(12, 8), \n            nn.ReLU(),\n            nn.Linear(8, num_class)\n            )\n\n    def forward(self, x):\n        return self.pipeline(x)\n\n    def predict(self, x): \n        return self.score(x) &gt; 0\n\nThis is a pretty simple architecture for our engineered features of which there are twenty-two. We are using a series of fully-connected linear layers, each punctuated by a ReLU nonlinearity activation function.\n\nnum_features = len(engineered_features)\n\nmeta_model = MetadataClassificationModel(num_features, num_class).to(device)\nsummary(meta_model, input_Size = (8, max_len))\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nMetadataClassificationModel              --\n├─Sequential: 1-1                        --\n│    └─Linear: 2-1                       414\n│    └─ReLU: 2-2                         --\n│    └─Linear: 2-3                       228\n│    └─ReLU: 2-4                         --\n│    └─Linear: 2-5                       104\n│    └─ReLU: 2-6                         --\n│    └─Linear: 2-7                       36\n=================================================================\nTotal params: 782\nTrainable params: 782\nNon-trainable params: 0\n=================================================================\n\n\nThis model is pretty lightweight compared to the lyric based model. Lets see how it performs!\n\nEPOCHS = 25\nfor epoch in range(1, EPOCHS + 1):\n    train(meta_model, train_loader, \"engineered\")\n    print(\"     test accuracy  \", accuracy(meta_model, val_loader, \"engineered\"))\n\n| epoch   1 | train accuracy    0.459 | time:  4.19s\n     test accuracy   0.5002215330084182\n| epoch   2 | train accuracy    0.589 | time:  4.00s\n     test accuracy   0.615861763402747\n| epoch   3 | train accuracy    0.636 | time:  4.65s\n     test accuracy   0.6278245458573327\n| epoch   4 | train accuracy    0.643 | time:  3.95s\n     test accuracy   0.6371289322108994\n| epoch   5 | train accuracy    0.645 | time:  3.90s\n     test accuracy   0.6371289322108994\n| epoch   6 | train accuracy    0.650 | time:  3.75s\n     test accuracy   0.640230394328755\n| epoch   7 | train accuracy    0.649 | time:  3.73s\n     test accuracy   0.6357997341603899\n| epoch   8 | train accuracy    0.652 | time:  3.78s\n     test accuracy   0.642002658396101\n| epoch   9 | train accuracy    0.649 | time:  3.51s\n     test accuracy   0.6468763845813026\n| epoch  10 | train accuracy    0.651 | time:  3.73s\n     test accuracy   0.640230394328755\n| epoch  11 | train accuracy    0.649 | time:  3.52s\n     test accuracy   0.6513070447496677\n| epoch  12 | train accuracy    0.654 | time:  3.54s\n     test accuracy   0.641116526362428\n| epoch  13 | train accuracy    0.654 | time:  3.74s\n     test accuracy   0.6504209127159947\n| epoch  14 | train accuracy    0.655 | time:  3.39s\n     test accuracy   0.6526362428001772\n| epoch  15 | train accuracy    0.656 | time:  3.48s\n     test accuracy   0.6424457244129376\n| epoch  16 | train accuracy    0.654 | time:  3.31s\n     test accuracy   0.6464333185644661\n| epoch  17 | train accuracy    0.657 | time:  3.27s\n     test accuracy   0.6451041205139566\n| epoch  18 | train accuracy    0.654 | time:  3.38s\n     test accuracy   0.6442179884802836\n| epoch  19 | train accuracy    0.658 | time:  3.29s\n     test accuracy   0.642002658396101\n| epoch  20 | train accuracy    0.660 | time:  3.32s\n     test accuracy   0.6446610544971201\n| epoch  21 | train accuracy    0.658 | time:  3.24s\n     test accuracy   0.6477625166149756\n| epoch  22 | train accuracy    0.657 | time:  3.14s\n     test accuracy   0.6482055826318122\n| epoch  23 | train accuracy    0.658 | time:  3.21s\n     test accuracy   0.6477625166149756\n| epoch  24 | train accuracy    0.656 | time:  3.17s\n     test accuracy   0.6455471865307931\n| epoch  25 | train accuracy    0.655 | time:  3.09s\n     test accuracy   0.6530793088170137\n\n\n\naccuracy(meta_model, val_loader, \"engineered\")\n\n0.6530793088170137\n\n\nWoah! Only using metadata, we achieved around 65% accuracy! This much better than our base rate, and higher than the lyrics only classification approach.\n\nper_class_accuracy(meta_model, val_loader, mode=\"engineered\")\n\n{0: 0.5689655172413793,\n 1: 0.6224489795918368,\n 2: 0.6128364389233955,\n 3: 0.7242647058823529}\n\n\nWe are also outperforming all of our base rates for each genre! Once again rock is our highest performer (around 72%) showing its distinction from other genres in categories like instrumentalness, energy, movement/places, etc."
  },
  {
    "objectID": "posts/Deep Music Genre Classification/index.html#combined-feature-classification",
    "href": "posts/Deep Music Genre Classification/index.html#combined-feature-classification",
    "title": "Deep Music Genre Classification",
    "section": "Combined Feature Classification",
    "text": "Combined Feature Classification\nWe have now explored successful approaches using lyrics and using metadata. Lets see how we perform when we combine the two!\n\nclass CombinedNet(nn.Module):\n    \n    def __init__(self, vocab_size, embedding_dim, num_class, num_features):\n        super().__init__()\n    \n        # engineered features pipeline\n        self.eng_pipeline = nn.Sequential(\n            nn.Linear(num_features, 18), \n            nn.ReLU(),\n            nn.Linear(18, 12), \n            nn.ReLU(),\n            nn.Linear(12, 8)\n            )\n        \n        # text pipeline \n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.relu = nn.ReLU()\n        self.fc = nn.Linear(embedding_dim, 8)\n\n        # combine the two pipelines\n        self.combine = nn.Sequential(\n            nn.Linear(16, 12), \n            nn.ReLU(),\n            nn.Linear(12, 8), \n            nn.ReLU(),\n            nn.Linear(8, num_class)\n        )\n    \n    def forward(self, x):\n        x_text, x_eng = x\n        x_text = x_text.to(device)  \n        x_eng = x_eng.to(device)\n        \n        # text pipeline:\n        x_text = self.embedding(x_text)\n        x_text = self.relu(x_text)\n        x_text = x_text.mean(axis = 1)\n        x_text = self.fc(x_text)\n\n        # engineered features pipeline:\n        x_eng = self.eng_pipeline(x_eng)\n\n        # then, combine them with: \n        x_comb = torch.cat([x_text, x_eng], dim = 1).to(device)\n        \n        # pass x_comb through a couple more fully-connected layers and return output\n        return self.combine(x_comb)\n\nThe main ideas from the other pipelines remain. We first train separately following similar procedures to above, then we concatenate the features and pass them through several more fully-connected layers. Notably changes come in our text pipeline where we removed a fully connected layer and our dropout. These changes were a result of trail and error testing. Additionally, we bring our separate pipelines together before they are compressed back into our four-class classification.\n\ncombined_model = CombinedNet(vocab_size, embedding_dim, num_class, num_features).to(device)\nsummary(combined_model, input_Size = (8, max_len))\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nCombinedNet                              --\n├─Sequential: 1-1                        --\n│    └─Linear: 2-1                       414\n│    └─ReLU: 2-2                         --\n│    └─Linear: 2-3                       228\n│    └─ReLU: 2-4                         --\n│    └─Linear: 2-5                       104\n├─Embedding: 1-2                         976,736\n├─ReLU: 1-3                              --\n├─Linear: 1-4                            264\n├─Sequential: 1-5                        --\n│    └─Linear: 2-6                       204\n│    └─ReLU: 2-7                         --\n│    └─Linear: 2-8                       104\n│    └─ReLU: 2-9                         --\n│    └─Linear: 2-10                      36\n=================================================================\nTotal params: 978,090\nTrainable params: 978,090\nNon-trainable params: 0\n=================================================================\n\n\nEvidently, our model once again has a huge amount of trainable parameters. Lets see how they do!\n\nEPOCHS = 25\nfor epoch in range(1, EPOCHS + 1):\n    train(combined_model, train_loader, \"both\")\n    print(\"     test accuracy  \", accuracy(combined_model, val_loader, \"both\"))\n\n| epoch   1 | train accuracy    0.495 | time:  5.70s\n     test accuracy   0.5423128046078866\n| epoch   2 | train accuracy    0.564 | time:  5.37s\n     test accuracy   0.5560478511298184\n| epoch   3 | train accuracy    0.571 | time:  5.38s\n     test accuracy   0.5724412937527692\n| epoch   4 | train accuracy    0.584 | time:  5.38s\n     test accuracy   0.5799734160389898\n| epoch   5 | train accuracy    0.594 | time:  5.33s\n     test accuracy   0.5516171909614532\n| epoch   6 | train accuracy    0.611 | time:  5.45s\n     test accuracy   0.5990252547629596\n| epoch   7 | train accuracy    0.631 | time:  5.34s\n     test accuracy   0.6322552060256978\n| epoch   8 | train accuracy    0.644 | time:  5.38s\n     test accuracy   0.615861763402747\n| epoch   9 | train accuracy    0.654 | time:  5.48s\n     test accuracy   0.6473194505981391\n| epoch  10 | train accuracy    0.669 | time:  5.75s\n     test accuracy   0.6464333185644661\n| epoch  11 | train accuracy    0.677 | time:  6.30s\n     test accuracy   0.6566238369517058\n| epoch  12 | train accuracy    0.692 | time:  5.92s\n     test accuracy   0.6544085068675233\n| epoch  13 | train accuracy    0.699 | time:  6.66s\n     test accuracy   0.6575099689853788\n| epoch  14 | train accuracy    0.711 | time:  6.78s\n     test accuracy   0.6575099689853788\n| epoch  15 | train accuracy    0.720 | time:  7.07s\n     test accuracy   0.6326982720425344\n| epoch  16 | train accuracy    0.729 | time:  7.26s\n     test accuracy   0.66371289322109\n| epoch  17 | train accuracy    0.741 | time:  7.66s\n     test accuracy   0.6548515728843598\n| epoch  18 | train accuracy    0.748 | time: 10.54s\n     test accuracy   0.6526362428001772\n| epoch  19 | train accuracy    0.759 | time:  9.17s\n     test accuracy   0.6570669029685423\n| epoch  20 | train accuracy    0.765 | time:  8.58s\n     test accuracy   0.6641559592379265\n| epoch  21 | train accuracy    0.776 | time:  6.94s\n     test accuracy   0.6495347806823216\n| epoch  22 | train accuracy    0.784 | time:  6.50s\n     test accuracy   0.6486486486486487\n| epoch  23 | train accuracy    0.789 | time:  6.38s\n     test accuracy   0.66371289322109\n| epoch  24 | train accuracy    0.800 | time:  6.19s\n     test accuracy   0.6544085068675233\n| epoch  25 | train accuracy    0.811 | time:  6.24s\n     test accuracy   0.6260522817899867\n\n\n\naccuracy(combined_model, val_loader, \"both\")\n\n0.6260522817899867\n\n\nAfter twenty-five epochs, we achieved an accuracy of around 62% which is slightly disappointing. If we look closely at the evolution of the our testing accuracy, we were steadily in the region of around 65% for a while. This drop may be a part of the training process or may be a reflection of the beginning of our model overfitting to the training data.\n\nper_class_accuracy(combined_model, val_loader, mode=\"both\")\n\n{0: 0.6839080459770115,\n 1: 0.4872448979591837,\n 2: 0.6977225672877847,\n 3: 0.7046568627450981}\n\n\nCurse you Jazz! We are doing significant better on all the other genres apart from jazz. This may be because of jazz lyrics being slightly less theme driven combined with the atypical structure of jazz music. Maybe swing rhythms, tempo changes and odd time signatures don’t fit neatly into any given category along with the lyrics."
  },
  {
    "objectID": "posts/Final Project Blog/main.html",
    "href": "posts/Final Project Blog/main.html",
    "title": "Evolution Based Weight Vector Optimization",
    "section": "",
    "text": "Project GitHub\n\nAbstract\nThis blog post explores the application of the principals of evolution to weight vector optimization problems. A comprehensive evolutionary optimizer class, with hyperparameters allowing for control over selection, inheritance, diversity, mutations rates, and mutation styles, was created. Exploratory experiments were then run to attempt to understand the strengths and limitations of the various values for each of these hyperparameters. Experiments were performed on generated data as well as the MNIST dataset. Due to computation limitations, a complete optimization loop on the MNIST dataset was out of scope for this project. However, a final accuracy of 82% was achieved.\n\n\nIntroduction\nThis blog post explores how features of evolution in nature can inspire solutions to overcoming the shortcomings of gradient descent. Gradient Descent only works on differentiable loss functions, meaning it can become stuck in local loss minima when attempting to model non-convex loss functions. In other words, gradient descent cannot explore the entire solution space on nondifferentiable loss functions. This limitation can be overcome by harnessing the characteristics of evolution and natural selection in nature. Evolution has a wide variety of applications concerning Machine Learning, but this project focuses on its applications to weight vector optimization Telikani et al. (2021).\nLewontin identifies 3 key population characteristics for evolution: phenotypic variation in a population, differential fitness, and fitness must be heritable Lewontin (1970). With these 3 characteristics, evolution then occurs as ‘fitter’ individuals are better able to pass on their traits to future generations, while less fit individuals are not. At the individual level, evolution requires a blueprint, self-replication, mutation, and selection. By applying these principles to machine learning models, this blog post explores the strengths and limitations of evolutionary principles when applied to weight vector optimization in machine learning. To satisfy the requirement of phenotypic variation, each evolutionary optimizer has an entire population of weight vectors storing different weights. The different weights result in different losses, which in combination with selection pressures regarding the resulting different losses, satisfy the differential fitness requirement. With weight vectors serving as our genetic blueprint, those weight vectors can be duplicated to create or refill the population of weight vectors. Slight random adjustments to those weight vectors during replication serve as the mutations, ensuring the continuation of phenotypic variation. A variety of methods can be used to eliminate population vectors during an iteration, including loss and diversity, which function as selection. Eliminating high-loss weight vectors allows only vectors with high accuracy to pass on their characteristics, while eliminating low diversity can ensure that the solution space is adequately explored. Through the implementation of hyperparameters, many variations of evolutionary machine learning algorithms are explored to better understand their strengths and weaknesses.\nThe many hyperparameters are then tested on both generated and real data from the MNIST dataset to develop initial hypotheses regarding the optimal parameterization for evolutionary weight vector optimization to succeed.\n\n\nValues Statement\nThe potential users of the evolutionary-based weight vector optimization class are researchers, data scientists, and developers, especially those who work on non-differentiable problems with which gradient descent-based solutions struggle. Our class provides both a potential solution to overcoming the limitations of gradient descent on non-differentiable classification problems and serves as a potential benchmark against which other algorithms can be compared.\nOne major potential impact of the widespread use of our algorithm, or similar ones, is the increase in computational power required to run them. Because each epoch of an evolutionary algorithm requires the computation of an entire population of new weight vectors, the computational power required for an epoch is higher than most algorithms. This has potential positive implications for the manufacturers of computational chips and the owners of servers. On the other hand, the potential negative effects of increased energy and material consumption to perform these computations cannot be overlooked either.\nBecause the majority of our work was focused on the creation of a class, and not the optimization of a specific algorithm, the potential for positive and negative impacts of our class depends on who gains access to the class and what they decide to do with it.\n\n\nMaterials and Methods\nProject dependencies: - torch - numpy - pandas - scikit-learn - matplotlib\nOur evolutionary optimizer translates biological principles into a deep neural network optimiser. Biological evolution, and thus our algorithmic approach, rely on four core attributes: blueprint, self-replication, mutation, and selection. Our “blueprints,” genes or DNA in the natural world, are our weight vectors for the parameters of the neural network. We begin with an initial “population” of \\(N\\) such vectors that are sampled uniformly at random. In each generation, every individual is evaluated on a mini-batch of examples, combining cross-entropy loss (exploitation) with an optional diversity penalty (exploration). The lowest‐loss individuals (and occasionally a small “sneaker” fraction of high-loss outliers) serve as parents for the next generation. Some elite low-loss survivors carry forward unchanged. New offspring are created via uniform crossover, where each weight entry, or gene, is inherited from \\(k\\) randomly chosen parents, then mutated by adding small Gaussian or Laplacian noise with some probability. Optionally, each child can receive a single gradient‐descent step to fine-tune its accuracy. Initially, we relied on synthetic binary-classification data generated using torch.rand to train and validate our evolutionary approach. This allowed us to develop a proof of concept that evolution could, in fact, solve problems and that our selection, crossover, mutation, etc., behaved as expected before we moved on to real-world inputs.\n\n\nExample of MNIST Digits\n\nWe decided to employ our evolutionary approach to the MNIST handwritten-digit dataset LeCun, Cortes, and Burges (2010). This dataset is made up of 70,000 gray-scale images of size 28×28 pixels, labeled 0–9. We accessed the dataset through torch datasets. In smaller experiments with the MNIST dataset, we opted to draw a random subset of anywhere from 1,000 to 20,000 digits to improve computational efficiency. Although the smaller subsets enabled rapid prototyping, they may have overrepresented certain rarer handwriting styles and potentially skewed accuracy.\n\n\nHyperparameters\n\nProof of concept/vanilla evolution:\n\n\nSelection Processes:\nIn nature, genetic traits are passed from one generation to the next by individuals that survive and successfully reproduce. These survivors make up the gene pool of their generation, while those that fail to reproduce are effectively excluded from the evolutionary process. In our implementation, we emulate this principle by defining fitness based on standard cross-entropy loss or diversity-augmented loss, depending on the user. At each generation, we sort the population in a minheap based on loss, then the top 50% (the half with the lowest loss) are selected to form the gene pool. The other half does not have the chance to reproduce. In the next section, we will dive into how we handle creating the next generation from the gene pool.\n\n\nIllustration of Gene Pool Selection\n\nMirroring the random nature of evolution, we incorporate some chance in the makeup of our gene pool. A small number of lower-performing individuals (10% by default) are included in the gene pool with low probability. These individuals, whom we call sneakers, introduce genetic variation that helps maintain a diversified population and prevents premature convergence.\n\n\nIllustration of Sneaker Population\n\nFinally, we employ an elitist strategy to preserve our high-performing solutions. Each generation, a percentage of the top performers based purely on cross-entropy loss are included in the gene pool and also survive unchanged and unmutated to the next generation. This preserves the integrity of the best solutions by keeping a lineage of high-performing individuals.\n\n\nOverview of New Generation Gene Makeup\n\n\n\nInheritance and Parent Quantity:\nAt each iteration of our evolutionary optimization, following the creation of a ‘gene pool’ in the selection stage, the population must be replenished with new individuals. There are three ways that this can be accomplished. 1: All new individuals are new randomized weight vectors with no input from the gene pool. 2: Each new individual has a single parent randomly selected from the gene pool from which its weights are inherited with random mutations. 3: Each individual has n parents randomly selected from the gene pool. Each feature weight is then inherited from the corresponding feature weight of a random one of its parents.\nThe first scenario, with no inherited weight vectors, is a baseline against which our true evolutionary models can be tested. This is not truly evolution, as it does not include any heritability of fitness for new individuals in the population Lewontin (1970).\nThe second Scenario, includes heritability of fitness, but with only a single parent for each child individual, the diversity can be expected to be more limited.\n\n\nDiagram of Inheritance when num_parents = 1\n\nThe Third Scenario, allows for a slightly reduced heritability of fitness, with the addition of diverse new individuals produced with each generation. The diversity rate is specifically limitted by the mutation_rate and mutation_intensity hyperparameters.\n\n\nDiagram of Inheritance when num_parents = 2\n\nFunctionally, this process occurs after selection has occured, and an overall gene pool of parents has been created. A random sampling with replacement is then performed on that gene pool, in which each new child is assigned n, the value of the num_parents hyperparameter passed to the function, parents from the gene pool. For each weight in each child’s weight vector, a random one of that child’s n parents is then chosen from which it inherits that specific weight. If num_parents = 0, then every child recieves a completely random weight vector. Once the weights have been assigned, the child weight vector is then mutated.\nAs discussed in the results section, the choice of the number of parents can have a significant impact on loss, accuracy, and diversity.\n\n\nHybridizing evolution with gradient descent:\nOur approach to evolutionary optimization incorporates a gradient-based refinement step that allows individuals to local optimize their performance (slightly) after being created. In essence, this hybrid evolutionary-gradient approach combines the global search strengths of evolutionary algorithms with the precise, local updates enabled by backpropagation. For each new individual generated during the evolutionary step, we apply a single gradient update to refine its weights. This is accomplished using a method implemented within the model that performs a forward pass, calculates cross-entropy, and uses PyTorch’s automatic differentiation to compute gradients. Weights are then updated according to the direction of the negative gradient, and scaled by a learning rate set to 0.5.\nThe backpropagation step is called once per individual in the evolutionary loop, immediately after crossover and mutation have produced a new weight vector candidate. The updated individual is then re-inserted into the population. By integrating this light update of gradient descent, the optimizer benefits from enhancing convergence rates - while still being able to prioritize diversity - with fewer generations of evolution.\n\n\nComputing Diversity and Diversity-Based Loss:\nOur evolutionary optimization implementation includes a mechanism for encouraging population diversity by directly incorporating a diversity term into the model’s loss function. Diversity is measured over the entire population of weight vectors, with four distinct methods implemented to quantify it. These include Euclidean distance, cosine dissimilarity, standard deviation, and variance. The Euclidean distance metric calculates the mean spatial difference between every pair of individuals in the population. Cosine dissimilarity measures the angular dissimilarity of weight vectors by computing one mines the cosine similarity between weight vectors. The standard deviation and variance metrics, on the other hand, operate across the whole population of weight vectors by computing the average distribution/variance of all weight vectors within a generation.\nOnce computed, the diversity score is used to modify the model’s loss. Specifically, during each evaluation of an individual weight vector in the population, the standard cross-entropy loss is calculated and then a diversity term is subtracted from it. This diversity term equals the above mentioned diversity value scaled by a user-set diversity coefficient. The effect of this subtraction is that models with higher diversity scores receive a lower total loss, incentivizing the optimizer to explore a broader range of solutions. This diversity-aware loss is only applied when explicitly enabled through a boolean flag in the model, giving flexibility for experiments that compare/evaluate the performance of diversity-based and non-diversity based evolutionary optimization.\n\n\nAdjustment from binary to multi-class classification:\nBecause our target task is classification on the MNIST dataset - which involves 10 possible output classes (digits 0 through 9), we implemented multiclass classification using Pytorch’s CrossEntropyLoss function. Unlike binary cross-entropy, which assumes a binary classification problem and compares scalar outputs to binary labels, cross-entropy loss compares a vector of probabilities (logits) against a single target value label. This function internally applies a softmax operation which evaluates the likelihood of each logit being the right output class.\nIn our implementation, the CrossEntropyLoss function is used in both the model’s forward loss evaluation and backpropagation step. This ensures that each prediction is treated as a multiclass decision and that the model can properly learn to distinguish between all 10 classes in the MNIST dataset.\n\n\nMutation Methods:\nOne thing that we created in our vanilla EVO optimizer was a random mutation mechanism. This mutation mechanism let’s us assign a small probability that each of the weight’s entries’ values can be nudged by a certain amount positively or negatively. This nudge and its intensity is modeled by a normal distribution around the current value, and what we call “Mutation Intensity” is the standard deviation of that normal distribution. This ensures that we are constantly updating our weights randomly, and that there is a chance for weights to get better. What we noticed is that only using normal distribution might not be sufficient in achieving fast convergence. Because the normal distribution’s tails flatten with the X-axis quickly, it does not give the slightest opportunity for the model to get an aggressive nudge.\n\nThis led us to explore different distributions that also share the characteristic that ensures the nudge is usually not too aggressive, but also allows ever so rarely for it to change the weight’s entry significantly. This distribution that we introduced is the Laplacian distribution.\n\nThis became another hyperparameter that allows us to see how different mutation methods affect different models that our EVO optimizer tries to solve.\n\n\n\nResults\n\nChoices in Selection Processes:\nBy default, we select the best 50% of the population to enter the gene pool, however, this is a hyperparameter that users can play with. We conducted some experiments on a small subset of 1000 digits from the MNIST dataset to examine how different gene pool sizes (10%–90% of the population) would affect our accuracy, loss, and diversity over 500 generations.\n\n\nGene Pool Size vs Accuracy\n\n\n\nGene Pool Size vs Loss\n\n\n\nGene Pool Size vs Diversity\n\nThere are several interesting things to note about these figures. Focusing on the extremes first, only picking 10% of the best individuals is advantageous if we look purely at accuracy. However, this came at the cost of significantly reducing diversity, with such a small portion of the population passing through at each generation. Having too homogeneous a population can lead to getting stuck in local minima without exploring the wider loss landscape. On the other hand, having too many members of the population reproduce increases exploration of the loss landscape, but reduces selection pressure, as individuals with suboptimal solutions continue to reproduce. We can see this illustrated above as the accuracy lags far behind all of the other gene pool sizes. Keeping the best half performed is a strong middle ground with comparatively great accuracy, second only to keeping the top 10%, while remaining diverse.\nWe also investigated the effects of varying the probability that “sneakers”—individuals from the bottom 10% of the population—could enter the gene pool. We tested probabilities from 0–45%.\n\n\nSneaker Probability vs Accuracy\n\n\n\nSneaker Probability vs Loss\n\n\n\nSneaker Probability vs Diversity\n\nInterestingly, across a range of sneaker probabilities, we didn’t observe much variation in loss or diversity. So it doesn’t impact our learning dynamics to a noticeable degree. However, having a 45% sneaker probability performed quite well, accuracy-wise. This may be a reflection of random variation of our dataset or starting genepool, but it may also suggest that a degree of genetic noise can occasionally help guide the population out of local minima. In future experiments, it would be insightful to set the hyperparameter to be above 50% and see the results.\nFinally, we explored the impacts of elitism by varying the percentage of top-performing individuals who we carried unchanged to the next generation.\n\n\nElitist Population Size vs Accuracy\n\n\n\nElitist Population Size vs Loss\n\n\n\nElitist Population Size vs Diversity\n\nWhen we have too many elites, we slow down evolutionary convergence. We aren’t introducing enough change from generation to generation to explore the landscape and improve our solution. We can see evidence of this in our stunted accuracy, low diversity, and higher loss when we increase the size of our elite population. However, when we eliminate elites or keep only 5%, we see noticeable improvements. Our loss is converging faster, we maintain a diverse population, and our accuracies after 500 generations are the highest. Keeping the elite population helps our accuracy, outperforming the population without elites by over 5% over 500 generations. Overall, we observed that on MNIST, modest elitism provides a valuable balance between preserving high-quality solutions and allowing diversity within the population.\n\n\nInheritance and Parent Quantity:\nTwo experiments were performed to explore the limitations and strengths of different inheritance methods, specifically the adjustment in the number of parents from which each child weight vector receives it’s own weight vector values.\nFor both experiments below hyperparameters that were held constant were:\n-Survivor Ratio: 0.1\n-Fitness Ratio: 0.5\n-Sneaker Probability: 0.01\n-Sneakers ratio: 0.1\n-mutation rate: 0.05\n-mutation intensity: 0.05\n\nGenerated Data Experiment:\nThis generated data experiment explores the performance of our model when varying the num_parents hyperparameter. A multi parent classification experiment was run on generated 2 dimensional data with 0.2 noise and 300 points. The accuracy, loss, and Euclidean diversity was tracked across 300 iterations. The experiment was run with the hyperparameter num_parents set to 0, 1, 2, 3, 5, and 10.\n\n\n\nFigures demonstrating loss, diversity, and accuracy performance of Evolutionary Optimization on Generated data using 0, 1, 2, 3, 5, and 10 parents over 300 iterations\n\nAs seen in the above visualization, Loss and Accuracy were comparable across all quantities of parents, while diversity varied significantly. In particular, with num_parents set to 0 and to a lesser extent 1, diversity was much lower than all other quantities of parents. The accuracy of the 0 parent model also performed worse than the other models over more iterations. With 0 parents evolution is conceptually replaced by random chance. The heritability, defined as a requirement for evolution by Lewontin (1970), is eliminated from the process.\nWhile this had a much smaller impact on this relatively simple experiment of generated data, the implications on a much more complex classification problem, such as MNIST, could be significant.\n\n\nMNIST Experiment:\nA similar, more complex experiment performed on a subset 1000 images from the MNIST dataset tested the accuracy, loss, and diversity of num_parents = 0, 1, 2, 5, 10 over 1000 iterations. As a significantly more complex classification problem, the strengths of including more parents become much clearer.\n\n\n\nFigures demonstrating loss, diversity, and accuracy performance of Evolutionary Optimization on a Subset of the MNIST dataset using 0, 1, 2, 5, and 10 parents over 1,000 iterations\n\nThe benefits of inheritance are clear, as the zero parent model has a significantly higher loss and lower accuracy throughout the 1000 iterations compared to all other models.\nStarting with loss, we can see that the loss for all test groups are relatively similar with the exception of num_parents = 0.\nWith regards to accuracy, we see a more nuanced picture. 0 parents performs poorly throughout the experiment, never reaching 21% accuracy. 1 parent has logarithmic like improvement in accuracy at around 35%. All higher quantities of parents follow a similar trajectory, but with major jumps in accuracy breaking the logarithmic like trend. This can be better understood by looking at diversity levels.\nDue to the random nature of the weight vectors for the 0-parent group, the diversity is constant and extremely high at 65.124. For all other groups, it is clearly shown that more parents results in maintained diversity. As selection occurs, and the population is replenished with weight vectors inherited from the gene pool, diversity decreases overall. But by allowing for more varied combinations from that gene pool, some level of diversity is preserved. This appears to have diminishing benefits as demonstrated by the similar diversity for both 5 and 10 parents.\nThis becomes a problem of optimizing the heritability of fitness and the phenotypic variation mentioned by Lewontin (1970). fewer parents means more pure inheritance of fitness, as the child will more closely resemble its parents. It also means less phenotypic diversity, as completely new weight vectors are less likely to emerge. The opposite with regards to both phenotypic variation and fitness heritability applies. The benefits of multi-parent inheritance are demonstrated by the declining improvement in accuracy when num_parents = 1. The lower diversity compared to the other models, and the importance of diversity in evolutionary algorithms in allowing for the exploration of the solution space, results in poorer performance of the single parent model. A single parent allows for the inheritance of fitness,leading to better performance compared to the 0 parent model Lewontin (1970). However, it does not allow for large enough variation in fitness. With lower diversity, the 1 parent model is less likely to find a global minimum compared to the 2+ parent models. While it does find some form of a local minimum, the lack of diversity results in a drop off in improvement at around 600 iterations, while the models with 2, 5, and 10 parents continue to have spikes in improvement.\nIn the context of classification of the MNIST dataset, evolutionary models benefit from the added diversity resulting from the use of larger quantities of parents contributing weights to each new child in the subsequent generation. While more computing power, and more iterations are required to truly optimize this hyperparameter, these experiments clearly demonstrate the benefits of multi-parent inheritance.\n\n\n\nQuantifying Diversity and Diversity-Based Loss:\nThis section evaluates the effect of diversity-aware loss functions in evolutionary training of a neural network classifier on a subset of the MNIST handwritten dataset. We experimented with four diversity metrics - Euclidean Distance, Cosine Dissimilarity, Standard Deviation (STD) and variance - and measured their influence on test accuracy, cross-entropy loss, and diversity levels in our weight population over 200 generations. Additional hyperparameter tuning was performed for the Cosine diversity metric to explore how mutation rate, mutation intensity, population size, and diversity coefficient influence outcomes.\nThe first experiment (figure 1) compared the test accuracy, loss, and normalized diversity across all four diversity metrics under a fixed training setup. All metrics enabled the model to reach between 75%-81% accuracy over 200 generations, with all other hyperparameters held constant. euclidean distance and STD slightly outperformed others in final diversity. All methods reduced loss substantially within 60 generations. When it came to Normalized diversity, all metrics except for, interestingly, cosine dissimilarity between weight vectors increased/maintained high diversity over time. Cosine dissimilarity diversity rapidly decayed to near-zero within 100 generations, while STD, variance and euclidean distance maintained high diversity levels, suggesting that cosine may be more prone to premature convergence or intrinsically mediates the impact of diverse weight populations.\n\nFigure 1: Comparing test accuracy, loss, and normalized diversity values for all 4 diversity metrics.\nTo better understand the behavior of the cosine dissimilarity metric, we ran additional training with varied diversity coefficients, population sizes, and mutation hyperparameters. The default hyperparameters used were population size 50, mutation rate 0.4, mutation intensity 0.5 and diversity coefficient 0.1. Increasing the diversity coefficient to 0.3 (figure 4) significantly improved diversity values - up to 0.2 - over each generation, confirming that the penalty term has a regulating effect on population diversity. When the diversity coefficient was set to 0.0 (figure 3), the model still trained to reasonable accuracy but showed completely flat diversity values, indicating the diversity term is implemented correctly to at least affect our metric value. Increasing population size to 100 (figure 5) improved diversity over each generation, especially in the first 100 generations, but did not substantially improve test accuracy. This suggests diminishing returns from larger populations in this setting. Raising mutation rate to 0.7 and intensity to 0.8 (figure 6) had a negligible to slightly positive impact on accuracy while maintaining diversity at moderate levels. Accuracy did experience more noisiness under these conditions, but ultimately achieved reasonable levels.\n\nFigure 2: Baseline experiment outputs to provide reference test accuracy, loss, and diversity values for cosine driven loss.\n\nFigure 3: Confirming working implementation of diversity coefficient’s effect on diversity based loss by setting diversity coefficient to 0.0\n\nFigure 4: Results showing the effects of increased diversity coefficient of 0.3 - i.e. higher effect of diversity punishment/reward on loss - on test accuracy, loss, and diversity values.\n\nFigure 5: Results for increased population size of 100 weight vectors on test accuracy, loss, and diversity values.\n\nFigure 6: Results for impact of high mutation rate and mutation intensity on test accuracy, loss, and diversity values.\nIn summary, all four diversity metrics led to successful convergence and comparable final test accuracies, with euclidean distance and STD slightly ahead. Cosine dissimilarity driven diversity tends to descend quickly, requiring further parameter tuning to explore what it takes to keep diversity high. Enabling the diversity penalty to loss had a clear and measurable effect on both training behavior and final diversity levels, validating its implementation. Mutation and population hyperparameters affected convergence stability and final accuracy but had less influence than the choice of diversity metric.\nThis study was constrained by computational limitations, which restricted the breadth of hyperparameter combinations we could explore. In particular, both the population size and the number of generations were limited in order to keep training time feasible. Larger populations and longer training schedules could potentially yield more robust insights into the effects of diversity-aware loss function. Further investigation into the behavior of cosine dissimilarity is warranted. Across multiple experiments we observed a consistent decline in diversity when using this metric. One possible explanation for this is that cosine dissimilarity only measures angular differences between vectors, ignoring their magnitudes. As a result, the population may converge to a set of similarly oriented but differently scaled vectors, which could be interpreted as low diversity by this metric. This limitation could implicitly constrain the optimizer’s ability to maintain variation during training, and future work could test this hypothesis more directly or explore hybrid metrics that include both angular and magnitude components. Additionally, we were limited in the size of training and test batches, which may influence generalization performance. It would be valuable to evaluate how increasing batch size or dataset subset size impact both diversity value and resulting model accuracy. Please note, all of these experiementes were run on a hybridized version of the evolution optimized DNNs which included, for every step of training one gradient descent step on each weight vector. This was done in hopes to reduce runtimes without straying too far from pure evolution. Pure evolution, we speculated, would have needed to require high data inputs, generation numbers, and population sizes to produce valuable results, which did not fit the computational capacities of our computers, nor our time constraints.\n\n\nFinal MNIST Results:\nAfter combining all of our implementations together, we trained a deep neural network with layers [764,32,10] to classify our MNIST dataset. We settled on the following hyperparameters:\nmodel.diversity_coeff = 0.2 optimizer = EvolutionOptimizer(model) optimizer.set_population_size(200) optimizer.use_backprop = False optimizer.set_survivors_ratio(0.1) optimizer.set_fitness_ratio(0.5) optimizer.set_sneaker_prob(0) optimizer.set_mutation_intensity(0.05) optimizer.mutation_rate = 0.05\n\nThe plot above shows that the accuracy rapidly increases during the early generations, indicating that the evolutionary algorithm quickly identifies promising weight vectors, significantly reducing the initial error. This is likely because the selection criteria is too strict so it rapidly eliminates poorly performing members of the population. The curve starts to smooth out, reflecting a deceleration in accuracy improvement as the optimizer converges on better solutions. After around 2000 generations, the accuracy curve stabilizes, indicating that the optimizer has reached a near-optimal solution for the given problem. The final accuracy appears to stabilize around 82%, suggesting that the current hyperparameter settings and genetic operators are effective but may have room for further optimization, possibly through adjustments to mutation rates, diversity coefficients, or parent selection mechanisms.\n\n\n\nConcluding Discussion:\nAs we combined all of our implementations of various evolutionary components, and created our unified optimizer that has significant flexibility in deciding the environment where our weights can evolve to solve the ML problems over time, we were ready to test this optimizer at a famous problem for deep neural networks: Classifying handwritten digits. Tweaking many of our hyperparameters lead to significantly different converging speeds, diversity metrics, and overall performance. This flexibility can be helpful in tailoring our algorithm to work in different context and on different models.\nOur intention from the beginning was to simulate how living creatures solve the problem of survival: Evolution. What encouraged us to explore this algorithm is the beauty of how living beings have evolved to solve the same problems very differently. This diversity that exist in nature is what got us thinking about ways we could achieve this concept in optimizing machine learning models on different datasets. If we can create a population of weights that can reproduce over time, and spread their genes and cross it with one another, what can we notice about the diversity of their solutions? This took us on a journey of simulating this natural process, abstracting it into simpler components, and specifying it to our context.\nOur project worked in many ways: our EVO optimizer managed to get the population to converge through random mutation, and maintain diversity by adjusting the diversity coefficient hyperparameter. We did however see a natural decay in diversity as the exploration phase ends and the exploitation phase begins where the population begin to converge around good solutions it found in the initial phase. Beyond the scope of the optimizer, our project worked in a sense that it provided us with the opportunity to investigate, design, and implement a complex environment for evolution. This has been the project that taught me at least the most about object oriented programming, and has taught us a lot about how to write legible code that will be built on by others.\nOur results are comparable to other evolutionary optimization algorithms in terms of convergence speed and diversity emphasis, however, different implementations have allowed for even more complex design decisions of the environment, more complex selection criteria (like tournament style), adaptive mutation rate, limitations on the mating process. These added complexity unlocks many different combinations of hyperparameters that outperform our simple-er implementation.\nIf we had more time, we would definitely work on improving speed. Currently, our code does not fully utilize GPU. There are a few python for-loops when popping our populations based on total loss and cross entropy loss. These are operations that, when vectorized, could speed up the training process significantly. In addition, we can add more design options for more complex evolutionary algorithms. Also, we would perform a grid search to find the best hyperparameters that would optimize for a deep neural network for handwritten MNIST dataset in terms of accuracy and diversity. Finally, we would implement an inference mechanism that would classify data using majority voting, assuming that the diversity in the population allows for a broader knowledge base to solve the problem, i.e, it would be interesting to see if the phenomenon of the wisdom of the crowd emerges under our current evolutionary algorithm within a population.\n\n\nGroup Contributions:\n\nLukka:\nAs a unit, the whole team contributed to the conceptualization and the early stages of building a working prototype. Lukka worked mainly on implementing, refining, and exploring the selection mechanisms in our evolutionary model. he also helped integrate the Laplacian mutation distribution. Lukka also helped include and streamline my work and the work of others into a central working file. This was work that helped build the base of how we would handle our object-oriented programming approach and handle tuning hyperparameters. He also spent considerable effort and time getting MNIST to run correctly on the Middlebury cluster to facilitate larger-scale experimentation. In all the team meetings, we all spent time digging into one another’s code, learning and helping on implementation, debugging, and developing conceptual frameworks.\n\n\nJiffy:\nFor this project, Jiffy contributed to both the conceptual development and the technical implementation of our evolutionary optimization framework. Early in the project, je created a demo notebook (evolution_demo_warmup.ipynb) that introduced the basic principles of evolutionary algorithms using synthetic data, aiming to outline a clear conceptual framework of evolution’s purpose and potential in our project. Jiffy was primarily responsible for implementing and optimizing the diversity-aware loss framework, including vectorized versions of the Euclidean distance and cosine dissimilarity metrics, as well as additional metrics based on standard deviation and variance. He added support for toggling these metrics and integrating them into the final loss calculation. Jiffy also extended our codebase to support multiclass classification, enabling us to apply our models to the MNIST dataset. Much of Jiffy’s experimentation involved running classification trials with varying diversity metrics and hyperparameters - mutation rate, intensity, and diversity coefficient - which he documented in a jupyter notebook (ExploringDiversity.ipynb). Jiffy wrote an initial training logic and data loading code for MNIST, and developed visualization tools using matplotlib to track accuracy, loss, and diversity across generations. He also implemented the hybrid optimization step, which combines evolution with gradient descent via backpropagation. For the final blog post, Jiffy focused on writing detailed technical explanations of the algorithmic components I implemented, along with reporting and analyzing the results of my experiments. This includes the materials/methods section on hybridizing evolution with gradient descent, computing diversity and how diversity-based loss was implemented, and the transition from binary to multiclass classification. It also includes the results writeup for ‘Quantifying Diversity and Diversity-based Loss’.\n\n\nJames:\nJames’ main contribution to this project was the creation, implementation, and experimentation on the benefits and limitations of adjusting the inheritance process, in terms of how many parents each child’s weight vector has. This included identifying scholarly sources which provided a framework for understanding how each change to our hyperparameters, and in the case of inheritance specifically, in the number of parents influences the forces of evolution Lewontin (1970). The majority his work is found in the multi-parent and multi-parent2 folders, although the important changes to the evo class were eventually merged with the blog-post EVO class. While the most important contributions can be found in /multi-parent2/MultiParent.ipynb, James spent considerable time working to overcome computational limitations of my computer and then working to have my code run on ADA. James wrote the abstract, the introduction, the values statement, and the Inheritance and Parent Quantity subsections of the Hyperparameter and results sections.\n\n\nYahya:\nFor this project, I contributed to both the conceptual foundation and technical implementation of our evolutionary optimization framework. Early in the project, I helped conceptualize and explain various mathematical approaches to designing the optimizer, providing the initial direction for our implementations. I also supplied resources to support my teammates in understanding the theoretical aspects of evolutionary algorithms. I implemented a proof-of-concept version of the optimizer, integrating basic mutation, crossover, and selection mechanisms, which served as the foundation for our more complex final implementation. This included designing and implementing the core structure of the optimizer, which became the base for further development. I also investigated different diversity metrics and incorporated them as terms in the loss function to maintain diverse populations and reduce the risk of premature convergence. I designed and refined the OOP structure throughout the project. Additionally, I designed the mutation mechanism and introduced a Laplacian distribution as an alternative to Gaussian mutation, allowing for a more varied exploration of the solution space. To address computational bottlenecks, I collaborated with Professor Vaccari to set up scripts for running Jupyter servers on Middlebury’s GPU nodes, resolving runtime issues for the team. Finally, I helped assemble the final version of the EVO optimizer and experimented with different hyperparameter combinations to fine-tune the model and achieve the results presented in our report.\n\n\n\nPersonal Reflection:\nThis project helped me strengthen my understanding of how machine learning problems are evaluated. I had a loose understanding of the loss landscape and how perceptron worked on a base level. Framing machine learning in evolutionary terms, helped me understand the wide array of possible solutions that can exist for certain problems. Moreover, slightly to my dismay, I finally encountered the impetus for optimizing code runtime. This was the first project for any class where I had ever had issues running my code fast. This introduced me to the beautiful world of the Middlebury ADA Cluster. Thinking about optimization and running things on the GPU made me think a lot more about the impacts of the code I was writing. How many gallons of water did my final project “guzzle”?\nCommunication was a struggle at times on this project, as I, on multiple occasions, felt down when something I or someone else had implemented was already in the codebase or was being worked on by someone else. However, when we stayed strong on our weekly and spontaneous meetings in person, things felt a lot better.\nLooking back on the project, I am immensely proud of the work we achieved. We set out with pretty ambitious plans for applying our application, not taking into account the time we would spend developing and testing it and the different hyperparameters. I valued the time spent on fine-tuning as it produced interesting results thinking both about machine learning and about evolution. I really wanted to explore symbolic regression but unfortunately we couldn’t access the established dataset, so creating one of our own may be a possible evolution of the project (no pun intended). Looking back I am also somewhat sad that we didn’t think about our end product much, we honestly just planned to run it on data and didn’t put much thought into cool visualizations. I could imagine viewing solutions in a dimension reduced space as one possible cool visual.\nThe experience of undertaking this project and the course in general has opened the doors for me to explore personal project of my own concerning machine learning, information visualization, and technical writing. The blog post website is something I hope to update with some of my previous projects and to showcase some new ones for my family, friends, graduate schools, and employers to check out! I will also be more equipped to work in larger group settings valuing active communication and task delegation.\n\n\n\n\n\nReferences\n\nLeCun, Yann, Corinna Cortes, and CJ Burges. 2010. “MNIST Handwritten Digit Database.” ATT Labs [Online]. Available: Http://Yann.lecun.com/Exdb/Mnist 2.\n\n\nLewontin, R. C. 1970. “The Units of Selection.” Annual Review of Ecology and Systematics 1: 1–18. http://www.jstor.org/stable/2096764.\n\n\nTelikani, Akbar, Amirhessam Tahmassebi, Wolfgang Banzhaf, and Amir H. Gandomi. 2021. “Evolutionary Machine Learning: A Survey.” ACM Comput. Surv. 54 (8). https://doi.org/10.1145/3467477."
  },
  {
    "objectID": "posts/Implementing the Perceptron Algorithm/index.html",
    "href": "posts/Implementing the Perceptron Algorithm/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nAbstract\nThis blog post explores the implementation of the perceptron algorithm in Python exploring both the standard and mini-batch versions of perceptron. We demonstrate how the algorithm iteratively adjusts the weight vector to reduce misclassifications by updating based on individual data points or averaged contributions from mini-batches. We discuss how and why our implementation works, demonstrate its capacities, and discuss parameter tuning and time complexity. Notably, we show the performance of the algorithm on linearly separable and non-linearly separable data. Through this post, I expanded my understanding of how basic learning algorithms work behind the scenes for classification purposes.\n\n\nImplementation\nIn perceptron.py, we implement the perceptron algorithm. The .score(), .loss() and .step() methods are pretty intuitive. The .grad() (Gradient Descent) I will address in further depth. The goal of perceptron is to adjust our weights w so that we push our line in the right direction to minimize our loss function. In other words, push our line in a direction that reduces the number of points we misclassify at each update. We do so with respect to a single point. We randomly select a point (or a set of points in mini-batch then average them) then see if it is misclassified, we push our weights w closer to classifying that point correctly. This push can be scaled the the learning rate, which is 1 by default, smaller values will make smaller jumps.\nSome important highlights in the code are:\n\nself.score(X) * y_ computes a score for a given point that is positive when we accurately classify a point\n(( ... &lt; 0 ) * y_) is an indicator random variable that will only add to our weights when we find a misclassified point, otherwise our binary encoding will nullify the rest of the term.\n.unsqueeze(1) * X Allows for us to use mini-batches and not encounter size broadcasting errors with matrices and vectors in PyTorch.\n\nOur actual update happens in the .step() method completing the implementation.\nFor the first part of the analysis we will be using a default batch size of k = 1 and learning rate of 1 which is the default perceptron algorithm as opposed to mini-batch perceptron that we will explore later.\nBelow we are using Professor Chodrow’s functions to generate and visualize linearly separable data to test our implementation.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nAs we can observe we have generated a set of linearly separable data that our perceptron should be able to separate with 100% accuracy (equivalent to 0 loss)\n\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y)\n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nloss\n\ntensor(0.)\n\n\nGreat! It worked, we have a perceptron that minimizes our loss to zero! We can see below the step by step process as the perceptron algorithm iteratively finds the line that separates the squares and the circles.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nHere is the line we found:\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n\n\n\n\n\n\n\n\n\n\nExperiments\nLet us delve into how we arrived to this line iteratively on two dimensional data. First, we are going to have a look at a situation where we know our data is linearly separable. We are going to explore this with a slightly modified function from the lecture notes. This skeleton implements the iterative aspect of perceptron. We loop our code until we have reached a loss score of 0 (separated the two clusters). Within each loop, we choose a point at random and find the loss score of that point given our current line. Then depending on whether or not we correctly classified it using our current line, we shift the line in the direction that would lead to correctly classifying that point. We do this, again, until our overall loss is zero.\n\ntorch.manual_seed(3141)\nX, y = perceptron_data(n_points=50, noise=0.3)\nn = X.shape[0]\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nplot_data = []\nloss_vec = []\n\nloss = 1\nwhile loss &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n        plot_data.append((old_w, torch.clone(p.w), i, loss))\n\n\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex=True, sharey=True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1: 0, 1: 1}\n\nplot_data = plot_data[-6:]\n\nfor current_ax, (old_w, new_w, i, loss) in enumerate(plot_data):\n    ax = axarr.ravel()[current_ax]\n    plot_perceptron_data(X, y, ax)\n    draw_line(old_w, x_min=-1, x_max=2, ax=ax, color=\"black\", linestyle=\"dashed\")\n    draw_line(new_w, x_min=-1, x_max=2, ax=ax, color=\"black\")\n    ax.scatter(X[i, 0], X[i, 1], color=\"black\", facecolors=\"none\", edgecolors=\"black\", \n               marker=markers[marker_map[2 * (y[i].item()) - 1]])\n    ax.set_title(f\"loss = {loss:.3f}\")\n    ax.set(xlim=(-1, 2), ylim=(-1, 2))\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe above figure shows the last six iterations of the perceptron as it is adjusts the weights based on local loss on data that we know is linearly separable. What is interesting to not here is that the line is not narrowly converging towards a loss of zero but rather, adjusts for each point without respect to the whole. The result of this method is that our loss seemingly jumps around in directions that may seem counter intuitive until we reach our loss of zero and terminate.\nThe graph below illustrates this quite well:\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThis is all nice and dandy when we have access to linearly separable data, but what about when our data is not linearly separable. In such a case, it is impossible to have a loss of zero. So we will implement a maximum number of iterations so that our code doesn’t run forever to no avail.\n\ntorch.manual_seed(124816)\nX, y = perceptron_data(n_points=100, noise=0.8)\nn = X.shape[0]\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nmax_iter = 1000 # Maximum number of iterations\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min=-2, x_max=3, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-2, 3), ylim=(-2, 3))\nplt.show()\n\n\n\n\n\n\n\n\nIn the figure above we can observe first that our data is not linearly separable, and thus even after 1000 iterations we achieved a loss score that is not terrible, but not zero.\nBelow we can see the evolution of the loss over these iterations, and note that there is not point with loss zero.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nplt.ylim(0, 0.55)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nWith non linearly separable data we can also see that in some cases our loss can be above 50% misclassified for certain lines. In addition, our final iteration was not a representation of the best we could get, rather where we were after 1000 iterations. We can see that in earlier updates we achieved closer to 15% misclassified.\nThus far we have only been working with two dimensional data, however the perceptron algorithm works with higher dimensional data! Below we are going to run the algorithm on a data set with seven dimensions and observe the evolution of the loss as we iterate.\n\ntorch.manual_seed(2003)\nX, y = perceptron_data(n_points=100, noise=1.72, p_dims=7)\nn = X.shape[0]\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nmax_iter = 1000 # Maximum number of iterations\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nplt.ylim(0, max(loss_vec) + 0.05)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nWe can see that over 1000 iterations our loss never reached zero. We can also note somewhat of a plateau of points around 0.25 that we would lead me to believe that the data is not linearly separable. We can have linearly separable data in seven dimensions as illustrated below. I chose seven because I like the number 7…\n\ntorch.manual_seed(2003)\nX, y = perceptron_data(n_points=100, noise=0.5, p_dims=7)\nn = X.shape[0]\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nmax_iter = 1000\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nplt.ylim(0, max(loss_vec) + 0.05)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAs we can see, in the end we do achieve a loss score of zero indicating we can accurately separate and classify the different sorts of points with a hyperplane of sorts in \\(\\mathbb{R}^7\\). This data was achieved by simply turning down the noise parameter when we generate points.\n\n\nMinibatch Perceptron\nHere we implement the mini-batch perceptron algorithm that computes each update using \\(k\\) random points at once.\nWe begin with a k = 1 that performs similar to regular perceptron.\n\ntorch.manual_seed(6791)\nX, y = perceptron_data(n_points=100, noise=0.25)\nn = X.shape[0]\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nk=1\n\nmax_iter = 1000\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i, k)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min=-2, x_max=3, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-2, 3), ylim=(-2, 3))\nplt.show()\n\n\n\n\n\n\n\n\nWe separate the two clusters just as we would in regular perceptron, and achieve a zero loss score.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nNext we with use k = 10 that can still find a separating line in two dimensions on the same data.\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nk=10\n\nmax_iter = 1000\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i, k)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min=-2, x_max=3, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-2, 3), ylim=(-2, 3))\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nGiven the same data set having a batch size of k = 10 increased the number of iterations we needed to find the separating line.\nWhat will our results be when the batch size is our entire dataset? That is k = n where n = 100. Lets first try it on the same dataset.\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nk=n\n\nmax_iter = 1000\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i, k)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min=-2, x_max=3, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-2, 3), ylim=(-2, 3))\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nWe can see that in this case we are able to find the separating line much faster!\nNow lets see what we can learn from using mini-batch perceptron on data that is not linearly separable. For this we will also tune our learning rate to be 0.01 so we take smaller steps at each iteration.\n\ntorch.manual_seed(1017)\nX, y = perceptron_data(n_points=200, noise=0.7)\nn = X.shape[0]\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nk=n\nlearning_rate = 0.01\n\nmax_iter = 1000\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i, k, learning_rate)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min=-2, x_max=3, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-2, 3), ylim=(-2, 3))\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nWe observe that even with non linearly separable data that the algorithm is still able to converge as we take smaller steps towards a line that fits pretty well!\n\n\nDiscussion\nThe runtime complexity of an iteration of the perceptron algorithm is O(p). This comes from doing the dot product in our .score() function on a matrix that is size \\(n \\times p\\), where \\(n\\) is the number of points and \\(p\\) is the number of features. In addition, updating our weights w is the same complexity. This is independent of the number of points \\(n\\).\nBased on this, when we do minibatch perceptron, we are doing k dot products which would give us O(kp). In some cases this might depend on \\(n\\) as it did in our final example where we set k = n. So while mini-batch helps us with non linearly separable data, it comes at the expense of increased time complexity.\n\n\nConclusion\nIn this post, we implemented the perceptron algorithm and discussed it’s inner workings. We also explored the benefits of mini-batch processing that although it’s per-iteration complexity is O(kp), can improve convergence characteristics for non linearly separable data. Our experiments on both linearly separable and non-linearly separable datasets highlighted the importance of tuning parameter like batch size and learning rate. Additionally, we showed how perceptron can accurately classify high dimensional data through our experimentation with testing on seven dimensional data. Overall, these findings gave us the conceptual foundations for linear classification and provided a practical roadmap for applying and optimizing perceptron-based models."
  }
]
[
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Classifying Palmer Penguins/index.html",
    "href": "posts/Classifying Palmer Penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nThis blog post will explore the use of machine learning to classify Palmer Penguins based on their physical measurements and location. Beginning with data preprocessing and exploration cleaned up and contextualized our analysis for the subsequent model training. We employed logistic regression and selected features in a repeatable and cross-validated manor achieving an 100% testing accuracy when classifying the penguins. This result shows the effectiveness of using culmen length, culmen depth, and island location as predictive features. The blog post also lays out provides a tradition workflow to use machine learning on ecological datasets.\n\n\nfrom IPython.display import Image, display\ndisplay(Image(filename='palmer_station.png'))\n\n\n\n\n\n\n\n\nImage Source\n\n\nData\nThis code block handles our Training Data Acquisition from the Palmer Penguins data set. This collection holds data for three types of Penguins living across three islands. There is a mix of quantitative measurements and qualitative observations.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nHere is a first look at our raw data:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nWe first extract the species name for data presentation purposed, drop unnecessary columns, remove missing values, and filter out any invalid data. We return the processed feature set (X_train) and target species labels (y_train).\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Mapping full species names exclude scientific names\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nThis is what our formatted feature set looks like:\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nData Exploration\nThe pair plot below gives us a introduces us to our data with a wide array of visualizations. These pairwise relationships are a great starting point to begin understanding the nature of the data.\n\nimport seaborn as sns\nsns.set_theme()\nsns.pairplot(data=train, hue=\"Species\")\n\n\n\n\n\n\n\n\nI scanned the above plot for data pairs that I thought showed promise in indicating species differentiation.\nHere is the relationship between Culmen Length (mm) and Culmen Depth (mm):\n\nsns.jointplot(data = train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue = \"Species\")\n\n\n\n\n\n\n\n\nWe can see that the three species of Penguins fall into three more or less distinct regions of the scatter plot with minimal overlap. This suggests that Culmen Length and Culmen Depth could be good points of reference when training a classification model.\nBelow is a plot that demonstrates the relationship between Gender, Flipper Length, and Body Mass:\n\nsns.catplot(data = X_train, x = \"Sex_FEMALE\", y = \"Flipper Length (mm)\", hue = \"Body Mass (g)\")\n\n\n\n\n\n\n\n\nThe above plot shows us that flipper length and body mass are positively correlated. In addition, we see that Males penguins have a higher growth ceiling in terms of body mass and flipper length.\nThe figure below illustrated some interesting data pertaining to species distribution over the islands. In addition, it gives us insight on the flipper length by species.\n\nimport numpy as np\ntrain.groupby([\"Island\", \"Species\"])[\"Flipper Length (mm)\"].agg(mean_flipper_length_mm=\"mean\", std_flipper_length_mm=\"std\")\n\n\n\n\n\n\n\n\n\nmean_flipper_length_mm\nstd_flipper_length_mm\n\n\nIsland\nSpecies\n\n\n\n\n\n\nBiscoe\nAdelie\n188.636364\n6.570855\n\n\nGentoo\n216.752577\n5.933715\n\n\nDream\nAdelie\n190.133333\n6.780989\n\n\nChinstrap\n196.000000\n7.423419\n\n\nTorgersen\nAdelie\n191.195122\n6.626536\n\n\n\n\n\n\n\nWe can observe that not every species is found on every island. The Adelie Penguin the only penguin found on all three surveyed islands. The Gentoo penguin is found exclusively on Biscoe Island and the Chinstrap penguin is found exclusively on Dream Island. In addition, the Gentoo penguins have the largest and least variable mean flipper length. They are followed by the Chinstrap penguins in size, then the Adelie with the smallest mean flipper lengths. Adelie and Chinstrap flipper lengths may overlap quite a bit, potentially making flipper length an unreliable feature for classifying the two species.\n\n\nModel Training\nIn the following we will employ a Logistic Regression model to classify our penguins. An integral step is figuring out which features we want to use to train our model. My methodology for this is a the brute force approach of testing out all the possible combinations using the handy combinations tool. The combinations tool will be used in conjunction with the cross_val_score tool from sklearn that will cross validate to avoid overfitting to the data. This step will help us perform better on data that the model has never seen before.\nAfter encountering maximum iteration issues with the logistic regression model, I scaled the data with the StandardScaler to be more manageable for the model\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\nLR = LogisticRegression()\nscaler = StandardScaler()\n\nX_train[all_quant_cols] = scaler.fit_transform(X_train[all_quant_cols])\nscore = 0\nfinal_cols = []\n\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    cross_val_scores = cross_val_score(LR, X_train[cols], y_train, cv = 10)\n    if cross_val_scores.mean() &gt; score:\n      score = cross_val_scores.mean()\n      final_cols = cols\n\nprint(f\"The best model scored {score*100}% accuracy when testing on training data using: \\n{final_cols}\")    \n\nThe best model scored 99.21538461538461% accuracy when testing on training data using: \n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nNow we have a repeatable, quantitative approach to justify training our model on the following features: Island, Culmen Length (mm), Culmen Depth (mm)\nI rearrange final_cols below to lead with quantitative features to conform to the graphing parameters that I will present later\n\nfinal_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nLR = LogisticRegression()\nLR.fit(X_train[final_cols], y_train)\nLR.score(X_train[final_cols], y_train)\n\n0.99609375\n\n\nOur model performed with ~99% accuracy when using our selected three features and testing on our training data. This validates some of our visual predictions we identified in our exploration section. This is a great accuracy to have… however we are still testing our model on the data it was trained with. Next we will test it against unseen data!\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nX_test, y_test = prepare_data(test)\nX_test[all_quant_cols] = scaler.fit_transform(X_test[all_quant_cols])\n\nLR.score(X_test[final_cols], y_test)\n\n1.0\n\n\nWow! We achieved an 100% testing accuracy on our test data! In context, we were able to correctly classify what type of penguin an individual was based on what island they were on and their culmen length and depth.\n\n\nEvaluation\nThe following block sets up a plot panel of decision regions that represent our classifier.\n\nfrom matplotlib import pyplot as plt\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (9, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[final_cols], y_train)\n\n\n\n\n\n\n\n\nAs we noted in our exploration, not all penguins are found on all islands. This mean that our model essentially only had to account for a maximum of two species of penguins on any given island. we can see that the Culmen Length and Depth were also clustered nicely to have clear linear segmentation for our decision regions.\nLets take a look at our confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[final_cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nAs we had a 100% testing accuracy, this is exactly the sort of confusion matrix we would expect. The diagonal representing our correct classifications. Above and below the diagonal are empty because we did not misclassify any penguins.\nHere is another way to digest the confusion matrix:\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguins who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguins who were classified as Adelie.\nThere were 0 Adelie penguins who were classified as Chinstrap.\nThere were 0 Adelie penguins who were classified as Gentoo.\nThere were 0 Chinstrap penguins who were classified as Adelie.\nThere were 11 Chinstrap penguins who were classified as Chinstrap.\nThere were 0 Chinstrap penguins who were classified as Gentoo.\nThere were 0 Gentoo penguins who were classified as Adelie.\nThere were 0 Gentoo penguins who were classified as Chinstrap.\nThere were 26 Gentoo penguins who were classified as Gentoo.\n\n\n\n\nDiscussion\nExploring the Palmer Penguins data is a great introduction to data analysis and machine learning. Our results highlighted the effectiveness of using Culmen Depth, Culmen Length and Island Location as predictive features to train a Logistic regression model. We began by exploring the data set. We set up a series of plots and tables that helped us contextualize the data and make predictions about which features of the data may be helpful. While this was useful for understanding the data we were working with, we needed a repeatable method for choosing our eventual three features. We then turned to training and testing logistic regression models on different feature combinations. We scored each combination with how it performed against the training data and was cross validated against smaller subsets of the data to avoid overfitting. Finally we used the cross validated features with the largest accuracy and tested them against a separate test data set. Here we achieved the desired 100% testing accuracy. Finally, we took a moment to evaluate these results by examining the decision regions, and looking at the confusion matrix. Visualizing decision regions highlighted how well logistic regression can separate species based on our selected features. The confusion matrix confirmed the reliability of our model as there were no misclassifications. This blog gave me good insight on data analysis and practical machine learning workflows. Several important takeaways were the importance of separating our training and testing data to ensure our model works on unseen data. The importance of cross validation is also key to not overfitting our data."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, my name is Lukka Wolff. Welcome to my Blog!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing the Perceptron Algorithm\n\n\n\n\n\nImplementing the Perceptron Algorithm in Python\n\n\n\n\n\nApr 2, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nAuditing Bias in Automated Decision-Making Systems\n\n\n\n\n\nMar 12, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nMar 5, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying Palmer Penguins using Machine Learning\n\n\n\n\n\nFeb 26, 2025\n\n\nLukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Abstract\nIn this study, we set out to design a data-driven system for automating credit decisions at a hypothetical bank. Our objective was to create a score function that captures the balance between potential revenue and risk, using predictive models trained on historical data to estimate the likelihood of loan repayment. Our primary drive as the hypothetical bank was to maximize profit, however, we observe that this choice has potentially negative impacts on our diverse borrowers.\n\n\nData\nWe are diving into a Credit Risk Dataset that simulates credit bureau data.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\nHere is a first look at the raw data:\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nI want to highlight several important features of this dataset.\n\nloan_percent_income is ratio of the loan amount to the individual’s income\nloan_int_rate is the annual interest rate on the loan.\nloan_status tells us whether or not the individual defaulted on their loan. This a a binary feature where 1 indicates the individual defaulted, and 0 indicates the loan was repaid in full. This is our Target Variable.\n\nLets have a look at how common defaulting is in our training data:\n\ndf_train[\"loan_status\"].value_counts(normalize=True)\n\nloan_status\n0    0.78242\n1    0.21758\nName: proportion, dtype: float64\n\n\nIn the dataset, around 21% of borrowers default on their loan. This is going to be the our base rate for prediction.\n\ndf_train[\"person_age\"].describe()\n\ncount    26064.000000\nmean        27.734385\nstd          6.362612\nmin         20.000000\n25%         23.000000\n50%         26.000000\n75%         30.000000\nmax        144.000000\nName: person_age, dtype: float64\n\n\nThere seems to be some slight errors in our data with the age of certain individuals. 144, although impressive, is highly unlikely. Thus, without context for why the data has such an outlier, I am going to filter our data to exclude persons over 100 years old. In addition, I will do some other basic data cleaning like removing NaN entries, encoding qualitative features, etc.\n\ndf_train = df_train[df_train[\"person_age\"] &lt; 100]\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(df_train[\"cb_person_default_on_file\"])\n\ndf_train[\"cb_person_default_on_file\"] = le.transform(df_train[\"cb_person_default_on_file\"])\n\ndf_train = df_train.dropna()\n\nNow our data set looks like this!\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\n1\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\n0\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\n0\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\n0\n10\n\n\n6\n21\n21700\nRENT\n2.0\nHOMEIMPROVEMENT\nD\n5500\n14.91\n1\n0.25\n0\n2\n\n\n\n\n\n\n\n\n\nExploration\nLet’s dive a bit into our data to see if we can pick up on any interesting trends.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(style=\"whitegrid\")\n\n\nsns.displot(data=df_train, x=\"loan_percent_income\", hue=\"loan_status\", kind=\"kde\", bw_adjust=0.9, fill=True, alpha=0.25)\n\n\n\n\n\n\n\n\nThe above figure uses Kernel Density Estimation (KDE), a technique that helps visualize the distribution of data points. In this case we are examining how loan_percent_income is distributed in our data set and its possible relation to defaulting on your loan. We can see here that there is a high density of people who repaid their loans where the loan amount factored between 10-20% of their annual income. This may point to the fact that 10-20% of ones income is a manageable amount, and where we see a spike in 1 around 35%+ may be an rate that is hard to pay off and thus lead to defaults. It is also notable that not many people are requesting loans that are 30%+ of their annual income.\n\nsns.kdeplot(data=df_train, x=\"cb_person_cred_hist_length\", y=\"loan_int_rate\", cmap=\"rocket\", fill=True)\n\n\n\n\n\n\n\n\nThis is another KDE plot that shows the relation in loan interest rates given by the bank and an individuals credit history length. We can observe that there is a high density of people who have a credit history length between 0-5 years. Within this rage there are two hot spots for the interest rates on their loans, around 7.5% and around 11% interest. So although a large amount of people have relatively short credit histories, the bank are differentiating their offers based on other factors.\n\nsns.catplot(data=df_train, kind=\"bar\", x=\"person_home_ownership\", y=\"loan_percent_income\", hue=\"loan_status\")\n\n\n\n\n\n\n\n\nThis figure really highlights the importance of loan_percent _income as a possible meaningful predictor of who will default. We see that across the board, regardless of home ownership type, those who repaid their loans requested loans that were around 15% of their annual income. However, those who defaulted on their loans were consistently requesting loans that were a much higher rate (ranging from ~17%-35%).\nLet’s examine how the connection between age, credit history length, and employment length.\n\nimport numpy as np\n\nbins = [0, 5, 10, 15, 20, np.inf]\nlabels = ['0-5', '5-10', '10-15', '15-20', '20+']\n\ndf_train.groupby(\n    pd.cut(df_train['cb_person_cred_hist_length'], bins=bins, labels=labels), observed=True\n)[['person_age', 'person_emp_length']].agg(['mean'])\n\n\n\n\n\n\n\n\nperson_age\nperson_emp_length\n\n\n\nmean\nmean\n\n\ncb_person_cred_hist_length\n\n\n\n\n\n\n0-5\n24.253351\n4.277778\n\n\n5-10\n29.975727\n5.417458\n\n\n10-15\n40.011823\n5.982576\n\n\n15-20\n41.297771\n6.046178\n\n\n20+\n57.318471\n5.923567\n\n\n\n\n\n\n\nAs expected credit history length increases with age. However it is interesting to note that the employment length (time spent at an individuals last job) remain roughly the same, between 4-6 years on average.\n\n\nModel Building\nNow we will dive into creating a model that will hopefully do a good job at predicting who we should give loans to. As a bank what we care about is profit! However, to train a model that will get us good profit, I will start by finding features to train our model on that perform with high accuracy.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\n\nall_qual_cols = [\"person_home_ownership\", \"loan_intent\"]\nall_quant_cols = [\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\", \"cb_person_default_on_file\"]\n\ndf_train = pd.get_dummies(df_train)\n\nLR = LogisticRegression(max_iter = 1000)\nscaler = StandardScaler()\n\n# df_train[all_quant_cols] = scaler.fit_transform(df_train[all_quant_cols])\nscore = 0\nfinal_cols = []\n\nX_train = df_train.drop(columns = \"loan_status\")\ny_train = df_train[\"loan_status\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    cross_val_scores = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    if cross_val_scores.mean() &gt; score:\n      score = cross_val_scores.mean()\n      final_cols = cols\n\n\nprint(f\"The best model scored {score*100:.3f}% accuracy when testing on training data using: \\n{final_cols}\")    \n\nThe best model scored 84.875% accuracy when testing on training data using: \n['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_percent_income']\n\n\nThis accuracy, although not stellar, should be high enough to hopefully be a good predictor to maximize our profit margins.\nBut how are we going to decide who to give loans to? Great question! We are going to give each loan applicant a score s which predicts their likelihood to default on a loan. Higher scores indicate greater reliability.\n\ndef linear_score(X, w):\n    return X@w\n\nThe above function calculates our linear score as a function of the features X and a weight vector w. We can get this vector of weight by fitting a Linear Regression model as shown below.\n\nLR.fit(X_train[final_cols], y_train)\nLR.coef_\n\narray([[-7.59666254e-01, -9.36504060e-02, -1.80228891e+00,\n         2.75434204e-01, -3.61277124e-03,  8.27700853e+00]])\n\n\n\nw = LR.coef_[0]\ns = linear_score(df_train[final_cols], w)\n\nWe now have s which is a vector of scores for each applicant. We can see the distribution of these scores below.\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nhist = ax.hist(s, bins = 50, color = \"purple\", alpha = 0.6, linewidth = 0.75, edgecolor = \"black\")\nlabs = ax.set(xlabel = r\"Score $s$\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\nAs a reminder, as the bank, we want to use these scores to determine who we should give loans to in order to turn the maximum profit. Thus we need to determine a threshold at which we make the most money and use it as a cutoff.\nBelow you can find some simplified functions for how we determine our profit margins. - The calculate_profit formula is used when a loan is fully repaid and assumes that the profit earned each year by the bank on a 10-year loan is equal to 25% of the interest rate each year, with the other 75% of the interest going into various costs to manage the bank. This corresponds to the True Negative category. - The calculate_loss formula is employed for defaults, it uses the same mechanism as above but assumes that the borrower defaults three years into the loan and that the bank loses 70% of the principal. This corresponds to the False Negative category.\n\ndef calculate_profit(df_train):\n    return (df_train[\"loan_amnt\"]*(1 + 0.25*(df_train[\"loan_int_rate\"]/ 100))**10) - df_train[\"loan_amnt\"]\n\ndef calculate_loss(df_train):\n    return (df_train[\"loan_amnt\"]*(1 + 0.25*(df_train[\"loan_int_rate\"]/ 100))**3) - (1.7*df_train[\"loan_amnt\"])\n\nBelow, we are testing threshold values between -4 and 4 and using our formulas to calculate our net gain at each interval. Our best threshold is the one that corresponds to the highest net gain.\n\nbest_profit = 0\nbest_threshold = 0\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nfor t in np.linspace(-4, 4, 101): \n    y_pred = s &gt;= t\n    tn = ((y_pred == 0) & (y_train == 0))\n    fn = ((y_pred == 0) & (y_train == 1))\n\n    gain = calculate_profit(df_train[tn]).sum() + calculate_loss(df_train[fn]).sum()\n    gain /= len(df_train)\n    ax.scatter(t, gain, color = \"steelblue\", s = 10)\n    if gain &gt; best_profit: \n        best_profit = gain\n        best_threshold = t\n\n\nax.axvline(best_threshold, linestyle = \"-.\", color = \"grey\", zorder = -10)\nlabs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Net benefit\", title = f\"Best benefit ${best_profit:.2f} at best threshold t = {best_threshold:.3f}\")\n\n\n\n\n\n\n\n\nOur net gain per person when we only give loans to people with a risk score of 2.64 or above is $1443.81 per person! Lets see if this threshold works on data the model has never seen before…\n\n\nBank Evaluation\nHere we are importing the test data set and applying the same preprocessing steps that we used on the training data.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\ndf_test[\"cb_person_default_on_file\"] = le.transform(df_test[\"cb_person_default_on_file\"])\ndf_test = df_test.dropna()\ndf_test = pd.get_dummies(df_test)\n# df_test[all_quant_cols] = scaler.fit_transform(df_test[all_quant_cols])\n\nX_test = df_test.drop(columns = \"loan_status\")\ny_test = df_test[\"loan_status\"]\n\nWe once again calculate a net gain per person score…\n\ntest_scores = linear_score(df_test[final_cols], w)\ny_pred = test_scores &gt; best_threshold\n\ntest_tn = ((y_pred == 0) & (y_test == 0))\ntest_fn = ((y_pred == 0) & (y_test == 1))\n\ntest_gain = calculate_profit(df_test.loc[test_tn]).sum() + calculate_loss(df_test.loc[test_fn]).sum()\ntest_gain /= len(df_test)\n\n\nprint(f\"The net benefit on the test set is ${test_gain:.2f}\")\n\nThe net benefit on the test set is $1384.50\n\n\nOur model did not perform as well on the test data… however, we are still making quite a bit of profit per person from the bank!\n\n\nBorrower’s Evaluation\nThis is all fine and dandy for the bank… make money = bank happy… but what about the borrowers?\nWe are adding columns to our data frame that show each borrowers score and our models prediction.\n\ndf_test[\"person_risk\"] = test_scores\ndf_test[\"loan_status_pred\"] = y_pred\n\n\n(df_test[\"loan_status_pred\"] == df_test[\"loan_status\"]).mean()\n\nnp.float64(0.8474960739835979)\n\n\nOur model accurately predicted who would default on their loan 84.75% of the time. So the majority of people who would have defaulted on their loans were accurately denied… but who exactly are the people?\n\nAge Group Disparity\nLets break down how the model treats people of a certain age groups.\n\nsns.kdeplot(data = df_test, x = \"person_age\", y = \"person_risk\", hue = \"loan_status_pred\", alpha = 0.75, fill = True, bw_adjust=0.7)\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data = df_test, x = \"person_age\", y = \"person_risk\", hue = \"loan_status_pred\")\n\n\n\n\n\n\n\n\nWe can see that most of the people who are receiving loans are under the age age of 40, with a majority falling between 20-30 years old. This makes sense as many of our applicants fall into this age range. This may be a little unfair for persons aged over 40…\n\n\nMedical Loans\nHow about people who are applying for medical loans. I highly important demographic of borrowers whose health and livelihoods may be contingent on access to a loan. What fraction of the test dataset consists of medical loans that were approved?\n\napp_med_loans = (df_test[\"loan_intent_MEDICAL\"] == True) & (df_test[\"loan_status_pred\"] == False)\nprint(f\"Only {app_med_loans.mean()*100:.2f}% of people applying for medical loans were approved\")\n\nOnly 16.66% of people applying for medical loans were approved\n\n\n\ndef_rate_med_loans = (df_test[\"loan_intent_MEDICAL\"] == True) & (df_test[\"loan_status\"] == 1)\nprint(f\"While only {def_rate_med_loans.mean()*100:.2f}% actually defaulted on their medical loans\")\n\nWhile only 5.32% actually defaulted on their medical loans\n\n\nThis is quite harrowing… operating on a purely profit based system is taking loans away from people who may really need this money and would not have defaulted! We will dive into the fairness of this decision later…\n\n\nEducational Loans\nLets see our model treats people applying for educational loans.\n\napp_edu_loans = (df_test[\"loan_intent_EDUCATION\"] == True) & (df_test[\"loan_status_pred\"] == False)\nprint(f\"Only {app_edu_loans.mean()*100:.2f}% of people applying for education loans were approved\")\n\nOnly 18.71% of people applying for education loans were approved\n\n\n\ndef_rate_edu_loans = (df_test[\"loan_intent_EDUCATION\"] == True) & (df_test[\"loan_status\"] == 1)\nprint(f\"While only {def_rate_edu_loans.mean()*100:.2f}% actually defaulted on their loans\")\n\nWhile only 3.44% actually defaulted on their loans\n\n\nOnce again we are denying loans to a huge amount of people who in reality paid off their loans.\n\n\nBusiness Venture Loans\n\napp_bus_loans = (df_test[\"loan_intent_VENTURE\"] == True) & (df_test[\"loan_status_pred\"] == False)\nprint(f\"Only {app_bus_loans.mean()*100:.2f}% of people applying for business venture loans were approved\")\n\nOnly 15.42% of people applying for business venture loans were approved\n\n\n\ndef_rate_bus_loans = (df_test[\"loan_intent_VENTURE\"] == True) & (df_test[\"loan_status\"] == 1)\nprint(f\"While only {def_rate_bus_loans.mean()*100:.2f}% actually defaulted on their loans\")\n\nWhile only 2.46% actually defaulted on their loans\n\n\nOnce again we denied people who would have paid back their loans!\n\n\n\nFairness Discussion\nWe observed some trends in our model that seemed pretty unfair, people who would have paid off their loans were denied… but what exactly does it mean to be fair? Fairness should be making a model that makes decisions without bias towards certain groups, especially dealing with sensitive features like race, gender, or socioeconomic status. Fairness also constitutes taking into account features that are relevant to the situation. At face value our model is treating people pretty harshly across the board with regards to the intent of their loan. We do however consider people age in the model, and we do seem to have a slight bias towards younger people… But in the medical loan situation, doesn’t it seem unfair that people are being denied. doesn’t the severity / intent behind the loan seem like an important and relevant factor? In a sense we are being biased towards people who need the money… people who are ill should be treated with extra care and that should be a relevant factor in where we draw our threshold and how we train our model.\n\n\nConclusion\nOur analysis revealed that automated decision system can be beneficial to a bank’s overall profitability by approving loans only when the expected risk-adjusted returns are maximized. However, the findings also reveal the importance of continuously monitoring the system’s performance to ensure that it does not inadvertently disadvantage certain segments of the population. We observed that borrowers applying for loans of high importance, such as medical loans were drastically undeserved. In addition, older generation wer not given access to credit all in the name of profit. Ultimately, this exploration highlights the need for a balanced approach that integrates profit maximization with fairness and inclusivity taking into account other factors that are relevant to the lives of borrowers."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Auditing Bias/index.html",
    "href": "posts/Auditing Bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Abstract\nThis project audits bias in automated decision-making systems by analyzing employment predictions from the 2018 American Community Survey (ACS) data for Georgia. A Random Forest Classifier was trained to predict employment status based on demographic features such as age, education, sex, disability, and nativity, while examining racial bias specifically between White and Black/African American individuals. The audit revealed approximately balanced accuracy, positive predictive values, and error rates across these racial groups, though slight discrepancies exist. Despite good numerical fairness, we must still consider ethical considerations regarding consent, data recency, and the ethical deployment of such models in different decision-making contexts.\n\n\nData and Feature Selection\nWe are using the folkables package to access data from the 2018 American Community Survey’s Public Use Microdata Sample (PUMS) for the state of Georgia.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"GA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000025\n5\n1\n3700\n3\n13\n1013097\n68\n51\n...\n124\n69\n65\n63\n117\n66\n14\n68\n114\n121\n\n\n1\nP\n2018GQ0000035\n5\n1\n1900\n3\n13\n1013097\n69\n56\n...\n69\n69\n7\n5\n119\n74\n78\n72\n127\n6\n\n\n2\nP\n2018GQ0000043\n5\n1\n4000\n3\n13\n1013097\n89\n23\n...\n166\n88\n13\n13\n15\n91\n163\n13\n89\n98\n\n\n3\nP\n2018GQ0000061\n5\n1\n500\n3\n13\n1013097\n10\n43\n...\n19\n20\n3\n9\n20\n3\n3\n10\n10\n10\n\n\n4\nP\n2018GQ0000076\n5\n1\n4300\n3\n13\n1013097\n11\n20\n...\n13\n2\n14\n2\n1\n2\n2\n13\n14\n12\n\n\n\n\n5 rows × 286 columns\n\n\n\nThis data set contains a large amount of features for each individual, so we are going to narrow it down to only those that we may use to train our model.\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n51\n13.0\n5\n16\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n2\n6.0\n\n\n1\n56\n16.0\n3\n16\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n1\n2.0\n2\n1\n6.0\n\n\n2\n23\n20.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n1.0\n2\n2\n1.0\n\n\n3\n43\n17.0\n1\n16\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n1\n2\n6.0\n\n\n4\n20\n19.0\n5\n16\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n\n\n\n\n\nA few key features to note are:\n\nESR is employment status (1 if employed, 0 if not)\nRAC1P is race (1 for White Alone, 2 for Black/African American Alone, 3 and above for other self-identified racial groups)\nSEX is binary sex (1 for male, 2 for female)\n\nNow we select for the features we want to use and we will be able to constuct a BasicProblem that expresses our desire to use these features to predict employment status ESR, using RAC1P as the group label.\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nWe now have a feature matrix features, a label vector label, and a group label vector group.\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(100855, 15)\n(100855,)\n(100855,)\n\n\nWe are now going to split our data into a training set and a testing set:\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nData Exploration\nBefore we dive straight into model training, lets take a deeper look at our data.\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\nHere is a quick look at our data frame containing our training data:\n\ndf.head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\ngroup\nlabel\n\n\n\n\n0\n48.0\n16.0\n1.0\n1.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n2\nTrue\n\n\n1\n52.0\n24.0\n2.0\n0.0\n2.0\n0.0\n1.0\n3.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n1\nTrue\n\n\n2\n55.0\n18.0\n5.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n3.0\n1.0\n2.0\n2.0\n2.0\n1.0\n2\nTrue\n\n\n3\n15.0\n12.0\n5.0\n3.0\n2.0\n1.0\n4.0\n3.0\n0.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2.0\n6\nFalse\n\n\n4\n26.0\n22.0\n1.0\n0.0\n2.0\n0.0\n5.0\n1.0\n4.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2\nFalse\n\n\n\n\n\n\n\n\nlen(df)\n\n80684\n\n\nThis data set contains information from \\(80684\\) individuals in the state of Georgia.\n\ndf[\"label\"].value_counts()\n\nlabel\nFalse    44664\nTrue     36020\nName: count, dtype: int64\n\n\n\ndf['label'].mean()\n\nnp.float64(0.44643299786822666)\n\n\nOf these individuals, 44.64% or \\(36020\\) individuals are employed.\n\ndf['group'].value_counts()\n\ngroup\n1    53302\n2    20239\n6     3267\n9     1996\n8     1589\n3      159\n5       66\n7       66\nName: count, dtype: int64\n\n\n\ndf['group'].value_counts(normalize=True)\n\ngroup\n1    0.660627\n2    0.250843\n6    0.040491\n9    0.024738\n8    0.019694\n3    0.001971\n5    0.000818\n7    0.000818\nName: proportion, dtype: float64\n\n\nThe two largest racial groups are 1 White Alone with 53302 individuals making up 66% of the data, and 2 Black/African American Alone with 20239 individuals making up 25% of the data.\n\ndf.groupby('group')['label'].mean()\n\ngroup\n1    0.460771\n2    0.416127\n3    0.433962\n5    0.348485\n6    0.481175\n7    0.484848\n8    0.429830\n9    0.330160\nName: label, dtype: float64\n\n\n~46% of White Alone individuals are employed and ~42% of Black/African American Alone individuals are employed.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(style=\"whitegrid\")\n\n\ndef plot_intersection(df, col1='group', col2='sex'):\n    prop_df = df.groupby([col1, col2])['label'].mean().reset_index()\n    plt.figure(figsize=(8, 5))\n\n    ax = sns.barplot(x=col1, y='label', hue=col2, data=prop_df)\n    \n    plt.title(f'Employment Proportion by {col1} and {col2}')\n    plt.xlabel(col1)\n    plt.ylabel(f'Employment Proportion (%)')\n\n    for p in ax.patches:\n        ax.annotate(f'{p.get_height()*100:.2f}', \n                   (p.get_x() + p.get_width() / 2., p.get_height()),\n                   ha = 'center', va = 'bottom',\n                   xytext = (0, 5), textcoords = 'offset points')\n    \n    plt.tight_layout()\n    plt.show()\n\ngroup_sex = plot_intersection(df, 'group', 'SEX')\n    \n\n\n\n\n\n\n\n\nFor many groups, the percentage of men who are employed is higher than that of women. One notable group where this is not the case is 2 Black/African American where the percentage of employed women is ~44% against ~39% for men.\nNATIVITY indicates a persons place of birth. 1 being Native born and 2 being Foreign born.\n\ngroup_nativity = plot_intersection(df, 'group', 'NATIVITY')\n\n\n\n\n\n\n\n\nInterestingly, across the board we see that the percentage of foreign born individuals who are employed is much higher than the proportion of native born individuals. However, as seen in the plot below, this may be attributed to the fact that few foreign born individuals on the extremities of the age, reducing the influence of youth and seniority as factors in employment proportion.\n\nsns.displot(data=df, x=\"AGEP\", hue=\"NATIVITY\", kind=\"kde\", bw_adjust=0.5, fill=True, alpha=0.75)\n\n\n\n\n\n\n\n\nDIS represents an individuals disability status. 1 with disability, and 2 without a disability.\n\ngroup_dis = plot_intersection(df, 'group', 'DIS')\n\n\n\n\n\n\n\n\nAcross the board we see that people without disability are employed at a much higher proportion than people with disabilities.\n\n\nSupplementary plots that I thought were interesting.\n\ncit_sex = plot_intersection(df, 'CIT', 'SEX')\n\n\n\n\n\n\n\n\n\nmar_sex = plot_intersection(df, 'MAR', 'SEX')\n\n\n\n\n\n\n\n\n\nschl_sex = plot_intersection(df, 'SCHL', 'SEX')\n\n\n\n\n\n\n\n\n\n\n\nModel Training\nWe are now ready to create a model and train it on our training data. We will first scale our data, then we will employ a Random Forest Classifier. This approach uses an array of decision trees on various sub-samples of the data and aggregates their results. Learn more here.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\nacc = 0\nbest_depth = 0\nfor depth in range(5, 20):\n    model = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=depth))\n    model.fit(X_train, y_train)\n    cv_scores = cross_val_score(model, X_train, y_train, cv = 5)\n    if cv_scores.mean() &gt; acc:\n        best_depth = depth\n        acc = cv_scores.mean()\n\nprint(f\"Best maximum tree depth: {best_depth}, Accuracy: {acc*100:.2f}%\")\n\nBest maximum tree depth: 16, Accuracy: 83.39%\n\n\nAbove, we tuned our model complexity using the max_depth parameter of the RandomForestClassifier. This controls how deep each tree in our forest can get which impacts how general our model is with regards to things like overfitting. We examined values ranging from 5 to 20 for the max depth and found the highest cross validated accuracy when max_depth=16.\n\nRF = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=best_depth))\nRF.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('randomforestclassifier',\n                 RandomForestClassifier(max_depth=16))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('randomforestclassifier',\n                 RandomForestClassifier(max_depth=16))]) StandardScaler?Documentation for StandardScalerStandardScaler() RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(max_depth=16) \n\n\n\n\nModel Audit\n\nOverall Measures\n\ny_hat = RF.predict(X_test)\n(y_hat == y_test).mean()\n\nnp.float64(0.8281195776114223)\n\n\nOur model has an overall accuracy of 82.81% on the testing data suite. Not too shabby!\nWe will now address the positive predictive value (PPV) of our model (we wont evaluate true sufficiency right now as we are skipping NPV). Given that the prediction is positive (y_hat = 1), how likely is it that the prediction is accurate (y_test = 1)? In other words, if we predict someone to be employed, how likely is it that they are actually employed?\nWe can approximate this value with the following code:\n\ntp = (y_hat == 1) & (y_test == 1)\nfp = (y_hat == 1) & (y_test == 0)\nppv = tp.sum() / (tp.sum() + fp.sum())\nprint(f\"Positive Predictive Value: {ppv*100:.2f}%\")\n\nPositive Predictive Value: 77.46%\n\n\nSo, when our model predicts someone is employed, they are actually employed 77.46% of the time.\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\ncm = confusion_matrix(y_test, y_hat)\nConfusionMatrixDisplay(cm, display_labels=['Not Employed', 'Employed']).plot(cmap='Blues')\nplt.title('Confusion Matrix')\n\nText(0.5, 1.0, 'Confusion Matrix')\n\n\n\n\n\n\n\n\n\nAbove is our confusion matrix, lets use this information to find the false negative and false positive rates for our model.\n\nfn = (y_hat == 0) & (y_test == 1)\nfnr = fn.sum() / (tp.sum() + fn.sum())\nprint(f\"False Negative Rate: {fnr*100:.2f}%\")\n\nFalse Negative Rate: 12.74%\n\n\nOur model incorrectly classified people as unemployed when they were in fact employed 12.74% of the time.\n\nfp = (y_hat == 1) & (y_test == 0)\ntn = (y_hat == 0) & (y_test == 0)\nfpr = fp.sum() / (fp.sum() + tn.sum())\nprint(f\"False Positive Rate: {fpr*100:.2f}%\")\n\nFalse Positive Rate: 20.84%\n\n\nOur model incorrectly classified people as employed when they were in fact unemployed 20.84% of the time.\n\n\nBy-Group Measures\nNow, lets explore how our model treats people in their respective groups. We are going to focus primarily on the possible discrepancies between individuals in 1 White Alone and 2 Black/African American Alone groups.\n\nwa = (y_hat == y_test)[group_test == 1].mean()\naa = (y_hat == y_test)[group_test == 2].mean()\nprint(f\"White Alone Accuracy: {wa*100:.2f}%\\nBlack/African American Alone Accuracy: {aa*100:.2f}%\")\n\nWhite Alone Accuracy: 82.40%\nBlack/African American Alone Accuracy: 83.17%\n\n\nOur model has pretty comparable accuracy scores for both groups, only slightly lower for White Alone individuals.\n\ntp_wa = ((y_hat == 1) & (y_test == 1) & (group_test == 1))\nfp_wa = ((y_hat == 1) & (y_test == 0) & (group_test == 1))\nppv_wa = tp_wa.sum() / (tp_wa.sum() + fp_wa.sum())\n\ntp_aa = ((y_hat == 1) & (y_test == 1) & (group_test == 2))\nfp_aa = ((y_hat == 1) & (y_test == 0) & (group_test == 2))\nppv_aa = tp_aa.sum() / (tp_aa.sum() + fp_aa.sum())\n\nprint(f\"PPV for White Alone: {ppv_wa*100:.2f}%\")\nprint(f\"PPV for Black/African American Alone: {ppv_aa*100:.2f}%\")\n\nPPV for White Alone: 77.96%\nPPV for Black/African American Alone: 75.84%\n\n\nOnce again our PPV rates are quite comparable, although the White Alone group does have a slightly higher PPV.\n\nfn_wa = (y_hat == 0) & (y_test == 1) & (group_test == 1)\nfnr_wa = fn_wa.sum() / (tp_wa.sum() + fn_wa.sum())\n\nfn_aa = (y_hat == 0) & (y_test == 1) & (group_test == 2)\nfnr_aa = fn_aa.sum() / (tp_aa.sum() + fn_aa.sum())\n\nprint(f\"FNR for White Alone: {fnr*100:.2f}%\")\nprint(f\"FNR for Black/African American Alone: {fnr_aa*100:.2f}%\")\n\nFNR for White Alone: 12.74%\nFNR for Black/African American Alone: 12.86%\n\n\nThe false negative rates are once again very similar across groups.\n\nfp_wa = ((y_hat == 1) & (y_test == 0) & (group_test == 1))\ntn_wa = ((y_hat == 0) & (y_test == 0) & (group_test == 1))\nfpr_wa = fp_wa.sum() / (fp_wa.sum() + tn_wa.sum())\n\nfp_aa = ((y_hat == 1) & (y_test == 0) & (group_test == 2))\ntn_aa = ((y_hat == 0) & (y_test == 0) & (group_test == 2))\nfpr_aa = fp_aa.sum() / (fp_aa.sum() + tn_aa.sum())\n\nprint(f\"FPR for White Alone: {fpr_wa*100:.2f}%\")\nprint(f\"FPR for Black/African American Alone: {fpr_aa*100:.2f}%\")\n\nFPR for White Alone: 21.33%\nFPR for Black/African American Alone: 19.64%\n\n\nThere is some slight discrepancy here as persons in the White Alone group is more often mistaken for having a job than persons in the Black/African American Alone group.\n\n\nBias Measures\nIn terms of accuracy, our model seems to be performing well. Let’s take a deeper look at how our model might be biased or unfair by examining calibration, error rate balance, and statistical parity.\nOur model can be considered well-calibrated or sufficient if it reflects equal likelihood of employment irrespective of the individuals’ group membership. That is, free from predictive bias, this our PPV for both groups should be the same. Looking back to our calculation of these scores, we saw that they were about equal. PPV for White Alone: 77.96% and PPV for Black/African American Alone: 75.84%. Thus, we can say our model is well-calibrated.\n\nOur model can only satisfy approximate error rate balance given that the true positive rate (TPR) and false positive rates (FPR) be equal on the two groups.\n\ntpr_wa = tp_wa.sum() / (tp_wa.sum() + fn_wa.sum())\ntpr_aa = tp_aa.sum() / (tp_aa.sum() + fn_aa.sum())\nprint(f\"TPR --&gt; WA: {tpr_wa*100:.2f}% ~~ AA: {tpr_aa*100:.2f}%\")\nprint(f\"FPR --&gt; WA: {fpr_wa*100:.2f}% ~~ AA: {fpr_aa*100:.2f}%\")\n\nTPR --&gt; WA: 86.69% ~~ AA: 87.14%\nFPR --&gt; WA: 21.33% ~~ AA: 19.64%\n\n\nWe can see that both groups have an approximately equal TPR and FPRs. Thus our model satisfies approximate error rate balance.\n\nOur model satisfies statistical parity if the proportion of individuals classified as employed is the same for each group.\n\nprop_wa = (y_hat == 1)[group_test == 1].mean()\nprop_aa = (y_hat == 1)[group_test == 2].mean()\nprint(f\"Proportion of White Alone Predicted to be Employed: {prop_wa*100:.2f}%\")\nprint(f\"Proportion of Black/African American Alone Predicted to be Employed: {prop_aa*100:.2f}%\")\n\nProportion of White Alone Predicted to be Employed: 51.74%\nProportion of Black/African American Alone Predicted to be Employed: 47.61%\n\n\nWe can observe some differences in these two scores, a higher proportion of White Alone persons are predicted to be employed than Black/African American Alone persons. The difference remains small as the rated are within 5% of one another… as we don’t have a set threshold we cant know if the difference is significant, ergo we cant say that we do or do not satisfy statistical parity.\n\n\n\nFeasible FNR and FPRs\n\np_wa = ((y_test == 1) & (y_hat == 1))[group_test == 1].mean()\np_aa = ((y_test == 1) & (y_hat == 1))[group_test == 2].mean()\nprint(f\"Prevalence of White Alone: {p_wa*100:.2f}%\")\nprint(f\"Prevalence of Black/African American Alone: {p_aa*100:.2f}%\")\n\nPrevalence of White Alone: 40.34%\nPrevalence of Black/African American Alone: 36.11%\n\n\nThe proportion of true positive values is higher with in the White Alone group.\n\nimport numpy as np\n\nplt.figure(figsize=(8, 5))\n\n\nfnr_range = np.linspace(0, 1, 100)\nfpr_aa_alt = (p_aa / (1 - p_aa)) * ((1 - ppv_aa) / ppv_aa) * (1 - fnr_aa)\nfpr_wa_alt = (p_wa / (1 - p_wa)) * ((1 - ppv_wa) / ppv_wa) * (1 - fnr_wa)\n\n# Calibrate on low PPV --&gt; PPv from Black/African American Alone\nfpr_aa_feasible = (p_aa / (1 - p_aa)) * ((1 - ppv_aa) / ppv_aa) * (1 - fnr_range) # PPVb is set equal to the observed value of PPVw\nfpr_wa_feasible = (p_wa / (1 - p_wa)) * ((1 - ppv_aa) / ppv_aa) * (1 - fnr_range) # pw and PPVw are both held fixed\n\nplt.plot(fnr_wa, fpr_wa_alt, 'o', color='orange', label='White Alone')\nplt.plot(fnr_aa, fpr_aa_alt, 'o', color='black', label='Black/African American Alone')\n\nplt.plot(fnr_range, fpr_aa_feasible, '-', color='black')\nplt.plot(fnr_range, fpr_wa_feasible, '-', color='orange')\n\n\n# Shading Attempts\n# delta = np.abs(ppv_wa - ppv_aa)\n# low = (p_wa / (1 - p_wa)) * ((1 - ppv_wa - (delta &lt; 0.05)) / ppv_wa - (delta &lt; 0.05)) * (1 - fnr_range)\n# high = (p_wa / (1 - p_wa)) * ((1 - ppv_wa + (delta &lt; 0.05)) / ppv_wa + (delta &lt; 0.05)) * (1 - fnr_range)\n# plt.fill_between(fnr_range, low, high, color='blue', alpha=0.3)\n\nplt.xlabel('False Negative Rate')\nplt.ylabel('False Positive Rate')\nplt.title('Feasible (FNR, FPR) combinations')\nplt.legend()\n\n\n\n\n\n\n\n\n\nOur current model appears to be working quite well for both groups as we can see their (FNR, FPR) points are not far separated. However, if we did want to equalize our false positive rates (classify someone as employed when they are not), this would necessitate an increase in the false negative rates for White Alone to around 0.22 from its current around 0.12. This would inevitably lead to a drop in accuracy. Contextually, this would mean classifying more White Alone persons who are actually employed as unemployed to match FPR.\nNonetheless, our model performs close to even for both groups.\n\n\n\nConclusion\nThere are many applications for a model of this sort. Making predictions on who is or isn’t employed would benefit many companies that operate on credit or lending. Knowing who is employed would help determine whether it would be wise to approve a higher credit limit, approve a mortgage application, or let someone lease a car. This is the case because employment can be widely used aa feature or indicator of someones ability to pay back their debts and the interest included therein. If you determine that better, your company can be more profitable. This could also be useful for marketing and advertising companies who can use these predictions to improve the targeting of their ads.\nDeploying this specific Random Forest Based model in a large scale setting could make the small discrepancies in error much larger. For instance, lets assume this model is being used by the US government to determine who needs unemployment aid. The small differences in FPRs (WA: ~21% & AA: ~19) could mean tens of millions of people could be classified as employed and not receive the help they need. This would also be slightly disproportionate as millions more White Alone people would not get the aid they need when compared to the ratio of Black/African American individuals. Conversely, this FPR could be to their advantage in a commercial setting that would give them lower interest rates on loans, and possibly better odds at landing a job.\nBased on my Bias Audit, most of my standards of evaluation were quite close. None of them differed enough for me to view the model as problematic with regards to White Alone or Black/African American. Of course, the model also used data from other groups, thus necessitating further exploration into those groups to determine if there are problematic levels of bias.\nBias aside, there could be other potential issues in deploying this algorithm. First and foremost, I believe that, with the exception of advertisement targeting, employment status should be requested and declared with the consent of the individual. There should be some government regulations in what industries are allowed to use such algorithms and how. In the situations that it is used, I worry about the transparency of the model processes. Transparency on what data the data is being used for as it is being collected, and where it came from when it is being used. Moreover, there should be transparency and easily interpretable in use cases like credit approvals, etc. Finally, using data from a Georgia may not be generalizable across the country or even in current day Georgia as many things have changed since 2018 and after COVID. There should be an effort made to keep the training data reflective of current day trends. Thus, even though the model appears acceptable with regards to numerical fairness metrics, there are broader ethical and practical concerns that we should address in order to deploy a responsible and effective model."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#our-model-can-be-considered-well-calibrated-or-sufficient-if-it-reflects-equal-likelihood-of-employment-irrespective-of-the-individuals-group-membership.-that-is-free-from-predictive-bias-this-our-ppv-for-both-groups-should-be-the-same.-looking-back-to-our-calculation-of-these-scores-we-saw-that-they-were-about-equal.-ppv-for-white-alone-77.81-and-ppv-for-blackafrican-american-alone-75.65.",
    "href": "posts/Auditing Bias/index.html#our-model-can-be-considered-well-calibrated-or-sufficient-if-it-reflects-equal-likelihood-of-employment-irrespective-of-the-individuals-group-membership.-that-is-free-from-predictive-bias-this-our-ppv-for-both-groups-should-be-the-same.-looking-back-to-our-calculation-of-these-scores-we-saw-that-they-were-about-equal.-ppv-for-white-alone-77.81-and-ppv-for-blackafrican-american-alone-75.65.",
    "title": "Auditing Bias",
    "section": "Our model can be considered well-calibrated or sufficient if it reflects equal likelihood of employment irrespective of the individuals’ group membership. That is, free from predictive bias, this our PPV for both groups should be the same. Looking back to our calculation of these scores, we saw that they were about equal. PPV for White Alone: 77.81% and PPV for Black/African American Alone: 75.65%.",
    "text": "Our model can be considered well-calibrated or sufficient if it reflects equal likelihood of employment irrespective of the individuals’ group membership. That is, free from predictive bias, this our PPV for both groups should be the same. Looking back to our calculation of these scores, we saw that they were about equal. PPV for White Alone: 77.81% and PPV for Black/African American Alone: 75.65%.\nOur model can only satisfy approximate error rate balance given that the true positive rate (TPR) and false positive rates (FPR) be equal on the two groups.\n\ntpr_wa = tp_wa.sum() / (tp_wa.sum() + fn_wa.sum())\ntpr_aa = tp_aa.sum() / (tp_aa.sum() + fn_aa.sum())\nprint(f\"TPR --&gt; WA: {tpr_wa*100:.2f}% ~~ AA: {tpr_aa*100:.2f}%\")\nprint(f\"FPR --&gt; WA: {fpr_wa*100:.2f}% ~~ AA: {fpr_aa*100:.2f}%\")\n\nTPR --&gt; WA: 87.14% ~~ AA: 87.43%\n*FPR* --&gt; WA: 21.63% ~~ AA: 19.91%\n\n\nWe can see that both groups have an approximately equal TPR and FPRs.\n–\nOur model satisfies statistical parity if the proportion of individuals classified as employed is the same for each group.\n\nprop_wa = (y_hat == 1)[group_test == 1].mean()\nprop_aa = (y_hat == 1)[group_test == 2].mean()\nprint(f\"Proportion of White Alone Predicted to be Employed: {prop_wa*100:.2f}%\")\nprint(f\"Proportion of Black/African American Alone Predicted to be Employed: {prop_aa*100:.2f}%\")\n\nProportion of White Alone Predicted to be Employed: 52.11%\nProportion of Black/African American Alone Predicted to be Employed: 47.89%\n\n\nWe can observe some differences in these two scores, a higher proportion of White Alone persons are predicted to be employed than Black/African American Alone persons. The difference remains small as the rated are within 5% of one another."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#we-can-see-that-both-groups-have-an-approximately-equal-tpr-and-fprs.",
    "href": "posts/Auditing Bias/index.html#we-can-see-that-both-groups-have-an-approximately-equal-tpr-and-fprs.",
    "title": "Auditing Bias",
    "section": "We can see that both groups have an approximately equal TPR and FPRs.",
    "text": "We can see that both groups have an approximately equal TPR and FPRs.\nOur model satisfies statistical parity if the proportion of individuals classified as employed is the same for each group.\n\nprop_wa = (y_hat == 1)[group_test == 1].mean()\nprop_aa = (y_hat == 1)[group_test == 2].mean()\nprint(f\"Proportion of White Alone Predicted to be Employed: {prop_wa*100:.2f}%\")\nprint(f\"Proportion of Black/African American Alone Predicted to be Employed: {prop_aa*100:.2f}%\")\n\nProportion of White Alone Predicted to be Employed: 52.11%\nProportion of Black/African American Alone Predicted to be Employed: 47.89%\n\n\nWe can observe some differences in these two scores, a higher proportion of White Alone persons are predicted to be employed than Black/African American Alone persons. The difference remains small as the rated are within 5% of one another."
  },
  {
    "objectID": "posts/Implementing the Perceptron Algorithm/index.html",
    "href": "posts/Implementing the Perceptron Algorithm/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n\nAbstract\n— enter here —\n\n\nImplementation Check\nIn perceptron.py, we implement the perceptron algorithm using…\nBelow we are using Professor Chodrow’s functions to generate and visualize linearly separable data to test our implementation.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nAs we can observe we have generated a set of linearly separable data that our perceptron should be able to separate with 100% accuracy (equivalent to 0 loss)\n\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nloss\n\ntensor(0.)\n\n\nGreat! It worked, we have a perceptron that minimizes our loss to zero! We can see below the step by step process as the perceptron algorithm iteratively finds the line that separates the squares and the circles.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nHere is the line we found:\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n\n\n\n\n\n\n\n\n\n\nExperiments\nLet us delve into how we arrived to this line iteratively on two dimensional data. First, we are going to have a look at a situation where we know our data is linearly separable. We are going to explore this with a slightly modified function from the lecture notes.\n\ntorch.manual_seed(3141)\nX, y = perceptron_data(n_points=50, noise=0.3)\nn = X.shape[0]\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nplot_data = []\nloss_vec = []\n\nloss = 1\nwhile loss &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n        plot_data.append((old_w, torch.clone(p.w), i, loss))\n\n\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex=True, sharey=True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1: 0, 1: 1}\n\nplot_data = plot_data[-6:]\n\nfor current_ax, (old_w, new_w, i, loss) in enumerate(plot_data):\n    ax = axarr.ravel()[current_ax]\n    plot_perceptron_data(X, y, ax)\n    draw_line(old_w, x_min=-1, x_max=2, ax=ax, color=\"black\", linestyle=\"dashed\")\n    draw_line(new_w, x_min=-1, x_max=2, ax=ax, color=\"black\")\n    ax.scatter(X[i, 0], X[i, 1], color=\"black\", facecolors=\"none\", edgecolors=\"black\", \n               marker=markers[marker_map[2 * (y[i].item()) - 1]])\n    ax.set_title(f\"loss = {loss:.3f}\")\n    ax.set(xlim=(-1, 2), ylim=(-1, 2))\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe above figure shows the last six iterations of the perceptron as it is adjusts the weights based on local loss on data that we know is linearly separable. What is interesting to not here is that the line is not narrowly converging towards a loss of zero but rather, adjusts for each point without respect to the whole. The result of this method is that our loss seemingly jumps around in directions that may seem counter intuitive until we reach our loss of zero and terminate.\nThe graph below illustrates this quite well:\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThis is all nice and dandy when we have access to linearly separable data, but what about when our data is not linearly separable. In such a case, it is impossible to have a loss of zero. So we will implement a maximum number of iterations so that our code doesn’t run forever to no avail.\n\ntorch.manual_seed(124816)\nX, y = perceptron_data(n_points=100, noise=0.8)\nn = X.shape[0]\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nmax_iter = 1000 # Maximum number of iterations\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min=-2, x_max=3, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-2, 3), ylim=(-2, 3))\nplt.show()\n\n\n\n\n\n\n\n\nIn the figure above we can observe first that our data is not linearly separable, and thus even after 1000 iterations we achieved a loss of 0.19, which is not terrible, but not zero.\nBelow we can see the evolution of the loss over these iterations, and note that there is not point with loss zero/\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nplt.ylim(0, 0.55)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nWith non linearly separable data we can also see that in some cases our loss can be above 50% misclassified for certain lines. In addition, our final iteration was not a representation of the best we could get, rather where we were after 1000 iterations. We can see that in earlier updates we achieved closer to 15% misclassified.\nThus far we have only been working with two dimensional data, however the perceptron algorithm works with higher dimensional data! Below we are going to run the algorithm on a data set with seven dimensions and observe the evolution of the loss as we iterate.\n\ntorch.manual_seed(2003)\nX, y = perceptron_data(n_points=100, noise=0.5, p_dims=7)\nn = X.shape[0]\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nmax_iter = 1000 # Maximum number of iterations\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nplt.ylim(0, max(loss_vec) + 0.05)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nWe can see that over 1000 iterations our loss never reached zero. We can also note somewhat of a plateau of points around 0.35 that we would lead me to believe that the data is not linearly separable. We can have linearly separable data in seven dimensions as illustrated below.\n\ntorch.manual_seed(2003)\nX, y = perceptron_data(n_points=100, noise=0.07, p_dims=7)\nn = X.shape[0]\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\nmax_iter = 1000 # Maximum number of iterations\nloss = 1\nwhile loss &gt; 0 and max_iter &gt; 0:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        old_w = torch.clone(p.w)\n        opt.step(x_i, y_i)\n        loss = p.loss(X, y).item()\n        \n        loss_vec.append(loss)\n\n    max_iter -= 1\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nplt.ylim(0, max(loss_vec) + 0.05)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAs we can see, in the end we do achieve a loss score of zero indicating we can accurately separate and classify the different sorts of points with a hyperplane of sorts in \\(\\mathbb{R}^7\\). This data was achieved by simply turning down the noise parameter when we generate points.\n\n\nMinibatch Perceptron"
  }
]